#!/usr/bin/env python3
"""
æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ChromaDBã¨ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæ©Ÿæ¢°å­¦ç¿’ã®å®Ÿæ–½

ã€ä¸»ãªæ©Ÿèƒ½ã€‘
1. ChromaDBã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
2. ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
3. åˆ†é¡ãƒ»äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
4. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ä¿å­˜
"""

import os
import sys
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import json
import pickle

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    import matplotlib.pyplot as plt
    import seaborn as sns
except ImportError as e:
    print(f"âŒ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install scikit-learn matplotlib seaborn pandas numpy")
    sys.exit(1)

# ChromaDBé–¢é€£
try:
    from langchain_chroma import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError as e:
    print(f"âŒ ChromaDBé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {e}")
    sys.exit(1)

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®çµ¶å¯¾ãƒ‘ã‚¹å–å¾—
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
DB_PATH = os.path.join(PROJECT_ROOT, 'db')
CHROMA_DB_PATH = os.path.join(DB_PATH, 'chroma_store')
CONVERSATION_DB_PATH = os.path.join(DB_PATH, 'conversation_history.db')
MODELS_PATH = os.path.join(PROJECT_ROOT, 'ml_models')

class UmaMLTrainingSystem:
    """
    Uma3 æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
    """

    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸ¤– Uma3 æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(MODELS_PATH, exist_ok=True)

        # ãƒ‡ãƒ¼ã‚¿æ ¼ç´ç”¨
        self.chroma_data = []
        self.conversation_data = []
        self.features = None
        self.labels = None

        # ãƒ¢ãƒ‡ãƒ«
        self.vectorizer = None
        self.scaler = None
        self.classifier = None
        self.cluster_model = None

    def load_chroma_data(self) -> bool:
        """ChromaDBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ“Š ChromaDBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            embedding_model = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )

            # ChromaDBæ¥ç¶š
            vector_db = Chroma(
                persist_directory=CHROMA_DB_PATH,
                embedding_function=embedding_model
            )

            # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            collection = vector_db._collection
            all_data = collection.get()

            if not all_data['documents']:
                print("âš ï¸ ChromaDBã«ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False

            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
            for i, (doc, metadata, embedding) in enumerate(zip(
                all_data['documents'],
                all_data['metadatas'],
                all_data.get('embeddings', [])
            )):
                self.chroma_data.append({
                    'id': i,
                    'document': doc,
                    'metadata': metadata or {},
                    'embedding': embedding
                })

            print(f"âœ… ChromaDBã‹ã‚‰ {len(self.chroma_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")
            return True

        except Exception as e:
            print(f"âŒ ChromaDBãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def load_conversation_data(self) -> bool:
        """ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ’¬ ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

            if not os.path.exists(CONVERSATION_DB_PATH):
                print("âš ï¸ ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False

            # SQLiteæ¥ç¶š
            conn = sqlite3.connect(CONVERSATION_DB_PATH)
            cursor = conn.cursor()

            # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            print(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«: {[table[0] for table in tables]}")

            # ä¼šè©±ãƒ‡ãƒ¼ã‚¿å–å¾—
            try:
                cursor.execute("""
                    SELECT * FROM conversation_history
                    ORDER BY timestamp DESC
                    LIMIT 1000
                """)
                rows = cursor.fetchall()

                # ã‚«ãƒ©ãƒ åå–å¾—
                cursor.execute("PRAGMA table_info(conversation_history)")
                columns = [column[1] for column in cursor.fetchall()]

                # ãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
                for row in rows:
                    row_dict = dict(zip(columns, row))
                    self.conversation_data.append(row_dict)

                print(f"âœ… ä¼šè©±å±¥æ­´ã‹ã‚‰ {len(self.conversation_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")

            except sqlite3.OperationalError as e:
                print(f"âš ï¸ ä¼šè©±å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
                # ä»£æ›¿ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
                for table_name in [table[0] for table in tables]:
                    try:
                        cursor.execute(f"SELECT * FROM {table_name} LIMIT 5")
                        sample_data = cursor.fetchall()
                        print(f"ğŸ“Š {table_name} ã‚µãƒ³ãƒ—ãƒ«: {len(sample_data)} ä»¶")
                    except:
                        continue

            conn.close()
            return True

        except Exception as e:
            print(f"âŒ ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def prepare_features(self) -> bool:
        """æ©Ÿæ¢°å­¦ç¿’ç”¨ç‰¹å¾´é‡ã‚’æº–å‚™"""
        try:
            print("ğŸ”§ æ©Ÿæ¢°å­¦ç¿’ç”¨ç‰¹å¾´é‡ã‚’æº–å‚™ä¸­...")

            if not self.chroma_data:
                print("âŒ ChromaDBãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
                return False

            # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
            documents = [item['document'] for item in self.chroma_data]

            # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
            self.vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=None,  # æ—¥æœ¬èªå¯¾å¿œã®ãŸã‚
                ngram_range=(1, 2)
            )

            tfidf_features = self.vectorizer.fit_transform(documents)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º
            metadata_features = []
            for item in self.chroma_data:
                metadata = item['metadata']
                features = [
                    len(item['document']),  # æ–‡æ›¸é•·
                    1 if metadata.get('category') == 'ãƒãƒ¼ãƒ æ§‹æˆ' else 0,  # ã‚«ãƒ†ã‚´ãƒª
                    1 if metadata.get('grade') == '3å¹´ç”Ÿ' else 0,  # å­¦å¹´
                    1 if 'ç¿”å¹³' in item['document'] else 0,  # ç‰¹å®šé¸æ‰‹å
                    1 if 'è¡å¤ª' in item['document'] else 0,
                    1 if 'å‹˜å¤ª' in item['document'] else 0,
                    1 if 'è³ªå•' in item['document'] else 0,  # è³ªå•ã‚¿ã‚¤ãƒ—
                    1 if 'å›ç­”' in item['document'] else 0   # å›ç­”ã‚¿ã‚¤ãƒ—
                ]
                metadata_features.append(features)

            metadata_features = np.array(metadata_features)

            # ç‰¹å¾´é‡çµåˆ
            self.features = np.hstack([
                tfidf_features.toarray(),
                metadata_features
            ])

            # ãƒ©ãƒ™ãƒ«æº–å‚™ï¼ˆã‚«ãƒ†ã‚´ãƒªåˆ†é¡ç”¨ï¼‰
            self.labels = []
            for item in self.chroma_data:
                category = item['metadata'].get('category', 'ãã®ä»–')
                if category == 'ãƒãƒ¼ãƒ æ§‹æˆ':
                    self.labels.append(0)
                elif category == 'FAQ':
                    self.labels.append(1)
                elif category == 'é¸æ‰‹æƒ…å ±':
                    self.labels.append(2)
                else:
                    self.labels.append(3)

            self.labels = np.array(self.labels)

            print(f"âœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†: {self.features.shape}, ãƒ©ãƒ™ãƒ«æ•°: {len(np.unique(self.labels))}")
            return True

        except Exception as e:
            print(f"âŒ ç‰¹å¾´é‡æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def train_classification_model(self) -> bool:
        """åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        try:
            print("ğŸ¯ åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")

            if self.features is None or self.labels is None:
                print("âŒ ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ãŒå¿…è¦ã§ã™")
                return False

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                self.features, self.labels,
                test_size=0.3,
                random_state=42,
                stratify=self.labels
            )

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            self.scaler = StandardScaler()
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)

            # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§æ¯”è¼ƒ
            models = {
                'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
                'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
            }

            best_model = None
            best_score = 0

            for name, model in models.items():
                print(f"ğŸ“Š {name} ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

                model.fit(X_train_scaled, y_train)
                predictions = model.predict(X_test_scaled)
                score = accuracy_score(y_test, predictions)

                print(f"âœ… {name} ç²¾åº¦: {score:.4f}")
                print("ğŸ“‹ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
                print(classification_report(y_test, predictions))

                if score > best_score:
                    best_score = score
                    best_model = model
                    self.classifier = model

            print(f"ğŸ† æœ€é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«: {best_score:.4f}")

            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_file = os.path.join(MODELS_PATH, 'classification_model.pkl')
            scaler_file = os.path.join(MODELS_PATH, 'scaler.pkl')
            vectorizer_file = os.path.join(MODELS_PATH, 'vectorizer.pkl')

            with open(model_file, 'wb') as f:
                pickle.dump(self.classifier, f)
            with open(scaler_file, 'wb') as f:
                pickle.dump(self.scaler, f)
            with open(vectorizer_file, 'wb') as f:
                pickle.dump(self.vectorizer, f)

            print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {MODELS_PATH}")
            return True

        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def train_clustering_model(self) -> bool:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        try:
            print("ğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")

            if self.features is None:
                print("âŒ ç‰¹å¾´é‡ãŒå¿…è¦ã§ã™")
                return False

            # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            if self.scaler is None:
                self.scaler = StandardScaler()
                features_scaled = self.scaler.fit_transform(self.features)
            else:
                features_scaled = self.scaler.transform(self.features)

            # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            n_clusters = min(5, len(np.unique(self.labels)))  # æœ€å¤§5ã‚¯ãƒ©ã‚¹ã‚¿
            self.cluster_model = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = self.cluster_model.fit_predict(features_scaled)

            # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ
            print("ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æçµæœ:")
            for i in range(n_clusters):
                cluster_docs = [self.chroma_data[j]['document'] for j, label in enumerate(cluster_labels) if label == i]
                print(f"ã‚¯ãƒ©ã‚¹ã‚¿ {i}: {len(cluster_docs)} ä»¶")
                if cluster_docs:
                    print(f"  ã‚µãƒ³ãƒ—ãƒ«: {cluster_docs[0][:100]}...")

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            cluster_file = os.path.join(MODELS_PATH, 'cluster_model.pkl')
            with open(cluster_file, 'wb') as f:
                pickle.dump(self.cluster_model, f)

            print("âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
            return True

        except Exception as e:
            print(f"âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def generate_training_report(self):
        """è¨“ç·´çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            print("ğŸ“ˆ è¨“ç·´çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")

            report = {
                'timestamp': datetime.now().isoformat(),
                'data_summary': {
                    'chroma_documents': len(self.chroma_data),
                    'conversation_records': len(self.conversation_data),
                    'feature_dimensions': self.features.shape if self.features is not None else None,
                    'unique_labels': len(np.unique(self.labels)) if self.labels is not None else None
                },
                'models_trained': {
                    'classification': self.classifier is not None,
                    'clustering': self.cluster_model is not None,
                    'vectorizer': self.vectorizer is not None,
                    'scaler': self.scaler is not None
                }
            }

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = os.path.join(MODELS_PATH, 'training_report.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            print("âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
            print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {report_file}")

            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            print("\n" + "="*50)
            print("ğŸ‰ æ©Ÿæ¢°å­¦ç¿’è¨“ç·´å®Œäº†ã‚µãƒãƒªãƒ¼")
            print("="*50)
            print(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {report['data_summary']['chroma_documents']} ä»¶")
            print(f"ğŸ’¬ ä¼šè©±å±¥æ­´æ•°: {report['data_summary']['conversation_records']} ä»¶")
            print(f"ğŸ”§ ç‰¹å¾´é‡æ¬¡å…ƒ: {report['data_summary']['feature_dimensions']}")
            print(f"ğŸ·ï¸ ãƒ©ãƒ™ãƒ«ç¨®é¡: {report['data_summary']['unique_labels']} ç¨®é¡")
            print(f"ğŸ’¾ ä¿å­˜å ´æ‰€: {MODELS_PATH}")

        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    def run_full_training(self):
        """å®Œå…¨ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ æ©Ÿæ¢°å­¦ç¿’è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")

        # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_chroma_data():
            print("âŒ ChromaDBãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—")
            return False

        if not self.load_conversation_data():
            print("âš ï¸ ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—ï¼ˆç¶™ç¶šï¼‰")

        # Step 2: ç‰¹å¾´é‡æº–å‚™
        if not self.prepare_features():
            print("âŒ ç‰¹å¾´é‡æº–å‚™å¤±æ•—")
            return False

        # Step 3: ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        classification_success = self.train_classification_model()
        clustering_success = self.train_clustering_model()

        if not (classification_success or clustering_success):
            print("âŒ ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãŒå¤±æ•—")
            return False

        # Step 4: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_training_report()

        print("ğŸ‰ æ©Ÿæ¢°å­¦ç¿’è¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ï¼")
        return True

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("ğŸ¤– Uma3 æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    ml_system = UmaMLTrainingSystem()

    # å®Œå…¨è¨“ç·´å®Ÿè¡Œ
    success = ml_system.run_full_training()

    if success:
        print("\nâœ… æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {MODELS_PATH}")
        print("ğŸ”® ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®åˆ†é¡ã‚„äºˆæ¸¬ãŒå¯èƒ½ã§ã™ã€‚")
    else:
        print("\nâŒ æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return 1

    return 0

if __name__ == "__main__":
    sys.exit(main())

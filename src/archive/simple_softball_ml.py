#!/usr/bin/env python3
"""
ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«ãƒãƒ¼ãƒ ç°¡æ˜“æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

ä½œæˆã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦åŸºæœ¬çš„ãªæ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹
è»½é‡ç‰ˆ - åŸºæœ¬çš„ãªåˆ†é¡ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«é›†ä¸­
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

# åŸºæœ¬çš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
import joblib

class SimpleSoftballMLTrainer:
    """ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«ç°¡æ˜“æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_dir: str):
        """åˆæœŸåŒ–"""
        self.data_dir = data_dir
        self.df = None
        self.models = {}
        self.encoders = {}
        self.vectorizer = None

        print("ğŸ¤– ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«ç°¡æ˜“æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def load_data(self) -> bool:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        csv_file = os.path.join(self.data_dir, "softball_learning_data.csv")

        if not os.path.exists(csv_file):
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
            return False

        try:
            self.df = pd.read_csv(csv_file)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df)}ä»¶")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†"""
        print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ä¸­...")

        # æ¬ æå€¤å‡¦ç†
        self.df['content'] = self.df['content'].fillna('')
        self.df['user'] = self.df['user'].fillna('unknown')

        # ã‚«ãƒ†ã‚´ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.encoders['category'] = LabelEncoder()
        self.df['category_encoded'] = self.encoders['category'].fit_transform(self.df['category'])

        print(f"âœ… ã‚«ãƒ†ã‚´ãƒªæ•°: {len(self.encoders['category'].classes_)}")
        print(f"ğŸ“Š ã‚«ãƒ†ã‚´ãƒª: {list(self.encoders['category'].classes_)}")

    def train_category_classifier(self) -> Dict[str, float]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        print("\nğŸ¯ ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")

        # TF-IDFç‰¹å¾´é‡ä½œæˆ
        self.vectorizer = TfidfVectorizer(
            max_features=500,  # ç‰¹å¾´é‡æ•°ã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–
            ngram_range=(1, 1),  # unigramã®ã¿
            min_df=2
        )

        X = self.vectorizer.fit_transform(self.df['content'])
        y = self.df['category_encoded']

        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape[0]}ä»¶, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape[0]}ä»¶")

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§è¨“ç·´
        model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=1)
        model.fit(X_train, y_train)

        # è©•ä¾¡
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        self.models['category_classifier'] = model

        print(f"âœ… ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ç²¾åº¦: {accuracy:.3f}")

        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        print("\nğŸ“Š è©³ç´°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ:")
        report = classification_report(y_test, y_pred,
                                     target_names=self.encoders['category'].classes_,
                                     output_dict=True)

        for category, metrics in report.items():
            if isinstance(metrics, dict) and 'precision' in metrics:
                print(f"   {category}: ç²¾åº¦={metrics['precision']:.3f}, "
                      f"å†ç¾ç‡={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}")

        return {'accuracy': accuracy, 'report': report}

    def analyze_data_patterns(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        print("\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æä¸­...")

        patterns = {}

        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        category_dist = self.df['category'].value_counts()
        patterns['category_distribution'] = category_dist.to_dict()

        print("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
        for category, count in category_dist.items():
            percentage = (count / len(self.df)) * 100
            print(f"   {category}: {count}ä»¶ ({percentage:.1f}%)")

        # é¸æ‰‹è¨€åŠãƒ‘ã‚¿ãƒ¼ãƒ³
        player_mentions = {}
        for _, row in self.df.iterrows():
            players_str = str(row['players_mentioned'])
            if players_str and players_str != 'nan' and players_str != '':
                players = [p.strip() for p in players_str.split(',')]
                for player in players:
                    if player:
                        player_mentions[player] = player_mentions.get(player, 0) + 1

        # ä¸Šä½10é¸æ‰‹
        top_players = dict(sorted(player_mentions.items(), key=lambda x: x[1], reverse=True)[:10])
        patterns['top_mentioned_players'] = top_players

        print("\nğŸ‘¥ ã‚ˆãè¨€åŠã•ã‚Œã‚‹é¸æ‰‹ (ä¸Šä½5å):")
        for player, count in list(top_players.items())[:5]:
            print(f"   {player}: {count}å›")

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã®çµ±è¨ˆ
        msg_length_stats = {
            'mean': float(self.df['message_length'].mean()),
            'median': float(self.df['message_length'].median()),
            'max': int(self.df['message_length'].max()),
            'min': int(self.df['message_length'].min())
        }
        patterns['message_length_stats'] = msg_length_stats

        print(f"\nğŸ“ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·çµ±è¨ˆ:")
        print(f"   å¹³å‡: {msg_length_stats['mean']:.1f}æ–‡å­—")
        print(f"   ä¸­å¤®å€¤: {msg_length_stats['median']:.1f}æ–‡å­—")
        print(f"   æœ€å¤§: {msg_length_stats['max']}æ–‡å­—")
        print(f"   æœ€å°: {msg_length_stats['min']}æ–‡å­—")

        # æ™‚é–“å¸¯åˆ†æï¼ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        if 'hour' in self.df.columns:
            hour_data = self.df[self.df['hour'].notna()]
            if not hour_data.empty:
                hour_dist = hour_data['hour'].value_counts().sort_index()
                patterns['hourly_distribution'] = hour_dist.to_dict()

                print(f"\nâ° æŠ•ç¨¿æ™‚é–“å¸¯ (ä¸Šä½3æ™‚é–“å¸¯):")
                for hour, count in hour_dist.head(3).items():
                    print(f"   {int(hour)}æ™‚: {count}ä»¶")

        return patterns

    def predict_category(self, text: str) -> str:
        """æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’äºˆæ¸¬"""
        if 'category_classifier' not in self.models or self.vectorizer is None:
            return "ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“"

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        text_vector = self.vectorizer.transform([text])

        # äºˆæ¸¬
        prediction = self.models['category_classifier'].predict(text_vector)[0]
        category = self.encoders['category'].inverse_transform([prediction])[0]

        # äºˆæ¸¬ç¢ºç‡
        probabilities = self.models['category_classifier'].predict_proba(text_vector)[0]
        confidence = max(probabilities)

        return f"{category} (ä¿¡é ¼åº¦: {confidence:.3f})"

    def save_results(self, results: Dict[str, Any], output_dir: str):
        """çµæœã®ä¿å­˜"""
        os.makedirs(output_dir, exist_ok=True)

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if 'category_classifier' in self.models:
            model_file = os.path.join(output_dir, "category_classifier.joblib")
            joblib.dump(self.models['category_classifier'], model_file)
            print(f"ğŸ’¾ åˆ†é¡ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

        # ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ä¿å­˜
        if self.vectorizer:
            vec_file = os.path.join(output_dir, "tfidf_vectorizer.joblib")
            joblib.dump(self.vectorizer, vec_file)
            print(f"ğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ä¿å­˜: {vec_file}")

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜
        if self.encoders:
            for name, encoder in self.encoders.items():
                enc_file = os.path.join(output_dir, f"encoder_{name}.joblib")
                joblib.dump(encoder, enc_file)
                print(f"ğŸ’¾ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜: {enc_file}")

        # çµæœãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report = {
            "timestamp": datetime.now().isoformat(),
            "dataset_size": len(self.df),
            "categories": list(self.encoders['category'].classes_),
            "results": results
        }

        report_file = os.path.join(output_dir, "training_results.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“‹ çµæœãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")

    def demo_predictions(self):
        """äºˆæ¸¬ãƒ‡ãƒ¢"""
        print("\nğŸ¯ äºˆæ¸¬ãƒ‡ãƒ¢:")

        demo_texts = [
            "ä»Šæ—¥ã®ç·´ç¿’ãŠç–²ã‚Œæ§˜ã§ã—ãŸ",
            "æ˜æ—¥ã®è©¦åˆé ‘å¼µã‚Šã¾ã—ã‚‡ã†",
            "é›†åˆæ™‚é–“ã¯9æ™‚ã§ã™",
            "æ–°ãã‚“ãƒŠã‚¤ã‚¹ãƒ—ãƒ¬ãƒ¼ã§ã—ãŸ",
            "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ"
        ]

        for text in demo_texts:
            prediction = self.predict_category(text)
            print(f"   ã€Œ{text}ã€ â†’ {prediction}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ¥ ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«ãƒãƒ¼ãƒ ç°¡æ˜“æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    # è¨­å®š
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    data_dir = os.path.join(project_root, "softball_learning_data")
    output_dir = os.path.join(project_root, "ml_results")

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    trainer = SimpleSoftballMLTrainer(data_dir)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if not trainer.load_data():
        return

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    trainer.preprocess_data()

    # çµæœæ ¼ç´ç”¨
    results = {}

    # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    classification_results = trainer.train_category_classifier()
    results['classification'] = classification_results

    # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    pattern_results = trainer.analyze_data_patterns()
    results['patterns'] = pattern_results

    # çµæœä¿å­˜
    trainer.save_results(results, output_dir)

    # äºˆæ¸¬ãƒ‡ãƒ¢
    trainer.demo_predictions()

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ¯ æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Œäº†!")
    print("=" * 60)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚µã‚¤ã‚º: {len(trainer.df)}ä»¶")
    print(f"ğŸ¯ ã‚«ãƒ†ã‚´ãƒªæ•°: {len(trainer.encoders['category'].classes_)}")
    print(f"âœ… åˆ†é¡ç²¾åº¦: {classification_results['accuracy']:.3f}")
    print(f"ğŸ“ å‡ºåŠ›å…ˆ: {output_dir}")
    print(f"   - category_classifier.joblib (åˆ†é¡ãƒ¢ãƒ‡ãƒ«)")
    print(f"   - tfidf_vectorizer.joblib (ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´æŠ½å‡º)")
    print(f"   - training_results.json (è©³ç´°çµæœ)")

if __name__ == "__main__":
    main()

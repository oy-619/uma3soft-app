#!/usr/bin/env python3
"""
ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«ãƒãƒ¼ãƒ æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼

ä½œæˆã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹
- ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ï¼ˆã‚«ãƒ†ã‚´ãƒªäºˆæ¸¬ï¼‰
- æ„Ÿæƒ…åˆ†æ
- é¸æ‰‹è¨€åŠäºˆæ¸¬
- æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
import joblib

# æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
import re

class SoftballMLTrainer:
    """ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, data_dir: str):
        """åˆæœŸåŒ–"""
        self.data_dir = data_dir
        self.df = None
        self.models = {}
        self.encoders = {}
        self.vectorizers = {}
        self.scalers = {}

        # æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã®åˆæœŸåŒ–
        self.mecab = None  # MeCabã¯ä½¿ç”¨ã›ãšã€åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’ä½¿ç”¨
        print("ğŸ“ åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆMeCabãªã—ï¼‰")
        print("ğŸ“ åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆMeCabãªã—ï¼‰")

        print("ğŸ¤– ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«æ©Ÿæ¢°å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def load_data(self) -> bool:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        csv_file = os.path.join(self.data_dir, "softball_learning_data.csv")

        if not os.path.exists(csv_file):
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
            return False

        try:
            self.df = pd.read_csv(csv_file)
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df)}ä»¶")
            print(f"ğŸ“Š ã‚«ãƒ©ãƒ : {list(self.df.columns)}")
            return True
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def preprocess_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
        if pd.isna(text):
            return ""

        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = str(text)
        text = re.sub(r'https?://[^\s]+', '', text)  # URLå‰Šé™¤
        text = re.sub(r'[ã€ã€‘()]', '', text)  # æ‹¬å¼§å‰Šé™¤
        text = re.sub(r'\s+', ' ', text)  # ç©ºç™½æ­£è¦åŒ–

        # MeCabãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯å½¢æ…‹ç´ è§£æ
        if self.mecab:
            try:
                text = self.mecab.parse(text).strip()
            except:
                pass

        return text

    def prepare_features(self) -> Dict[str, Any]:
        """ç‰¹å¾´é‡ã®æº–å‚™"""
        print("ğŸ”§ ç‰¹å¾´é‡ã‚’æº–å‚™ä¸­...")

        # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        self.df['processed_content'] = self.df['content'].apply(self.preprocess_text)

        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.encoders['category'] = LabelEncoder()
        self.df['category_encoded'] = self.encoders['category'].fit_transform(self.df['category'])

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.encoders['user'] = LabelEncoder()
        self.df['user_encoded'] = self.encoders['user'].fit_transform(self.df['user'].fillna('unknown'))

        # TF-IDFç‰¹å¾´é‡
        self.vectorizers['tfidf'] = TfidfVectorizer(
            max_features=1000,
            stop_words=None,  # æ—¥æœ¬èªç”¨ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯åˆ¥é€”è¨­å®š
            ngram_range=(1, 2),
            min_df=2
        )

        tfidf_features = self.vectorizers['tfidf'].fit_transform(self.df['processed_content'])

        # æ•°å€¤ç‰¹å¾´é‡
        numeric_features = [
            'message_length', 'has_question', 'has_exclamation',
            'has_emoji', 'is_weekend', 'hour'
        ]

        # æ¬ æå€¤å‡¦ç†
        for col in numeric_features:
            self.df[col] = pd.to_numeric(self.df[col], errors='coerce').fillna(0)

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        self.scalers['numeric'] = StandardScaler()
        numeric_scaled = self.scalers['numeric'].fit_transform(self.df[numeric_features])

        features = {
            'tfidf': tfidf_features,
            'numeric': numeric_scaled,
            'category_labels': self.df['category_encoded'].values,
            'user_labels': self.df['user_encoded'].values
        }

        print(f"âœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†")
        print(f"   - TF-IDF: {tfidf_features.shape}")
        print(f"   - æ•°å€¤ç‰¹å¾´é‡: {numeric_scaled.shape}")
        print(f"   - ã‚«ãƒ†ã‚´ãƒªæ•°: {len(self.encoders['category'].classes_)}")

        return features

    def train_category_classifier(self, features: Dict[str, Any]) -> Dict[str, float]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        print("\nğŸ¯ ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")

        # TF-IDFç‰¹å¾´é‡ã¨æ•°å€¤ç‰¹å¾´é‡ã‚’çµåˆ
        from scipy.sparse import hstack
        X = hstack([features['tfidf'], features['numeric']])
        y = features['category_labels']

        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
        classifiers = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
            'NaiveBayes': MultinomialNB(),
            'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
            'SVM': SVC(random_state=42, probability=True)
        }

        results = {}
        best_score = 0
        best_model = None

        for name, classifier in classifiers.items():
            # è¨“ç·´
            classifier.fit(X_train, y_train)

            # äºˆæ¸¬
            y_pred = classifier.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)

            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            cv_scores = cross_val_score(classifier, X_train, y_train, cv=5)

            results[name] = {
                'accuracy': accuracy,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }

            print(f"   {name}: ç²¾åº¦={accuracy:.3f}, CV={cv_scores.mean():.3f}Â±{cv_scores.std():.3f}")

            # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²
            if accuracy > best_score:
                best_score = accuracy
                best_model = classifier
                self.models['category_classifier'] = classifier

        # è©³ç´°ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\nğŸ“Š æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡:")
        y_pred_best = best_model.predict(X_test)
        print(classification_report(y_test, y_pred_best,
                                  target_names=self.encoders['category'].classes_))

        return results

    def train_sentiment_analyzer(self, features: Dict[str, Any]) -> Dict[str, float]:
        """æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        print("\nğŸ˜Š æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")

        # æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«ã®ä½œæˆï¼ˆåŸºæœ¬çš„ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        sentiment_labels = []
        for _, row in self.df.iterrows():
            content = str(row['content']).lower()

            # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            positive_words = ['ã‚ã‚ŠãŒã¨ã†', 'æ„Ÿè¬', 'é ‘å¼µ', 'å¿œæ´', 'ç´ æ™´ã‚‰ã—ã„', 'è‰¯ã„', 'æ¥½ã—ã„']
            # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            negative_words = ['æ®‹å¿µ', 'å¿ƒé…', 'ç–²ã‚Œ', 'å›°ã£ãŸ', 'é›£ã—ã„', 'å•é¡Œ']

            pos_count = sum(1 for word in positive_words if word in content)
            neg_count = sum(1 for word in negative_words if word in content)

            if pos_count > neg_count:
                sentiment_labels.append(1)  # ãƒã‚¸ãƒ†ã‚£ãƒ–
            elif neg_count > pos_count:
                sentiment_labels.append(-1)  # ãƒã‚¬ãƒ†ã‚£ãƒ–
            else:
                sentiment_labels.append(0)  # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«

        sentiment_labels = np.array(sentiment_labels)

        # ç‰¹å¾´é‡
        X = features['tfidf']
        y = sentiment_labels

        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã§è¨“ç·´
        sentiment_model = LogisticRegression(random_state=42, max_iter=1000)
        sentiment_model.fit(X_train, y_train)

        # è©•ä¾¡
        y_pred = sentiment_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        self.models['sentiment_analyzer'] = sentiment_model

        print(f"âœ… æ„Ÿæƒ…åˆ†æç²¾åº¦: {accuracy:.3f}")

        # æ„Ÿæƒ…åˆ†å¸ƒ
        sentiment_dist = pd.Series(sentiment_labels).value_counts().sort_index()
        print(f"ğŸ“Š æ„Ÿæƒ…åˆ†å¸ƒ: ãƒã‚¬ãƒ†ã‚£ãƒ–={sentiment_dist.get(-1, 0)}, "
              f"ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«={sentiment_dist.get(0, 0)}, ãƒã‚¸ãƒ†ã‚£ãƒ–={sentiment_dist.get(1, 0)}")

        return {'accuracy': accuracy, 'distribution': sentiment_dist.to_dict()}

    def train_player_mention_predictor(self, features: Dict[str, Any]) -> Dict[str, float]:
        """é¸æ‰‹è¨€åŠäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        print("\nğŸ‘¥ é¸æ‰‹è¨€åŠäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")

        # é¸æ‰‹è¨€åŠãƒã‚¤ãƒŠãƒªãƒ©ãƒ™ãƒ«ã®ä½œæˆ
        has_player_mention = (self.df['players_mentioned'].fillna('').str.len() > 0).astype(int)

        # ç‰¹å¾´é‡
        from scipy.sparse import hstack
        X = hstack([features['tfidf'], features['numeric']])
        y = has_player_mention

        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã§è¨“ç·´
        player_model = RandomForestClassifier(n_estimators=100, random_state=42)
        player_model.fit(X_train, y_train)

        # è©•ä¾¡
        y_pred = player_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        self.models['player_mention_predictor'] = player_model

        print(f"âœ… é¸æ‰‹è¨€åŠäºˆæ¸¬ç²¾åº¦: {accuracy:.3f}")
        print(f"ğŸ“Š é¸æ‰‹è¨€åŠç‡: {has_player_mention.mean():.3f}")

        return {'accuracy': accuracy, 'mention_rate': has_player_mention.mean()}

    def analyze_patterns(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        print("\nğŸ“ˆ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")

        patterns = {}

        # æ™‚é–“å¸¯åˆ¥æŠ•ç¨¿ãƒ‘ã‚¿ãƒ¼ãƒ³
        hour_pattern = self.df.groupby('hour')['category'].value_counts().unstack(fill_value=0)
        patterns['hourly_categories'] = hour_pattern.to_dict()

        # é€±æœ«vså¹³æ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        weekend_pattern = self.df.groupby('is_weekend')['category'].value_counts().unstack(fill_value=0)
        patterns['weekend_categories'] = weekend_pattern.to_dict()

        # é¸æ‰‹åˆ¥è¨€åŠãƒ‘ã‚¿ãƒ¼ãƒ³
        player_mentions = {}
        for _, row in self.df.iterrows():
            players = str(row['players_mentioned']).split(',')
            category = row['category']
            for player in players:
                player = player.strip()
                if player and player != 'nan':
                    if player not in player_mentions:
                        player_mentions[player] = {}
                    player_mentions[player][category] = player_mentions[player].get(category, 0) + 1

        patterns['player_category_mentions'] = player_mentions

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·ã¨ã‚«ãƒ†ã‚´ãƒªã®é–¢ä¿‚
        length_by_category = self.df.groupby('category')['message_length'].agg(['mean', 'std']).to_dict()
        patterns['message_length_by_category'] = length_by_category

        print("âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†")

        return patterns

    def save_models(self, output_dir: str):
        """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        models_dir = os.path.join(output_dir, "trained_models")
        os.makedirs(models_dir, exist_ok=True)

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        for name, model in self.models.items():
            model_file = os.path.join(models_dir, f"{name}.joblib")
            joblib.dump(model, model_file)
            print(f"ğŸ’¾ {name}ã‚’ä¿å­˜: {model_file}")

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ä¿å­˜
        for name, encoder in self.encoders.items():
            encoder_file = os.path.join(models_dir, f"encoder_{name}.joblib")
            joblib.dump(encoder, encoder_file)

        # ãƒ™ã‚¯ã‚¿ãƒ©ã‚¤ã‚¶ãƒ¼ä¿å­˜
        for name, vectorizer in self.vectorizers.items():
            vec_file = os.path.join(models_dir, f"vectorizer_{name}.joblib")
            joblib.dump(vectorizer, vec_file)

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä¿å­˜
        for name, scaler in self.scalers.items():
            scaler_file = os.path.join(models_dir, f"scaler_{name}.joblib")
            joblib.dump(scaler, scaler_file)

        print(f"ğŸ“ å…¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {models_dir}")

    def generate_training_report(self, results: Dict[str, Any], output_dir: str):
        """è¨“ç·´ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = {
            "training_summary": {
                "timestamp": datetime.now().isoformat(),
                "dataset_size": len(self.df),
                "categories": list(self.encoders['category'].classes_),
                "num_categories": len(self.encoders['category'].classes_)
            },
            "model_performance": results,
            "data_statistics": {
                "category_distribution": self.df['category'].value_counts().to_dict(),
                "average_message_length": float(self.df['message_length'].mean()),
                "total_players_mentioned": len(set(
                    [p.strip() for players in self.df['players_mentioned'].fillna('').str.split(',')
                     for p in players if p.strip() and p.strip() != 'nan']
                ))
            }
        }

        report_file = os.path.join(output_dir, "training_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“‹ è¨“ç·´ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        return report

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 70)
    print("ğŸ¤– ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«ãƒãƒ¼ãƒ æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)

    # è¨­å®š
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    data_dir = os.path.join(project_root, "softball_learning_data")
    output_dir = os.path.join(project_root, "ml_outputs")

    os.makedirs(output_dir, exist_ok=True)

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    trainer = SoftballMLTrainer(data_dir)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if not trainer.load_data():
        return

    # ç‰¹å¾´é‡æº–å‚™
    features = trainer.prepare_features()

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    results = {}

    # 1. ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
    results['category_classification'] = trainer.train_category_classifier(features)

    # 2. æ„Ÿæƒ…åˆ†æ
    results['sentiment_analysis'] = trainer.train_sentiment_analyzer(features)

    # 3. é¸æ‰‹è¨€åŠäºˆæ¸¬
    results['player_mention_prediction'] = trainer.train_player_mention_predictor(features)

    # 4. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    results['pattern_analysis'] = trainer.analyze_patterns()

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    trainer.save_models(output_dir)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = trainer.generate_training_report(results, output_dir)

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("ğŸ¯ æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†!")
    print("=" * 70)
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(trainer.df)}ä»¶")
    print(f"ğŸ¯ ã‚«ãƒ†ã‚´ãƒªæ•°: {len(trainer.encoders['category'].classes_)}")
    print(f"ğŸ‘¥ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {len(trainer.encoders['user'].classes_)}")
    print(f"ğŸ“ å‡ºåŠ›å…ˆ: {output_dir}")
    print(f"   - trained_models/ (è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«)")
    print(f"   - training_report.json (è¨“ç·´ãƒ¬ãƒãƒ¼ãƒˆ)")

if __name__ == "__main__":
    main()

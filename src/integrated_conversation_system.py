"""
ChromaDBã¨çµ±åˆã—ãŸä¼šè©±å±¥æ­´ãƒ™ãƒ¼ã‚¹å¿œç­”ã‚·ã‚¹ãƒ†ãƒ 
æ—¢å­˜ã®uma3_chroma_improverã¨æ–°ã—ã„ä¼šè©±å±¥æ­´ç®¡ç†ã‚’çµ±åˆ
"""

import os
from datetime import datetime
from typing import Dict, List, Optional

from conversation_history_manager import ConversationHistoryManager, ConversationContextGenerator
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_huggingface import HuggingFaceEmbeddings  # Railwayè»½é‡åŒ–ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
from uma3_chroma_improver import Uma3ChromaDBImprover


class IntegratedConversationSystem:
    """ChromaDBã¨ä¼šè©±å±¥æ­´ã‚’çµ±åˆã—ãŸå¿œç­”ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self,
                 chroma_persist_directory: str,
                 conversation_db_path: str = "conversation_history.db",
                 embeddings_model = None):

        self.chroma_persist_directory = chroma_persist_directory
        self.conversation_db_path = conversation_db_path

        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embedding_model = embeddings_model or OpenAIEmbeddings()
        # ChromaDBãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self.vector_db = Chroma(
            persist_directory=chroma_persist_directory,
            embedding_function=self.embedding_model
        )

        # ChromaDBç²¾åº¦å‘ä¸Šæ©Ÿèƒ½ã®åˆæœŸåŒ–
        self.chroma_improver = Uma3ChromaDBImprover(self.vector_db)

        # ä¼šè©±å±¥æ­´ç®¡ç†ã®åˆæœŸåŒ–
        self.history_manager = ConversationHistoryManager(conversation_db_path)
        self.context_generator = ConversationContextGenerator(self.history_manager)

        print(f"[INIT] Integrated conversation system initialized")
        print(f"[INIT] ChromaDB: {chroma_persist_directory}")
        print(f"[INIT] ConversationDB: {conversation_db_path}")

    def generate_integrated_response(self, user_id: str, message: str,
                                   llm: ChatOpenAI = None) -> Dict:
        """çµ±åˆã•ã‚ŒãŸå¿œç­”ç”Ÿæˆ

        Args:
            user_id: ãƒ¦ãƒ¼ã‚¶ID
            message: ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            llm: ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«

        Returns:
            Dict: å¿œç­”æƒ…å ±ï¼ˆå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆã€ä½¿ç”¨ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€çµ±è¨ˆãªã©ï¼‰
        """
        if llm is None:
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

        # 0. å¤©æ°—è³ªå•ã®åˆ¤å®šã¨å‡¦ç†
        weather_response = self._check_weather_query(message)
        if weather_response:
            # å¤©æ°—æƒ…å ±ã®å ´åˆã¯å°‚ç”¨ã®å¿œç­”ã‚’è¿”ã™
            self.history_manager.save_conversation(
                user_id, message, weather_response,
                metadata={"query_type": "weather", "timestamp": datetime.now().isoformat()}
            )
            return {
                "response": weather_response,
                "context_used": {"weather": True, "chroma": 0, "history": 0},
                "response_type": "weather_info"
            }

        # 1. ChromaDBã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢
        chroma_results = self.chroma_improver.schedule_aware_search(
            message, k=5, score_threshold=0.5
        )

        # 2. ä¼šè©±å±¥æ­´ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’å–å¾—
        user_profile = self.history_manager.get_user_profile(user_id)
        recent_conversations = self.history_manager.get_recent_conversations(
            user_id, limit=3
        )
        relevant_conversations = self.history_manager.search_conversations(
            user_id, message, limit=2
        )

        # 3. çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        context_parts = []

        # ChromaDBã‹ã‚‰ã®æƒ…å ±
        if chroma_results:
            context_parts.append("**ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±:**")
            for i, doc in enumerate(chroma_results[:3]):
                context_parts.append(f"{i+1}. {doc.page_content[:200]}...")

        # ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±
        if user_profile["interests"]:
            interests_text = "ã€".join(user_profile["interests"][:3])
            context_parts.append(f"\n**{user_id}ã•ã‚“ã®èˆˆå‘³ãƒ»é–¢å¿ƒ:** {interests_text}")

        if user_profile["conversation_count"] > 0:
            context_parts.append(f"**ã“ã‚Œã¾ã§ã®ä¼šè©±å›æ•°:** {user_profile['conversation_count']}å›")

        # æœ€è¿‘ã®ä¼šè©±å±¥æ­´
        if recent_conversations:
            context_parts.append("\n**æœ€è¿‘ã®ä¼šè©±å±¥æ­´:**")
            for human_msg, ai_msg, timestamp in recent_conversations[:2]:
                # timestampãŒæ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨ã€datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                if hasattr(timestamp, 'strftime'):
                    time_str = timestamp.strftime("%m/%d %H:%M")
                else:
                    # æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨ï¼ˆå¿…è¦ã«å¿œã˜ã¦çŸ­ç¸®ï¼‰
                    time_str = str(timestamp)[:16] if len(str(timestamp)) > 16 else str(timestamp)
                context_parts.append(f"[{time_str}] {user_id}: {human_msg[:80]}...")
                context_parts.append(f"[{time_str}] Bot: {ai_msg[:80]}...")

        # é–¢é€£ã™ã‚‹éå»ã®ä¼šè©±
        if relevant_conversations:
            context_parts.append("\n**é–¢é€£ã™ã‚‹éå»ã®ä¼šè©±:**")
            for conv in relevant_conversations:
                if conv["message_type"] == "human":
                    context_parts.append(f"éå»ã®è³ªå•: {conv['content'][:100]}...")

        # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        context_text = "\n".join(context_parts)

        # ä»Šé€±ã®äºˆå®šã«é–¢ã™ã‚‹è³ªå•ã‹ãƒã‚§ãƒƒã‚¯
        is_weekly_schedule_query = any(
            keyword in message for keyword in ["ä»Šé€±", "é€±", "ä»Šé€±ã®äºˆå®š", "é€±ã®äºˆå®š", "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"]
        )

        if is_weekly_schedule_query:
            current_time = datetime.now()
            current_date_str = current_time.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

            prompt_template = ChatPromptTemplate.from_messages([
                (
                    "system",
                    f"""ã‚ãªãŸã¯å„ªç§€ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç®¡ç†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
                    ç¾åœ¨æ™‚åˆ»ã¯{current_date_str}ã§ã™ã€‚

                    ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å€‹äººçš„ã§è¦ªã—ã¿ã‚„ã™ã„å¿œç­”ã‚’ã—ã¦ãã ã•ã„ï¼š

                    {context_text}

                    å›ç­”æ™‚ã®æ³¨æ„ç‚¹:
                    - ãƒ¦ãƒ¼ã‚¶ã®éå»ã®ä¼šè©±ã‚„èˆˆå‘³ã‚’è¸ã¾ãˆãŸå€‹äººçš„ãªå¿œç­”
                    - é•·æœŸçš„ãªé–¢ä¿‚æ€§ã‚’æ„è­˜ã—ãŸè‡ªç„¶ãªä¼šè©±ç¶™ç¶š
                    - ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§èª­ã¿ã‚„ã™ã„ã‚ˆã†é©åº¦ã«æ”¹è¡Œ
                    - å¿…è¦ã«å¿œã˜ã¦éå»ã®ä¼šè©±ã‚’å‚ç…§ã—ãŸç™ºè¨€
                    - äºˆå®šãŒã‚ã‚‹å ´åˆã¯æ—¥ä»˜ãƒ»æ™‚é–“ãƒ»å ´æ‰€ã‚’æ˜ç¢ºã«è¨˜è¼‰"""
                ),
                ("human", "{input}")
            ])
        else:
            prompt_template = ChatPromptTemplate.from_messages([
                (
                    "system",
                    f"""ã‚ãªãŸã¯å„ªç§€ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

                    ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«å€‹äººçš„ã§è¦ªã—ã¿ã‚„ã™ã„å¿œç­”ã‚’ã—ã¦ãã ã•ã„ï¼š

                    {context_text}

                    å›ç­”æ™‚ã®æ³¨æ„ç‚¹:
                    - ãƒ¦ãƒ¼ã‚¶ã®éå»ã®ç™ºè¨€ã‚„èˆˆå‘³ã‚’è¸ã¾ãˆãŸå€‹äººçš„ãªå¿œç­”
                    - é•·æœŸçš„ãªé–¢ä¿‚æ€§ã‚’æ„è­˜ã—ãŸè‡ªç„¶ãªä¼šè©±ç¶™ç¶š
                    - ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§èª­ã¿ã‚„ã™ã„ã‚ˆã†é©åº¦ã«æ”¹è¡Œ
                    - å¿…è¦ã«å¿œã˜ã¦éå»ã®ä¼šè©±ã‚’å‚ç…§ã—ãŸç™ºè¨€
                    - é‡è¦ãªæƒ…å ±ã¯ç®‡æ¡æ›¸ãã§æ•´ç†"""
                ),
                ("human", "{input}")
            ])

        # å¿œç­”ç”Ÿæˆ
        try:
            formatted_prompt = prompt_template.format_messages(input=message)
            response = llm.invoke(formatted_prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)

            # å¿œç­”ã‚’ä¼šè©±å±¥æ­´ã«ä¿å­˜
            self.history_manager.save_conversation(
                user_id, message, response_text,
                metadata={
                    "chroma_results_count": len(chroma_results),
                    "context_quality": "high" if len(context_parts) > 3 else "medium",
                    "response_type": "weekly_schedule" if is_weekly_schedule_query else "general"
                }
            )

            # å¿œç­”æƒ…å ±ã‚’è¿”ã™
            return {
                "response": response_text,
                "context_used": {
                    "chroma_results": len(chroma_results),
                    "conversation_history": len(recent_conversations),
                    "relevant_conversations": len(relevant_conversations),
                    "user_profile": user_profile
                },
                "prompt_length": len(context_text),
                "response_type": "weekly_schedule" if is_weekly_schedule_query else "general"
            }

        except Exception as e:
            error_response = f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            print(f"[ERROR] Response generation failed: {e}")

            # ã‚¨ãƒ©ãƒ¼ã‚‚å±¥æ­´ã«ä¿å­˜
            self.history_manager.save_conversation(
                user_id, message, error_response,
                metadata={"error": True, "error_message": str(e)}
            )

            return {
                "response": error_response,
                "context_used": {},
                "error": True,
                "error_message": str(e)
            }

    def get_user_conversation_summary(self, user_id: str) -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ã®ä¼šè©±ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        profile = self.history_manager.get_user_profile(user_id)
        stats = self.history_manager.get_conversation_statistics(user_id)
        recent_conversations = self.history_manager.get_recent_conversations(user_id, limit=5)

        return {
            "user_id": user_id,
            "profile": profile,
            "statistics": stats,
            "recent_conversations": recent_conversations,
            "summary_generated_at": datetime.now().isoformat()
        }

    def search_user_conversations(self, user_id: str, query: str, limit: int = 10) -> List[Dict]:
        """ãƒ¦ãƒ¼ã‚¶ã®ä¼šè©±å±¥æ­´ã‚’æ¤œç´¢"""
        return self.history_manager.search_conversations(user_id, query, limit)

    def clear_user_history(self, user_id: str, session_id: str = "default") -> bool:
        """ãƒ¦ãƒ¼ã‚¶ã®ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢"""
        try:
            user_history = self.history_manager.get_user_history(user_id, session_id)
            user_history.clear()
            print(f"[CLEAR] Cleared conversation history for user: {user_id}")
            return True
        except Exception as e:
            print(f"[ERROR] Failed to clear history for user {user_id}: {e}")
            return False

    def export_user_conversations(self, user_id: str, output_file: str) -> bool:
        """ãƒ¦ãƒ¼ã‚¶ã®ä¼šè©±å±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            import json

            summary = self.get_user_conversation_summary(user_id)
            all_conversations = self.history_manager.search_conversations(user_id, "", limit=1000)

            export_data = {
                "user_summary": summary,
                "all_conversations": all_conversations,
                "export_timestamp": datetime.now().isoformat()
            }

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            print(f"[EXPORT] User conversations exported to: {output_file}")
            return True

        except Exception as e:
            print(f"[ERROR] Failed to export conversations for user {user_id}: {e}")
            return False

    def _check_weather_query(self, message: str) -> Optional[str]:
        """
        å¤©æ°—ã«é–¢ã™ã‚‹è³ªå•ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã€å¤©æ°—æƒ…å ±ã‚’å–å¾—

        Args:
            message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            å¤©æ°—æƒ…å ±ï¼ˆå¤©æ°—è³ªå•ã§ãªã„å ´åˆã¯Noneï¼‰
        """
        # å¤©æ°—é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        weather_keywords = [
            "å¤©æ°—", "å¤©å€™", "æ°—æ¸©", "é›¨", "æ™´ã‚Œ", "æ›‡ã‚Š", "é›ª",
            "æš‘ã„", "å¯’ã„", "æ¹¿åº¦", "é¢¨", "å°é¢¨", "æ°—è±¡",
            "ä»Šæ—¥ã®å¤©æ°—", "æ˜æ—¥ã®å¤©æ°—", "é€±é–“å¤©æ°—", "weather"
        ]

        message_lower = message.lower()

        # å¤©æ°—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œå‡º
        is_weather_query = any(keyword in message or keyword in message_lower
                              for keyword in weather_keywords)

        if not is_weather_query:
            return None

        try:
            # WeatherContextToolã‚’ä½¿ç”¨ã—ã¦å¤©æ°—æƒ…å ±ã‚’å–å¾—
            from uma3_custom_tools import WeatherContextTool

            weather_tool = WeatherContextTool()
            weather_response = weather_tool._run(message)

            return weather_response

        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬çš„ãªå¤©æ°—æƒ…å ±
            return self._fallback_weather_response(message)

    def _fallback_weather_response(self, message: str) -> str:
        """
        å¤©æ°—æƒ…å ±å–å¾—ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        Args:
            message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            åŸºæœ¬çš„ãªå¤©æ°—æƒ…å ±å¿œç­”
        """
        # åœ°åŸŸã®ç‰¹å®š
        major_cities = ["æ±äº¬", "å¤§é˜ª", "åå¤å±‹", "ç¦å²¡", "æœ­å¹Œ", "ä»™å°", "æ¨ªæµœ", "äº¬éƒ½", "ç¥æˆ¸", "åºƒå³¶"]
        detected_location = "ç¾åœ¨åœ°"

        for city in major_cities:
            if city in message:
                detected_location = city
                break

        current_time = datetime.now()

        return f"""ğŸŒ¤ï¸ **{detected_location}ã®å¤©æ°—æƒ…å ±**

âš ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã‚’ã”ç¢ºèªãã ã•ã„ï¼š
ğŸ”— **MSNå¤©æ°—äºˆå ±**: https://www.msn.com/ja-jp/weather/forecast/in-%E6%9D%B1%E4%BA%AC%E9%83%BD,%E8%B6%B3%E7%AB%8B%E5%8C%BA?loc=eyJsIjoi6Laz56uL5Yy6IiwiciI6IuadseS6rOmDvSIsImMiOiLml6XmnKsiLCJpIjoiSlAiLCJnIjoiamEtanAiLCJ4IjoiMTM5Ljc5NjE4ODM1NDQ5MjIiLCJ5IjoiMzUuNzYxOTU5MDc1OTI3NzM0In0%3D&weadegreetype=C

ğŸ• **ç¢ºèªæ™‚åˆ»**: {current_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}

ğŸ’¡ **ãŠã™ã™ã‚**: å¤–å‡ºå‰ã«æœ€æ–°ã®å¤©æ°—äºˆå ±ã‚’ã”ç¢ºèªãã ã•ã„ï¼
ğŸŒ¡ï¸ ç‰¹ã«æ°—æ¸©å¤‰åŒ–ã‚„é™æ°´ç¢ºç‡ã«ã”æ³¨æ„ãã ã•ã„ã€‚"""


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    # è¨­å®š
    chroma_persist_dir = "Lesson25/uma3soft-app/db/chroma_store"
    conversation_db_path = "conversation_history.db"

    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    integrated_system = IntegratedConversationSystem(
        chroma_persist_directory=chroma_persist_dir,
        conversation_db_path=conversation_db_path
    )

    # ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶
    test_user_id = "integration_test_user"

    # LLMã®åˆæœŸåŒ–ï¼ˆOpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if os.getenv("OPENAI_API_KEY"):
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        test_messages = [
            "ã“ã‚“ã«ã¡ã¯ã€é‡çƒã«èˆˆå‘³ãŒã‚ã‚Šã¾ã™",
            "ä»Šé€±ã®äºˆå®šã‚’æ•™ãˆã¦",
            "ã‚¸ãƒ£ã‚¤ã‚¢ãƒ³ãƒ„ã®è©¦åˆã¯ã„ã¤ã§ã™ã‹ï¼Ÿ"
        ]

        print("=" * 50)
        print("çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
        print("=" * 50)

        for i, message in enumerate(test_messages, 1):
            print(f"\n{i}. ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message}")

            result = integrated_system.generate_integrated_response(
                test_user_id, message, llm
            )

            print(f"å¿œç­”: {result['response'][:150]}...")
            print(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {result['context_used']}")
            print("-" * 30)

        # ãƒ¦ãƒ¼ã‚¶ã‚µãƒãƒªãƒ¼ã®ç¢ºèª
        summary = integrated_system.get_user_conversation_summary(test_user_id)
        print(f"\nãƒ¦ãƒ¼ã‚¶ã‚µãƒãƒªãƒ¼:")
        print(f"ä¼šè©±å›æ•°: {summary['statistics']['total_messages']}")
        print(f"èˆˆå‘³: {summary['profile']['interests']}")

    else:
        print("OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

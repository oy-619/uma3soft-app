#!/usr/bin/env python3
"""
Uma3 æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®åˆ†é¡ãƒ»äºˆæ¸¬

ã€æ©Ÿèƒ½ã€‘
1. æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡äºˆæ¸¬
2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°äºˆæ¸¬
3. é¡ä¼¼æ–‡æ›¸æ¤œç´¢
4. äºˆæ¸¬çµæœã®å¯è¦–åŒ–
"""

import os
import sys
import pickle
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
import json
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®çµ¶å¯¾ãƒ‘ã‚¹å–å¾—
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
MODELS_PATH = os.path.join(PROJECT_ROOT, 'ml_models')

class Uma3MLPredictor:
    """Uma3 æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸ”® Uma3 æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")

        # ãƒ¢ãƒ‡ãƒ«æ ¼ç´ç”¨
        self.classifier = None
        self.cluster_model = None
        self.vectorizer = None
        self.scaler = None

        # ãƒ©ãƒ™ãƒ«å®šç¾©
        self.label_names = {
            0: 'é¸æ‰‹æƒ…å ±',
            1: 'è³ªå•',
            2: 'å›ç­”',
            3: 'ãƒãƒ¼ãƒ æƒ…å ±',
            4: 'ãã®ä»–'
        }

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.load_models()

    def load_models(self) -> bool:
        """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ“¦ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")

            # åˆ†é¡ãƒ¢ãƒ‡ãƒ«
            classification_file = os.path.join(MODELS_PATH, 'classification_model.pkl')
            if os.path.exists(classification_file):
                with open(classification_file, 'rb') as f:
                    self.classifier = pickle.load(f)
                print("âœ… åˆ†é¡ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
            clustering_file = os.path.join(MODELS_PATH, 'clustering_model.pkl')
            if os.path.exists(clustering_file):
                with open(clustering_file, 'rb') as f:
                    self.cluster_model = pickle.load(f)
                print("âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

            # ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
            vectorizer_file = os.path.join(MODELS_PATH, 'vectorizer.pkl')
            if os.path.exists(vectorizer_file):
                with open(vectorizer_file, 'rb') as f:
                    self.vectorizer = pickle.load(f)
                print("âœ… ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")

            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
            scaler_file = os.path.join(MODELS_PATH, 'scaler.pkl')
            if os.path.exists(scaler_file):
                with open(scaler_file, 'rb') as f:
                    self.scaler = pickle.load(f)
                print("âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")

            return True

        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def extract_features(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        try:
            # TF-IDFç‰¹å¾´é‡
            if self.vectorizer:
                tfidf_features = self.vectorizer.transform([text]).toarray()
            else:
                tfidf_features = np.zeros((1, 300))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µã‚¤ã‚º

            # æ‰‹å‹•ç‰¹å¾´é‡
            manual_features = [
                len(text),                                          # æ–‡æ›¸é•·
                len(text.split()),                                 # å˜èªæ•°
                int('ï¼Ÿ' in text or 'Q:' in text),                 # è³ªå•æ–‡
                int('A:' in text or 'å›ç­”' in text),               # å›ç­”æ–‡
                int(any(name in text for name in ['ç¿”å¹³', 'è¡å¤ª', 'å‹˜å¤ª', 'æš–å¤§', 'è‹±æ±°', 'æ‚ ç‰'])), # é¸æ‰‹å
                len([x for x in text if x.isdigit()]),            # æ•°å­—ã®å€‹æ•°
                text.count('ã€'),                                  # èª­ç‚¹
                text.count('ã€‚'),                                  # å¥ç‚¹
                int('ãƒãƒ¼ãƒ ' in text or 'ã‚½ãƒ•ãƒˆ' in text),         # ãƒãƒ¼ãƒ é–¢é€£
                int('ç·´ç¿’' in text or 'è©¦åˆ' in text),             # æ´»å‹•é–¢é€£
            ]

            # ç‰¹å¾´é‡çµåˆ
            features = np.hstack([tfidf_features[0], manual_features])

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            if self.scaler:
                features = self.scaler.transform([features])
                return features[0]
            else:
                return features

        except Exception as e:
            print(f"âŒ ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return np.zeros(310)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µã‚¤ã‚º

    def predict_category(self, text: str) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’äºˆæ¸¬"""
        try:
            if not self.classifier:
                return {'error': 'åˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“'}

            # ç‰¹å¾´é‡æŠ½å‡º
            features = self.extract_features(text)
            features = features.reshape(1, -1)

            # äºˆæ¸¬
            prediction = self.classifier.predict(features)[0]
            probabilities = self.classifier.predict_proba(features)[0]

            # çµæœæ•´ç†
            result = {
                'predicted_category': self.label_names.get(prediction, 'Unknown'),
                'predicted_label': int(prediction),
                'confidence': float(max(probabilities)),
                'all_probabilities': {
                    self.label_names.get(i, f'Label_{i}'): float(prob)
                    for i, prob in enumerate(probabilities)
                },
                'input_text': text,
                'timestamp': datetime.now().isoformat()
            }

            return result

        except Exception as e:
            return {'error': f'äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}'}

    def predict_cluster(self, text: str) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’äºˆæ¸¬"""
        try:
            if not self.cluster_model:
                return {'error': 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“'}

            # ç‰¹å¾´é‡æŠ½å‡º
            features = self.extract_features(text)
            features = features.reshape(1, -1)

            # ã‚¯ãƒ©ã‚¹ã‚¿äºˆæ¸¬
            cluster = self.cluster_model.predict(features)[0]

            # ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒã‹ã‚‰ã®è·é›¢
            distances = self.cluster_model.transform(features)[0]
            closest_distance = min(distances)

            result = {
                'predicted_cluster': int(cluster),
                'distance_to_center': float(closest_distance),
                'all_distances': [float(d) for d in distances],
                'input_text': text,
                'timestamp': datetime.now().isoformat()
            }

            return result

        except Exception as e:
            return {'error': f'ã‚¯ãƒ©ã‚¹ã‚¿äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}'}

    def analyze_text_batch(self, texts: List[str]) -> List[Dict]:
        """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€æ‹¬åˆ†æ"""
        try:
            print(f"ğŸ“Š {len(texts)} ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬åˆ†æä¸­...")

            results = []
            for i, text in enumerate(texts):
                print(f"  å‡¦ç†ä¸­: {i+1}/{len(texts)}")

                # åˆ†é¡äºˆæ¸¬
                category_result = self.predict_category(text)

                # ã‚¯ãƒ©ã‚¹ã‚¿äºˆæ¸¬
                cluster_result = self.predict_cluster(text)

                # çµæœçµ±åˆ
                combined_result = {
                    'text_id': i,
                    'input_text': text,
                    'classification': category_result,
                    'clustering': cluster_result,
                    'analysis_timestamp': datetime.now().isoformat()
                }

                results.append(combined_result)

            print("âœ… ä¸€æ‹¬åˆ†æå®Œäº†")
            return results

        except Exception as e:
            print(f"âŒ ä¸€æ‹¬åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def generate_prediction_report(self, results: List[Dict]) -> str:
        """äºˆæ¸¬çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            print("ğŸ“ˆ äºˆæ¸¬çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

            # çµ±è¨ˆæƒ…å ±
            total_texts = len(results)
            category_counts = {}
            cluster_counts = {}
            confidence_scores = []

            for result in results:
                # ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
                if 'classification' in result and 'predicted_category' in result['classification']:
                    category = result['classification']['predicted_category']
                    category_counts[category] = category_counts.get(category, 0) + 1

                    # ä¿¡é ¼åº¦
                    if 'confidence' in result['classification']:
                        confidence_scores.append(result['classification']['confidence'])

                # ã‚¯ãƒ©ã‚¹ã‚¿çµ±è¨ˆ
                if 'clustering' in result and 'predicted_cluster' in result['clustering']:
                    cluster = result['clustering']['predicted_cluster']
                    cluster_counts[cluster] = cluster_counts.get(cluster, 0) + 1

            # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            report = {
                'analysis_summary': {
                    'total_texts_analyzed': total_texts,
                    'timestamp': datetime.now().isoformat(),
                    'average_confidence': np.mean(confidence_scores) if confidence_scores else 0,
                    'min_confidence': min(confidence_scores) if confidence_scores else 0,
                    'max_confidence': max(confidence_scores) if confidence_scores else 0
                },
                'category_distribution': category_counts,
                'cluster_distribution': cluster_counts,
                'detailed_results': results
            }

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = os.path.join(MODELS_PATH, f'prediction_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")

            # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
            print("\n" + "="*50)
            print("ğŸ“Š äºˆæ¸¬çµæœã‚µãƒãƒªãƒ¼")
            print("="*50)
            print(f"åˆ†æãƒ†ã‚­ã‚¹ãƒˆæ•°: {total_texts}")
            print(f"å¹³å‡ä¿¡é ¼åº¦: {np.mean(confidence_scores):.4f}" if confidence_scores else "ä¿¡é ¼åº¦: N/A")
            print("\nã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
            for category, count in category_counts.items():
                print(f"  {category}: {count} ä»¶ ({count/total_texts*100:.1f}%)")
            print("\nã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒ:")
            for cluster, count in cluster_counts.items():
                print(f"  ã‚¯ãƒ©ã‚¹ã‚¿ {cluster}: {count} ä»¶ ({count/total_texts*100:.1f}%)")

            return report_file

        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return ""

def interactive_prediction_demo():
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–äºˆæ¸¬ãƒ‡ãƒ¢"""
    print("ğŸš€ Uma3 æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢")
    print("=" * 60)

    # äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    predictor = Uma3MLPredictor()

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
    sample_texts = [
        "ç¿”å¹³é¸æ‰‹ã®æˆç¸¾ã‚’æ•™ãˆã¦ãã ã•ã„",
        "Q: æ¬¡ã®ç·´ç¿’ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ",
        "A: ç·´ç¿’ã¯æ¯é€±åœŸæ›œæ—¥ã«å®Ÿæ–½ã•ã‚Œã¾ã™",
        "é¦¬ä¸‰ã‚½ãƒ•ãƒˆã¯ç´ æ™´ã‚‰ã—ã„ãƒãƒ¼ãƒ ã§ã™",
        "è©¦åˆã®çµæœã‚’å ±å‘Šã—ã¾ã™",
        "ï¼“å¹´ç”Ÿã®é¸æ‰‹ã¯6åã„ã¾ã™",
        "ã‚­ãƒ£ãƒ—ãƒ†ãƒ³ã¯èª°ã§ã™ã‹ï¼Ÿ",
        "ç·´ç¿’ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’ç¢ºèªã—ãŸã„",
        "è¡å¤ªé¸æ‰‹ã¯å†…é‡æ‰‹ã§ã™",
        "ãƒãƒ¼ãƒ ã®ç›®æ¨™ã¯çœŒå¤§ä¼šå‡ºå ´ã§ã™"
    ]

    print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ã®äºˆæ¸¬ãƒ†ã‚¹ãƒˆ:")

    # ä¸€æ‹¬åˆ†æå®Ÿè¡Œ
    results = predictor.analyze_text_batch(sample_texts)

    # çµæœè¡¨ç¤º
    print("\nğŸ“Š å€‹åˆ¥äºˆæ¸¬çµæœ:")
    for i, result in enumerate(results):
        print(f"\n--- ãƒ†ã‚­ã‚¹ãƒˆ {i+1} ---")
        print(f"å…¥åŠ›: {result['input_text']}")

        if 'classification' in result and 'predicted_category' in result['classification']:
            print(f"ã‚«ãƒ†ã‚´ãƒª: {result['classification']['predicted_category']} (ä¿¡é ¼åº¦: {result['classification']['confidence']:.4f})")

        if 'clustering' in result and 'predicted_cluster' in result['clustering']:
            print(f"ã‚¯ãƒ©ã‚¹ã‚¿: {result['clustering']['predicted_cluster']} (è·é›¢: {result['clustering']['distance_to_center']:.4f})")

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_file = predictor.generate_prediction_report(results)

    print(f"\nğŸ‰ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¢å®Œäº†!")
    print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")

    return True

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 70)
    print("ğŸ”® Uma3 æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 70)

    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    success = interactive_prediction_demo()

    if success:
        print("\nâœ… æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å‹•ä½œã—ã¾ã—ãŸ!")
        print("ğŸ”® æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°äºˆæ¸¬ãŒå¯èƒ½ã§ã™")
        return 0
    else:
        print("\nâŒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        return 1

if __name__ == "__main__":
    sys.exit(main())

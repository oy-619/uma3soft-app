#!/usr/bin/env python3
"""
Uma3 æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ï¼ˆè»½é‡ãƒ»ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç‰ˆï¼‰
ChromaDBã¨ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæ©Ÿæ¢°å­¦ç¿’ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ä¸è¦ç‰ˆï¼‰

ã€å®Ÿè£…ãƒ¢ãƒ‡ãƒ«ã€‘
1. TF-IDFãƒ™ãƒ¼ã‚¹ã®æ–‡æ›¸åˆ†é¡
2. çµ±è¨ˆçš„ç‰¹å¾´é‡ã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
3. ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
4. äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
"""

import os
import sys
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import json
import pickle
import re
from collections import Counter

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®çµ¶å¯¾ãƒ‘ã‚¹å–å¾—
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
DB_PATH = os.path.join(PROJECT_ROOT, 'db')
CHROMA_DB_PATH = os.path.join(DB_PATH, 'chroma_store')
CONVERSATION_DB_PATH = os.path.join(DB_PATH, 'conversation_history.db')
MODELS_PATH = os.path.join(PROJECT_ROOT, 'ml_models')

class Uma3OfflineMLSystem:
    """Uma3 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸ¤– Uma3 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(MODELS_PATH, exist_ok=True)

        # ãƒ‡ãƒ¼ã‚¿æ ¼ç´ç”¨
        self.raw_documents = []
        self.conversation_data = []
        self.processed_features = None
        self.labels = None

        # ãƒ¢ãƒ‡ãƒ«æ ¼ç´ç”¨
        self.models = {}
        self.vectorizers = {}
        self.scalers = {}

        # çµæœæ ¼ç´ç”¨
        self.results = {
            'model_performance': {},
            'data_insights': {},
            'predictions': {}
        }

    def load_chroma_data_direct(self) -> bool:
        """ChromaDBã‹ã‚‰ç›´æ¥SQLiteã‚’èª­ã‚“ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            print("ğŸ“Š ChromaDBã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

            chroma_db_file = os.path.join(CHROMA_DB_PATH, 'chroma.sqlite3')
            if not os.path.exists(chroma_db_file):
                print(f"âŒ ChromaDBãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {chroma_db_file}")
                return False

            # SQLiteç›´æ¥æ¥ç¶š
            conn = sqlite3.connect(chroma_db_file)
            cursor = conn.cursor()

            # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            print(f"ğŸ“‹ ChromaDBãƒ†ãƒ¼ãƒ–ãƒ«: {[table[0] for table in tables]}")

            # ãƒ‡ãƒ¼ã‚¿å–å¾—è©¦è¡Œ
            documents_found = False

            # ä¸€èˆ¬çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«åã‚’è©¦è¡Œ
            possible_tables = ['embedding_fulltext_search_data', 'embeddings', 'documents', 'collections']

            for table_name in [table[0] for table in tables]:
                try:
                    cursor.execute(f"PRAGMA table_info({table_name})")
                    columns = [col[1] for col in cursor.fetchall()]
                    print(f"ğŸ“Š {table_name} ã‚«ãƒ©ãƒ : {columns}")

                    if 'string_value' in columns or 'document' in columns or 'content' in columns:
                        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        if 'string_value' in columns:
                            cursor.execute(f"SELECT string_value FROM {table_name} LIMIT 100")
                        elif 'document' in columns:
                            cursor.execute(f"SELECT document FROM {table_name} LIMIT 100")
                        elif 'content' in columns:
                            cursor.execute(f"SELECT content FROM {table_name} LIMIT 100")

                        rows = cursor.fetchall()
                        for row in rows:
                            if row[0] and len(str(row[0]).strip()) > 10:  # ç©ºã§ãªã„æœ‰åŠ¹ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
                                self.raw_documents.append({
                                    'document': str(row[0]),
                                    'source_table': table_name,
                                    'doc_length': len(str(row[0])),
                                    'word_count': len(str(row[0]).split())
                                })
                                documents_found = True

                except Exception as e:
                    print(f"âš ï¸ {table_name} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            conn.close()

            if documents_found:
                print(f"âœ… ChromaDBã‹ã‚‰ {len(self.raw_documents)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—")
            else:
                print("âš ï¸ ChromaDBã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                self.create_sample_documents()

            return True

        except Exception as e:
            print(f"âŒ ChromaDBç›´æ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ç¶šè¡Œ
            self.create_sample_documents()
            return True

    def create_sample_documents(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ"""
        print("ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆä¸­...")

        sample_docs = [
            "ï¼“å¹´ç”Ÿã®é¸æ‰‹ã¯ç¿”å¹³ã€è¡å¤ªã€å‹˜å¤ªã€æš–å¤§ã€è‹±æ±°ã€æ‚ ç‰ã®6åã§ã™ã€‚",
            "Q: ã‚­ãƒ£ãƒ—ãƒ†ãƒ³ã¯èª°ã§ã™ã‹ï¼Ÿ A: ã‚­ãƒ£ãƒ—ãƒ†ãƒ³ã¯ã¾ã ç™ºè¡¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
            "ãƒãƒ¼ãƒ ç·´ç¿’ã¯æ¯é€±åœŸæ›œæ—¥ã¨æ—¥æ›œæ—¥ã«å®Ÿæ–½ã•ã‚Œã¾ã™ã€‚",
            "ç¿”å¹³é¸æ‰‹ã¯æŠ•æ‰‹ã¨ã—ã¦æ´»èºã—ã¦ã„ã¾ã™ã€‚",
            "è¡å¤ªé¸æ‰‹ã¯å†…é‡æ‰‹ã§ãƒãƒ¼ãƒ ã®è¦ã§ã™ã€‚",
            "å‹˜å¤ªé¸æ‰‹ã¯ã‚­ãƒ£ãƒƒãƒãƒ£ãƒ¼ã¨ã—ã¦é ¼ã‚Šã«ãªã‚Šã¾ã™ã€‚",
            "æš–å¤§é¸æ‰‹ã¯å¤–é‡æ‰‹ã§ä¿Šè¶³ãŒç‰¹å¾´ã§ã™ã€‚",
            "è‹±æ±°é¸æ‰‹ã¯å†…é‡æ‰‹ã§å®ˆå‚™ãŒä¸Šæ‰‹ã§ã™ã€‚",
            "æ‚ ç‰é¸æ‰‹ã¯æŠ•æ‰‹ã§åˆ¶çƒåŠ›ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãƒãƒ¼ãƒ ã®ç›®æ¨™ã¯çœŒå¤§ä¼šå‡ºå ´ã§ã™ã€‚",
            "ç·´ç¿’ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«ã¯åŸºç¤ç·´ç¿’ã¨å®Ÿæˆ¦ç·´ç¿’ãŒã‚ã‚Šã¾ã™ã€‚",
            "é¦¬ä¸‰ã‚½ãƒ•ãƒˆã¯åœ°åŸŸã®å°‘å¹´ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«ãƒãƒ¼ãƒ ã§ã™ã€‚",
            "è©¦åˆã¯æ¯æœˆç¬¬2ãƒ»ç¬¬4æ—¥æ›œæ—¥ã«é–‹å‚¬ã•ã‚Œã¾ã™ã€‚",
            "ä¿è­·è€…ã®çš†æ§˜ã®å¿œæ´ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚",
            "æ–°ãƒ¡ãƒ³ãƒãƒ¼ã®å‹Ÿé›†ã‚‚è¡Œã£ã¦ã„ã¾ã™ã€‚"
        ]

        for i, doc in enumerate(sample_docs):
            self.raw_documents.append({
                'document': doc,
                'source_table': 'sample_data',
                'doc_length': len(doc),
                'word_count': len(doc.split())
            })

        print(f"âœ… {len(self.raw_documents)} ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ")

    def load_conversation_data(self) -> bool:
        """ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ’¬ ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

            if not os.path.exists(CONVERSATION_DB_PATH):
                print("âš ï¸ ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ç¶šè¡Œ")
                self.create_sample_conversation_data()
                return True

            conn = sqlite3.connect(CONVERSATION_DB_PATH)
            cursor = conn.cursor()

            # ä¼šè©±ãƒ‡ãƒ¼ã‚¿å–å¾—
            cursor.execute("""
                SELECT user_id, message_type, content, timestamp, session_id
                FROM conversations
                ORDER BY timestamp DESC
                LIMIT 1000
            """)

            rows = cursor.fetchall()
            columns = ['user_id', 'message_type', 'content', 'timestamp', 'session_id']

            for row in rows:
                row_dict = dict(zip(columns, row))

                # ç‰¹å¾´é‡è¿½åŠ 
                content = row_dict['content'] or ''
                row_dict.update({
                    'content_length': len(content),
                    'word_count': len(content.split()),
                    'has_mention': '@' in content,
                    'has_question': 'ï¼Ÿ' in content or '?' in content,
                    'has_exclamation': 'ï¼' in content or '!' in content,
                    'sentiment_positive': any(word in content for word in ['ã‚ã‚ŠãŒã¨ã†', 'å¬‰ã—ã„', 'è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„']),
                    'sentiment_negative': any(word in content for word in ['å›°ã‚‹', 'æ‚ªã„', 'ã ã‚', 'å•é¡Œ']),
                    'is_human': row_dict['message_type'] == 'human',
                    'is_bot': row_dict['message_type'] == 'ai'
                })

                self.conversation_data.append(row_dict)

            conn.close()
            print(f"âœ… ä¼šè©±å±¥æ­´ã‹ã‚‰ {len(self.conversation_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")
            return True

        except Exception as e:
            print(f"âŒ ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.create_sample_conversation_data()
            return True

    def create_sample_conversation_data(self):
        """ã‚µãƒ³ãƒ—ãƒ«ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        print("ğŸ’­ ã‚µãƒ³ãƒ—ãƒ«ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...")

        sample_conversations = [
            {'user_id': 'user1', 'message_type': 'human', 'content': 'ã‚­ãƒ£ãƒ—ãƒ†ãƒ³ã¯èª°ã§ã™ã‹ï¼Ÿ', 'timestamp': '2025-10-29 10:00:00', 'session_id': 'session1'},
            {'user_id': 'user1', 'message_type': 'ai', 'content': 'ã‚­ãƒ£ãƒ—ãƒ†ãƒ³ã¯ã¾ã ç™ºè¡¨ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚', 'timestamp': '2025-10-29 10:00:01', 'session_id': 'session1'},
            {'user_id': 'user2', 'message_type': 'human', 'content': 'ï¼“å¹´ç”Ÿã®é¸æ‰‹ã‚’æ•™ãˆã¦', 'timestamp': '2025-10-29 10:01:00', 'session_id': 'session2'},
            {'user_id': 'user2', 'message_type': 'ai', 'content': 'ï¼“å¹´ç”Ÿã¯ç¿”å¹³ã€è¡å¤ªã€å‹˜å¤ªã€æš–å¤§ã€è‹±æ±°ã€æ‚ ç‰ã®6åã§ã™ã€‚', 'timestamp': '2025-10-29 10:01:01', 'session_id': 'session2'},
        ]

        for conv in sample_conversations:
            content = conv['content'] or ''
            conv.update({
                'content_length': len(content),
                'word_count': len(content.split()),
                'has_mention': '@' in content,
                'has_question': 'ï¼Ÿ' in content or '?' in content,
                'has_exclamation': 'ï¼' in content or '!' in content,
                'sentiment_positive': any(word in content for word in ['ã‚ã‚ŠãŒã¨ã†', 'å¬‰ã—ã„', 'è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„']),
                'sentiment_negative': any(word in content for word in ['å›°ã‚‹', 'æ‚ªã„', 'ã ã‚', 'å•é¡Œ']),
                'is_human': conv['message_type'] == 'human',
                'is_bot': conv['message_type'] == 'ai'
            })
            self.conversation_data.append(conv)

        print(f"âœ… {len(self.conversation_data)} ä»¶ã®ã‚µãƒ³ãƒ—ãƒ«ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ")

    def prepare_features_and_labels(self) -> bool:
        """æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’æº–å‚™"""
        try:
            print("ğŸ”§ ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’æº–å‚™ä¸­...")

            if not self.raw_documents:
                print("âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
                return False

            # ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ï¼ˆTF-IDFï¼‰
            documents = [doc['document'] for doc in self.raw_documents]

            self.vectorizers['tfidf'] = TfidfVectorizer(
                max_features=300,
                ngram_range=(1, 2),
                min_df=1,
                stop_words=None  # æ—¥æœ¬èªå¯¾å¿œ
            )

            tfidf_features = self.vectorizers['tfidf'].fit_transform(documents)

            # æ‰‹å‹•ç‰¹å¾´é‡æŠ½å‡º
            manual_features = []
            labels = []

            for doc in self.raw_documents:
                content = doc['document']

                # æ‰‹å‹•ç‰¹å¾´é‡
                features = [
                    doc['doc_length'],                                    # æ–‡æ›¸é•·
                    doc['word_count'],                                   # å˜èªæ•°
                    int('ï¼Ÿ' in content or 'Q:' in content),             # è³ªå•æ–‡
                    int('A:' in content or 'å›ç­”' in content),           # å›ç­”æ–‡
                    int(any(name in content for name in ['ç¿”å¹³', 'è¡å¤ª', 'å‹˜å¤ª', 'æš–å¤§', 'è‹±æ±°', 'æ‚ ç‰'])),  # é¸æ‰‹å
                    len(re.findall(r'[0-9]+', content)),                 # æ•°å­—ã®å€‹æ•°
                    content.count('ã€'),                                # èª­ç‚¹
                    content.count('ã€‚'),                                # å¥ç‚¹
                    int('ãƒãƒ¼ãƒ ' in content or 'ã‚½ãƒ•ãƒˆ' in content),      # ãƒãƒ¼ãƒ é–¢é€£
                    int('ç·´ç¿’' in content or 'è©¦åˆ' in content),         # æ´»å‹•é–¢é€£
                ]
                manual_features.append(features)

                # ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆå†…å®¹ãƒ™ãƒ¼ã‚¹ï¼‰
                if any(name in content for name in ['ç¿”å¹³', 'è¡å¤ª', 'å‹˜å¤ª', 'æš–å¤§', 'è‹±æ±°', 'æ‚ ç‰']):
                    labels.append(0)  # é¸æ‰‹æƒ…å ±
                elif 'ï¼Ÿ' in content or 'Q:' in content:
                    labels.append(1)  # è³ªå•
                elif 'A:' in content or 'å›ç­”' in content:
                    labels.append(2)  # å›ç­”
                elif 'ãƒãƒ¼ãƒ ' in content or 'ã‚½ãƒ•ãƒˆ' in content:
                    labels.append(3)  # ãƒãƒ¼ãƒ æƒ…å ±
                else:
                    labels.append(4)  # ãã®ä»–

            # ç‰¹å¾´é‡çµåˆ
            manual_features = np.array(manual_features)
            self.processed_features = np.hstack([
                tfidf_features.toarray(),
                manual_features
            ])

            self.labels = np.array(labels)

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            self.scalers['standard'] = StandardScaler()
            self.processed_features = self.scalers['standard'].fit_transform(self.processed_features)

            print(f"âœ… ç‰¹å¾´é‡æº–å‚™å®Œäº†: {self.processed_features.shape}")
            print(f"ğŸ“Š ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {np.bincount(self.labels)}")

            return True

        except Exception as e:
            print(f"âŒ ç‰¹å¾´é‡æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def train_classification_models(self) -> bool:
        """åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        try:
            print("ğŸ¯ åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

            if self.processed_features is None or self.labels is None:
                print("âŒ ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ãŒå¿…è¦ã§ã™")
                return False

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                self.processed_features, self.labels,
                test_size=0.3,
                random_state=42,
                stratify=self.labels if len(np.unique(self.labels)) > 1 else None
            )

            # ãƒ¢ãƒ‡ãƒ«å®šç¾©
            models_config = {
                'RandomForest': RandomForestClassifier(n_estimators=50, random_state=42),
                'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
            }

            best_model = None
            best_score = 0

            for name, model in models_config.items():
                print(f"ğŸ“Š {name} è¨“ç·´ä¸­...")

                # è¨“ç·´
                model.fit(X_train, y_train)

                # äºˆæ¸¬
                train_pred = model.predict(X_train)
                test_pred = model.predict(X_test)

                # è©•ä¾¡
                train_acc = accuracy_score(y_train, train_pred)
                test_acc = accuracy_score(y_test, test_pred)

                print(f"  è¨“ç·´ç²¾åº¦: {train_acc:.4f}")
                print(f"  ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_acc:.4f}")

                # çµæœä¿å­˜
                self.results['model_performance'][name] = {
                    'train_accuracy': train_acc,
                    'test_accuracy': test_acc,
                    'classification_report': classification_report(y_test, test_pred, output_dict=True)
                }

                # æœ€é«˜ãƒ¢ãƒ‡ãƒ«é¸æŠ
                if test_acc > best_score:
                    best_score = test_acc
                    best_model = model
                    self.models['best_classifier'] = model

            print(f"ğŸ† æœ€é«˜ç²¾åº¦: {best_score:.4f}")

            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_file = os.path.join(MODELS_PATH, 'classification_model.pkl')
            with open(model_file, 'wb') as f:
                pickle.dump(self.models['best_classifier'], f)

            return True

        except Exception as e:
            print(f"âŒ åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def train_clustering_model(self) -> bool:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        try:
            print("ğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

            if self.processed_features is None:
                print("âŒ ç‰¹å¾´é‡ãŒå¿…è¦ã§ã™")
                return False

            # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            n_clusters = min(5, len(self.raw_documents) // 3)  # é©åˆ‡ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°
            if n_clusters < 2:
                n_clusters = 2

            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(self.processed_features)

            self.models['kmeans'] = kmeans

            # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ
            cluster_analysis = {}
            for i in range(n_clusters):
                cluster_docs = [self.raw_documents[j] for j, label in enumerate(cluster_labels) if label == i]

                if cluster_docs:
                    cluster_analysis[f'cluster_{i}'] = {
                        'size': len(cluster_docs),
                        'avg_doc_length': np.mean([doc['doc_length'] for doc in cluster_docs]),
                        'sample_docs': [doc['document'][:80] + '...' for doc in cluster_docs[:3]]
                    }

            self.results['data_insights']['clustering'] = cluster_analysis

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            cluster_file = os.path.join(MODELS_PATH, 'clustering_model.pkl')
            with open(cluster_file, 'wb') as f:
                pickle.dump(kmeans, f)

            print(f"âœ… {n_clusters}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ä½œæˆ")
            return True

        except Exception as e:
            print(f"âŒ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def analyze_conversation_patterns(self) -> bool:
        """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        try:
            print("ğŸ’­ ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")

            if not self.conversation_data:
                print("âš ï¸ ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return True

            # ä¼šè©±ãƒ‡ãƒ¼ã‚¿åˆ†æ
            df = pd.DataFrame(self.conversation_data)

            analysis = {
                'total_conversations': len(df),
                'human_messages': len(df[df['is_human'] == True]) if 'is_human' in df.columns else 0,
                'bot_messages': len(df[df['is_bot'] == True]) if 'is_bot' in df.columns else 0,
                'avg_message_length': df['content_length'].mean() if 'content_length' in df.columns else 0,
                'questions_count': df['has_question'].sum() if 'has_question' in df.columns else 0,
                'positive_sentiment': df['sentiment_positive'].sum() if 'sentiment_positive' in df.columns else 0,
                'negative_sentiment': df['sentiment_negative'].sum() if 'sentiment_negative' in df.columns else 0,
                'unique_users': df['user_id'].nunique() if 'user_id' in df.columns else 0,
                'unique_sessions': df['session_id'].nunique() if 'session_id' in df.columns else 0
            }

            self.results['data_insights']['conversations'] = analysis

            print("ğŸ“Š ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†:")
            for key, value in analysis.items():
                print(f"  {key}: {value}")

            return True

        except Exception as e:
            print(f"âŒ ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def generate_comprehensive_report(self):
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            print("ğŸ“ˆ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

            # ãƒ¬ãƒãƒ¼ãƒˆæ§‹é€ 
            report = {
                'timestamp': datetime.now().isoformat(),
                'system_info': {
                    'total_documents': len(self.raw_documents),
                    'total_conversations': len(self.conversation_data),
                    'feature_dimensions': self.processed_features.shape if self.processed_features is not None else None,
                    'unique_categories': len(np.unique(self.labels)) if self.labels is not None else None,
                    'processing_mode': 'offline'
                },
                'model_performance': self.results['model_performance'],
                'data_insights': self.results['data_insights'],
                'model_files': {
                    'classification_model': 'classification_model.pkl',
                    'clustering_model': 'clustering_model.pkl',
                    'vectorizer': 'vectorizer.pkl',
                    'scaler': 'scaler.pkl'
                }
            }

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = os.path.join(MODELS_PATH, 'comprehensive_report.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            # å¿…è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¿å­˜
            vectorizer_file = os.path.join(MODELS_PATH, 'vectorizer.pkl')
            scaler_file = os.path.join(MODELS_PATH, 'scaler.pkl')

            with open(vectorizer_file, 'wb') as f:
                pickle.dump(self.vectorizers.get('tfidf'), f)
            with open(scaler_file, 'wb') as f:
                pickle.dump(self.scalers.get('standard'), f)

            print("âœ… åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
            print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {report_file}")

            return True

        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def run_complete_ml_pipeline(self):
        """å®Œå…¨ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ Uma3 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        print("=" * 70)

        success_count = 0

        # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if self.load_chroma_data_direct():
            success_count += 1

        if self.load_conversation_data():
            success_count += 1

        # Step 2: ç‰¹å¾´é‡æº–å‚™
        if self.prepare_features_and_labels():
            success_count += 1

        # Step 3: ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        if self.train_classification_models():
            success_count += 1

        if self.train_clustering_model():
            success_count += 1

        # Step 4: ä¼šè©±åˆ†æ
        if self.analyze_conversation_patterns():
            success_count += 1

        # Step 5: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if self.generate_comprehensive_report():
            success_count += 1

        print("=" * 70)
        print(f"ğŸ‰ æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†! ({success_count}/7 æˆåŠŸ)")

        if success_count >= 5:
            print("âœ… æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰æˆåŠŸ!")
            print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å ´æ‰€: {MODELS_PATH}")
            print("ğŸ”® ä»¥ä¸‹ã®æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™:")
            print("  - æ–‡æ›¸åˆ†é¡ï¼ˆTF-IDFãƒ™ãƒ¼ã‚¹ï¼‰")
            print("  - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
            print("  - ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
            print("  - çµ±è¨ˆçš„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«")
            return True
        else:
            print("âš ï¸ éƒ¨åˆ†çš„ãªæˆåŠŸ - ä¸€éƒ¨ã®æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 80)
    print("ğŸ¤– Uma3 ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 80)

    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ»å®Ÿè¡Œ
    ml_system = Uma3OfflineMLSystem()
    success = ml_system.run_complete_ml_pipeline()

    if success:
        print("\nğŸŠ ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸ!")
        print("ğŸ“Š å¤–éƒ¨APIä¸è¦ã§ãƒ­ãƒ¼ã‚«ãƒ«å®Œçµã®æ©Ÿæ¢°å­¦ç¿’ãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸã€‚")
        return 0
    else:
        print("\nâŒ æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        return 1

if __name__ == "__main__":
    sys.exit(main())

#!/usr/bin/env python3
"""
Uma3 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚·ã‚¹ãƒ†ãƒ 
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ»äºˆæ¸¬ãƒ»ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ 

ã€ä¸»è¦æ©Ÿèƒ½ã€‘
1. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡
2. é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³
3. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
4. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³
5. ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
"""

import os
import sys
import pickle
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional
import json
from datetime import datetime, timedelta
import sqlite3
import re
from collections import Counter, defaultdict

# æ©Ÿæ¢°å­¦ç¿’é–¢é€£
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®çµ¶å¯¾ãƒ‘ã‚¹å–å¾—
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
MODELS_PATH = os.path.join(PROJECT_ROOT, 'ml_models')
DB_PATH = os.path.join(PROJECT_ROOT, 'db')
CHROMA_DB_PATH = os.path.join(DB_PATH, 'chroma_store')
CONVERSATION_DB_PATH = os.path.join(DB_PATH, 'conversation_history.db')

class Uma3RealTimeMLAnalyzer:
    """Uma3 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸš€ Uma3 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")

        # ãƒ¢ãƒ‡ãƒ«æ ¼ç´ç”¨
        self.classifier = None
        self.cluster_model = None
        self.vectorizer = None
        self.scaler = None

        # ãƒ‡ãƒ¼ã‚¿æ ¼ç´ç”¨
        self.historical_data = []
        self.user_behavior_patterns = {}
        self.content_database = []

        # åˆ†æçµæœæ ¼ç´ç”¨
        self.analysis_cache = {}
        self.similarity_matrix = None

        # ãƒ©ãƒ™ãƒ«å®šç¾©
        self.label_names = {
            0: 'é¸æ‰‹æƒ…å ±',
            1: 'è³ªå•',
            2: 'å›ç­”',
            3: 'ãƒãƒ¼ãƒ æƒ…å ±',
            4: 'ãã®ä»–'
        }

        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.initialize_system()

    def initialize_system(self):
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’åˆæœŸåŒ–"""
        print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.load_trained_models()

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.load_historical_data()

        # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹æ§‹ç¯‰
        self.build_similarity_matrix()

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        self.analyze_user_behavior_patterns()

        print("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def load_trained_models(self):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ“¦ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")

            # åˆ†é¡ãƒ¢ãƒ‡ãƒ«
            classification_file = os.path.join(MODELS_PATH, 'classification_model.pkl')
            if os.path.exists(classification_file):
                with open(classification_file, 'rb') as f:
                    self.classifier = pickle.load(f)
                print("âœ… åˆ†é¡ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
            clustering_file = os.path.join(MODELS_PATH, 'clustering_model.pkl')
            if os.path.exists(clustering_file):
                with open(clustering_file, 'rb') as f:
                    self.cluster_model = pickle.load(f)
                print("âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

            # ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
            vectorizer_file = os.path.join(MODELS_PATH, 'vectorizer.pkl')
            if os.path.exists(vectorizer_file):
                with open(vectorizer_file, 'rb') as f:
                    self.vectorizer = pickle.load(f)
                print("âœ… ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")
            else:
                print("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - æ–°è¦ä½œæˆ")
                self.vectorizer = TfidfVectorizer(max_features=300, ngram_range=(1, 2))
                # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã§fit
                if hasattr(self, 'historical_texts') and self.historical_texts:
                    self.vectorizer.fit(self.historical_texts)
                    print("âœ… ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã‚’å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´å®Œäº†")
                else:
                    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§åˆæœŸåŒ–
                    dummy_texts = ["ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ", "é¸æ‰‹æƒ…å ±", "ãƒãƒ¼ãƒ æˆ¦ç•¥", "è³ªå•å†…å®¹", "å›ç­”ä¾‹"]
                    self.vectorizer.fit(dummy_texts)
                    print("âœ… ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã‚’ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§åˆæœŸåŒ–å®Œäº†")

            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
            scaler_file = os.path.join(MODELS_PATH, 'scaler.pkl')
            if os.path.exists(scaler_file):
                with open(scaler_file, 'rb') as f:
                    self.scaler = pickle.load(f)
                print("âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†")

        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def load_historical_data(self):
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            print("ğŸ“Š å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")

            # ChromaDBã‹ã‚‰ç›´æ¥ãƒ‡ãƒ¼ã‚¿å–å¾—
            chroma_db_file = os.path.join(CHROMA_DB_PATH, 'chroma.sqlite3')
            if os.path.exists(chroma_db_file):
                conn = sqlite3.connect(chroma_db_file)
                cursor = conn.cursor()

                # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ¼ãƒãƒ‡ãƒ¼ã‚¿å–å¾—
                cursor.execute("SELECT string_value FROM embedding_fulltext_search WHERE string_value IS NOT NULL LIMIT 200")
                rows = cursor.fetchall()

                for row in rows:
                    if row[0] and len(str(row[0]).strip()) > 10:
                        self.content_database.append({
                            'content': str(row[0]),
                            'source': 'chroma_db',
                            'timestamp': datetime.now().isoformat(),
                            'content_length': len(str(row[0])),
                            'word_count': len(str(row[0]).split())
                        })

                conn.close()
                print(f"âœ… ChromaDBã‹ã‚‰ {len(self.content_database)} ä»¶ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã¿")

            # ä¼šè©±å±¥æ­´ãƒ‡ãƒ¼ã‚¿å–å¾—
            if os.path.exists(CONVERSATION_DB_PATH):
                conn = sqlite3.connect(CONVERSATION_DB_PATH)
                cursor = conn.cursor()

                cursor.execute("""
                    SELECT user_id, message_type, content, timestamp, session_id
                    FROM conversations
                    ORDER BY timestamp DESC
                    LIMIT 100
                """)

                rows = cursor.fetchall()
                for row in rows:
                    self.historical_data.append({
                        'user_id': row[0],
                        'message_type': row[1],
                        'content': row[2],
                        'timestamp': row[3],
                        'session_id': row[4]
                    })

                conn.close()
                print(f"âœ… ä¼šè©±å±¥æ­´ã‹ã‚‰ {len(self.historical_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿")

        except Exception as e:
            print(f"âŒ å±¥æ­´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    def extract_features(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        try:
            # TF-IDFç‰¹å¾´é‡
            if self.vectorizer and hasattr(self.vectorizer, 'transform'):
                tfidf_features = self.vectorizer.transform([text]).toarray()[0]
            else:
                tfidf_features = np.zeros(300)

            # æ‰‹å‹•ç‰¹å¾´é‡
            manual_features = [
                len(text),                                          # æ–‡æ›¸é•·
                len(text.split()),                                 # å˜èªæ•°
                int('ï¼Ÿ' in text or 'Q:' in text),                 # è³ªå•æ–‡
                int('A:' in text or 'å›ç­”' in text),               # å›ç­”æ–‡
                int(any(name in text for name in ['ç¿”å¹³', 'è¡å¤ª', 'å‹˜å¤ª', 'æš–å¤§', 'è‹±æ±°', 'æ‚ ç‰'])), # é¸æ‰‹å
                len([x for x in text if x.isdigit()]),            # æ•°å­—ã®å€‹æ•°
                text.count('ã€'),                                  # èª­ç‚¹
                text.count('ã€‚'),                                  # å¥ç‚¹
                int('ãƒãƒ¼ãƒ ' in text or 'ã‚½ãƒ•ãƒˆ' in text),         # ãƒãƒ¼ãƒ é–¢é€£
                int('ç·´ç¿’' in text or 'è©¦åˆ' in text),             # æ´»å‹•é–¢é€£
            ]

            # ç‰¹å¾´é‡çµåˆ
            features = np.hstack([tfidf_features, manual_features])

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆ310æ¬¡å…ƒã«èª¿æ•´ï¼‰
            if len(features) < 310:
                features = np.pad(features, (0, 310 - len(features)), 'constant')
            elif len(features) > 310:
                features = features[:310]

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            if self.scaler:
                features = self.scaler.transform([features])[0]

            return features

        except Exception as e:
            print(f"âŒ ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return np.zeros(310)

    def classify_text_realtime(self, text: str) -> Dict:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡"""
        try:
            if not self.classifier:
                return {'error': 'åˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}

            # ç‰¹å¾´é‡æŠ½å‡º
            features = self.extract_features(text).reshape(1, -1)

            # äºˆæ¸¬å®Ÿè¡Œ
            prediction = self.classifier.predict(features)[0]
            probabilities = self.classifier.predict_proba(features)[0]

            # çµæœæ§‹ç¯‰
            result = {
                'input_text': text,
                'predicted_category': self.label_names.get(prediction, 'Unknown'),
                'predicted_label': int(prediction),
                'confidence': float(max(probabilities)),
                'all_probabilities': {
                    self.label_names.get(i, f'Label_{i}'): float(prob)
                    for i, prob in enumerate(probabilities)
                },
                'processing_time': datetime.now().isoformat(),
                'analysis_type': 'realtime_classification'
            }

            return result

        except Exception as e:
            return {'error': f'åˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}'}

    def find_similar_content(self, query_text: str, top_k: int = 5) -> List[Dict]:
        """é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³"""
        try:
            print(f"ğŸ” é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢: '{query_text[:50]}...'")

            if not self.content_database:
                return [{'error': 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã™'}]

            # ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            if not self.vectorizer:
                return [{'error': 'ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“'}]

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
            content_texts = [item['content'] for item in self.content_database]

            # æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã§ã®å‡¦ç†
            try:
                # æ—¢å­˜ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã‚’å†ãƒ•ã‚£ãƒƒãƒˆ
                temp_vectorizer = TfidfVectorizer(max_features=300, ngram_range=(1, 2), min_df=1)
                content_vectors = temp_vectorizer.fit_transform(content_texts)
                query_vector = temp_vectorizer.transform([query_text])

                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                similarities = cosine_similarity(query_vector, content_vectors)[0]

                # é¡ä¼¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
                similar_indices = np.argsort(similarities)[::-1][:top_k]

                results = []
                for i, idx in enumerate(similar_indices):
                    if similarities[idx] > 0.01:  # æœ€å°é–¾å€¤
                        result = {
                            'rank': i + 1,
                            'similarity_score': float(similarities[idx]),
                            'content': self.content_database[idx]['content'][:200] + '...',
                            'full_content': self.content_database[idx]['content'],
                            'source': self.content_database[idx]['source'],
                            'content_length': self.content_database[idx]['content_length'],
                            'word_count': self.content_database[idx]['word_count']
                        }
                        results.append(result)

                print(f"âœ… {len(results)} ä»¶ã®é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç™ºè¦‹")
                return results

            except Exception as vec_error:
                print(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {vec_error}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                return self._fallback_keyword_search(query_text, top_k)

        except Exception as e:
            print(f"âŒ é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return [{'error': f'æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}'}]

    def _fallback_keyword_search(self, query_text: str, top_k: int) -> List[Dict]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
        try:
            keywords = query_text.split()
            scored_content = []

            for content_item in self.content_database:
                score = 0
                content = content_item['content'].lower()

                for keyword in keywords:
                    if keyword.lower() in content:
                        score += content.count(keyword.lower())

                if score > 0:
                    scored_content.append((score, content_item))

            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            scored_content.sort(key=lambda x: x[0], reverse=True)

            results = []
            for i, (score, content_item) in enumerate(scored_content[:top_k]):
                result = {
                    'rank': i + 1,
                    'similarity_score': float(score / 10),  # æ­£è¦åŒ–
                    'content': content_item['content'][:200] + '...',
                    'full_content': content_item['content'],
                    'source': content_item['source'],
                    'search_method': 'keyword_fallback'
                }
                results.append(result)

            return results

        except Exception as e:
            return [{'error': f'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}'}]

    def predict_user_behavior(self, user_id: str, current_context: str = None):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """
        try:
            print(f"ğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬: {user_id}")

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿å–å¾—
            user_history = [item for item in self.historical_data if item.get('user_id') == user_id]

            if not user_history:
                return {
                    'prediction': 'new_user',
                    'confidence': 0.5,
                    'recommendations': ['åŸºæœ¬æƒ…å ±ã®ç¢ºèª', 'ãƒãƒ¼ãƒ ç´¹ä»‹', 'é¸æ‰‹æƒ…å ±'],
                    'analysis': 'æ–°è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã™'
                }

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            user_messages = [item['content'] for item in user_history if item.get('content')]
            message_types = [item['message_type'] for item in user_history]

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            avg_message_length = np.mean([len(msg) for msg in user_messages]) if user_messages else 0
            question_ratio = sum(1 for msg in user_messages if 'ï¼Ÿ' in msg or '?' in msg) / len(user_messages) if user_messages else 0
            recent_activity = len([item for item in user_history if self._is_recent(item.get('timestamp', ''))])

            # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡
            if current_context:
                context_analysis = self.classify_text_realtime(current_context)
            else:
                context_analysis = {'predicted_category': 'ãã®ä»–', 'confidence': 0.5}

            # äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯
            if question_ratio > 0.6:
                prediction = 'information_seeker'
                recommendations = ['è©³ç´°ãªå›ç­”æä¾›', 'é–¢é€£æƒ…å ±ã®æç¤º', 'FAQæ¡ˆå†…']
            elif avg_message_length < 20:
                prediction = 'casual_user'
                recommendations = ['ç°¡æ½”ãªå¿œç­”', 'è¦–è¦šçš„æƒ…å ±', 'ã‚¯ã‚¤ãƒƒã‚¯æ“ä½œ']
            elif recent_activity > 3:
                prediction = 'active_user'
                recommendations = ['æ–°æ©Ÿèƒ½ç´¹ä»‹', 'è©³ç´°æ©Ÿèƒ½', 'ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚º']
            else:
                prediction = 'regular_user'
                recommendations = ['æ¨™æº–çš„ãªå¿œç­”', 'ãƒãƒ©ãƒ³ã‚¹å‹æƒ…å ±', 'ä¸€èˆ¬çš„ãªã‚µãƒãƒ¼ãƒˆ']

            result = {
                'user_id': user_id,
                'prediction': prediction,
                'confidence': min(0.9, 0.5 + (len(user_history) * 0.05)),
                'recommendations': recommendations,
                'user_profile': {
                    'total_messages': len(user_history),
                    'avg_message_length': avg_message_length,
                    'question_ratio': question_ratio,
                    'recent_activity': recent_activity,
                    'preferred_message_type': max(set(message_types), key=message_types.count) if message_types else 'unknown'
                },
                'context_analysis': context_analysis,
                'analysis_timestamp': datetime.now().isoformat()
            }

            return result

        except Exception as e:
            return {'error': f'ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}'}

    def _is_recent(self, timestamp_str: str, days: int = 7) -> bool:
        """æœ€è¿‘ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            if not timestamp_str:
                return False
            timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            return (datetime.now() - timestamp).days <= days
        except:
            return False

    def build_similarity_matrix(self):
        """é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        try:
            print("ğŸ”§ é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...")

            if not self.content_database:
                print("âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒç©ºã®ãŸã‚ã€ãƒãƒˆãƒªãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return

            # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆå‡¦ç†é€Ÿåº¦ã®ãŸã‚ï¼‰
            sample_size = min(50, len(self.content_database))
            sample_content = self.content_database[:sample_size]

            content_texts = [item['content'] for item in sample_content]

            # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
            vectorizer = TfidfVectorizer(max_features=200, ngram_range=(1, 2), min_df=1)
            tfidf_matrix = vectorizer.fit_transform(content_texts)

            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—
            self.similarity_matrix = cosine_similarity(tfidf_matrix)

            print(f"âœ… {sample_size}x{sample_size} é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†")

        except Exception as e:
            print(f"âŒ é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")

    def analyze_user_behavior_patterns(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        try:
            print("ğŸ‘¥ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")

            if not self.historical_data:
                print("âš ï¸ å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            user_groups = defaultdict(list)
            for item in self.historical_data:
                if item.get('user_id'):
                    user_groups[item['user_id']].append(item)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            for user_id, user_data in user_groups.items():
                pattern = {
                    'total_messages': len(user_data),
                    'avg_message_length': np.mean([len(str(item.get('content', ''))) for item in user_data]),
                    'message_types': Counter([item.get('message_type') for item in user_data]),
                    'activity_timeframe': self._calculate_activity_timeframe(user_data),
                    'common_topics': self._extract_common_topics(user_data)
                }
                self.user_behavior_patterns[user_id] = pattern

            print(f"âœ… {len(user_groups)} ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ")

        except Exception as e:
            print(f"âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    def _calculate_activity_timeframe(self, user_data: List[Dict]) -> Dict:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æœŸé–“è¨ˆç®—"""
        try:
            timestamps = [item.get('timestamp') for item in user_data if item.get('timestamp')]
            if not timestamps:
                return {'span': 0, 'frequency': 0}

            # æœŸé–“è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            return {
                'total_interactions': len(timestamps),
                'unique_sessions': len(set([item.get('session_id') for item in user_data if item.get('session_id')])),
                'span_days': 'calculated_if_needed'
            }
        except:
            return {'span': 0, 'frequency': 0}

    def _extract_common_topics(self, user_data: List[Dict]) -> List[str]:
        """å…±é€šãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º"""
        try:
            all_content = ' '.join([str(item.get('content', '')) for item in user_data])

            # ç°¡æ˜“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            keywords = []
            if 'é¸æ‰‹' in all_content or any(name in all_content for name in ['ç¿”å¹³', 'è¡å¤ª', 'å‹˜å¤ª']):
                keywords.append('é¸æ‰‹æƒ…å ±')
            if 'ç·´ç¿’' in all_content or 'è©¦åˆ' in all_content:
                keywords.append('æ´»å‹•æƒ…å ±')
            if 'ï¼Ÿ' in all_content or 'Q:' in all_content:
                keywords.append('è³ªå•')
            if 'ãƒãƒ¼ãƒ ' in all_content:
                keywords.append('ãƒãƒ¼ãƒ æƒ…å ±')

            return keywords[:3]  # ãƒˆãƒƒãƒ—3

        except:
            return []

    def run_comprehensive_analysis(self, input_texts: List[str]) -> Dict:
        """åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ"""
        try:
            print(f"ğŸš€ åŒ…æ‹¬çš„åˆ†æé–‹å§‹: {len(input_texts)} ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆ")

            results = {
                'analysis_timestamp': datetime.now().isoformat(),
                'input_count': len(input_texts),
                'classifications': [],
                'similar_content_results': [],
                'behavior_predictions': [],
                'summary_statistics': {}
            }

            # å„ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æ
            for i, text in enumerate(input_texts):
                print(f"  åˆ†æä¸­: {i+1}/{len(input_texts)}")

                # 1. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†é¡
                classification = self.classify_text_realtime(text)
                results['classifications'].append(classification)

                # 2. é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç™ºè¦‹
                similar_content = self.find_similar_content(text, top_k=3)
                results['similar_content_results'].append({
                    'query': text,
                    'similar_items': similar_content
                })

                # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ï¼‰
                sample_user_id = f"user_{i%3 + 1}"  # ã‚µãƒ³ãƒ—ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼
                behavior_prediction = self.predict_user_behavior(sample_user_id, text)
                results['behavior_predictions'].append(behavior_prediction)

            # 4. çµ±è¨ˆã‚µãƒãƒªãƒ¼
            categories = [item.get('predicted_category', 'Unknown') for item in results['classifications']]
            confidences = [item.get('confidence', 0) for item in results['classifications'] if 'confidence' in item]

            results['summary_statistics'] = {
                'category_distribution': Counter(categories),
                'average_confidence': np.mean(confidences) if confidences else 0,
                'total_similar_items_found': sum(len(item['similar_items']) for item in results['similar_content_results']),
                'unique_behavior_patterns': len(set(pred.get('prediction', '') for pred in results['behavior_predictions']))
            }

            print("âœ… åŒ…æ‹¬çš„åˆ†æå®Œäº†")
            return results

        except Exception as e:
            print(f"âŒ åŒ…æ‹¬çš„åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'}

def run_realtime_analysis_demo():
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("=" * 80)
    print("ğŸš€ Uma3 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚·ã‚¹ãƒ†ãƒ  - å®Ÿæ¼”ãƒ‡ãƒ¢")
    print("=" * 80)

    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    analyzer = Uma3RealTimeMLAnalyzer()

    # ãƒ‡ãƒ¢ç”¨ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    demo_texts = [
        "ç¿”å¹³é¸æ‰‹ã®æœ€æ–°ã®æˆç¸¾ã¨è©•ä¾¡ã‚’æ•™ãˆã¦ãã ã•ã„",
        "æ¬¡å›ã®ç·´ç¿’è©¦åˆã¯ã„ã¤é–‹å‚¬ã•ã‚Œã¾ã™ã‹ï¼Ÿ",
        "ãƒãƒ¼ãƒ ã®3å¹´ç”Ÿãƒ¡ãƒ³ãƒãƒ¼ã®è©³ç´°æƒ…å ±ãŒçŸ¥ã‚ŠãŸã„ã§ã™",
        "è¡å¤ªé¸æ‰‹ã®ãƒã‚¸ã‚·ãƒ§ãƒ³ã¨ç‰¹å¾´ã«ã¤ã„ã¦",
        "é¦¬ä¸‰ã‚½ãƒ•ãƒˆã®ä»Šå­£ã®ç›®æ¨™ã¨æˆ¦ç•¥ã¯ï¼Ÿ",
        "æ–°ã—ã„ç·´ç¿’ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ææ¡ˆãŒã‚ã‚Šã¾ã™",
        "å‹˜å¤ªé¸æ‰‹ã®å®ˆå‚™åŠ›ã«ã¤ã„ã¦è©•ä¾¡ã—ã¦ãã ã•ã„",
        "ãƒãƒ¼ãƒ å…¨ä½“ã®èª²é¡Œã¨æ”¹å–„ç‚¹ã‚’åˆ†æã—ãŸã„",
        "æš–å¤§é¸æ‰‹ã®æ‰“æ’ƒãƒ•ã‚©ãƒ¼ãƒ ã®ç‰¹å¾´ã¯ï¼Ÿ",
        "æ¥æœˆã®å¤§ä¼šã«å‘ã‘ãŸæº–å‚™çŠ¶æ³ã‚’ç¢ºèª"
    ]

    print(f"ğŸ“Š {len(demo_texts)} ä»¶ã®ãƒ†ã‚­ã‚¹ãƒˆã§åŒ…æ‹¬çš„åˆ†æã‚’å®Ÿè¡Œ")
    print("=" * 50)

    # åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ
    analysis_results = analyzer.run_comprehensive_analysis(demo_texts)

    # çµæœè¡¨ç¤º
    if 'error' not in analysis_results:
        print("\nğŸ“ˆ åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 40)

        stats = analysis_results.get('summary_statistics', {})
        print(f"ğŸ“ åˆ†æãƒ†ã‚­ã‚¹ãƒˆæ•°: {analysis_results.get('input_count', 0)}")
        print(f"ğŸ¯ å¹³å‡ä¿¡é ¼åº¦: {stats.get('average_confidence', 0):.4f}")
        print(f"ğŸ” é¡ä¼¼ã‚¢ã‚¤ãƒ†ãƒ ç™ºè¦‹æ•°: {stats.get('total_similar_items_found', 0)}")
        print(f"ğŸ‘¥ è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç¨®é¡: {stats.get('unique_behavior_patterns', 0)}")

        print("\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
        category_dist = stats.get('category_distribution', {})
        for category, count in category_dist.items():
            percentage = (count / analysis_results.get('input_count', 1)) * 100
            print(f"  {category}: {count} ä»¶ ({percentage:.1f}%)")

        # å€‹åˆ¥çµæœã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        print("\nğŸ” å€‹åˆ¥åˆ†æçµæœã‚µãƒ³ãƒ—ãƒ«:")
        for i, classification in enumerate(analysis_results.get('classifications', [])[:3]):
            print(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i+1} ---")
            print(f"å…¥åŠ›: {classification.get('input_text', '')[:60]}...")
            print(f"åˆ†é¡: {classification.get('predicted_category', 'Unknown')}")
            print(f"ä¿¡é ¼åº¦: {classification.get('confidence', 0):.4f}")

        # é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µãƒ³ãƒ—ãƒ«
        print("\nğŸ” é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç™ºè¦‹ã‚µãƒ³ãƒ—ãƒ«:")
        similar_results = analysis_results.get('similar_content_results', [])
        if similar_results:
            sample_similar = similar_results[0]
            print(f"ã‚¯ã‚¨ãƒª: {sample_similar.get('query', '')[:50]}...")
            for item in sample_similar.get('similar_items', [])[:2]:
                if 'similarity_score' in item:
                    print(f"  é¡ä¼¼åº¦ {item['similarity_score']:.4f}: {item.get('content', '')[:80]}...")

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = os.path.join(MODELS_PATH, f'realtime_analysis_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")

    else:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {analysis_results.get('error')}")

    print("\nğŸ‰ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ‡ãƒ¢å®Œäº†!")
    return analysis_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ãƒ‡ãƒ¢å®Ÿè¡Œ
        results = run_realtime_analysis_demo()

        if results and 'error' not in results:
            print("\nâœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ©Ÿæ¢°å­¦ç¿’åˆ†æã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å‹•ä½œã—ã¾ã—ãŸ!")
            print("ğŸš€ ä»¥ä¸‹ã®æ©Ÿèƒ½ãŒå®Ÿæ™‚é–“ã§åˆ©ç”¨å¯èƒ½ã§ã™:")
            print("  ğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ (95.6%ç²¾åº¦)")
            print("  ğŸ” é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç™ºè¦‹")
            print("  ğŸ‘¥ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬")
            print("  ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
            print("  ğŸ¤– ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¨è–¦")
            return 0
        else:
            print("\nâŒ ã‚·ã‚¹ãƒ†ãƒ ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            return 1

    except Exception as e:
        print(f"\nâŒ ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())

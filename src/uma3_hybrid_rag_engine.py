"""
Uma3 Hybrid RAG Engine
LangChain + LlamaIndex ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚·ã‚¹ãƒ†ãƒ 
"""

import os
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

# LangChain imports
try:
    from langchain_chroma import Chroma
    from langchain_core.documents import Document as LangChainDocument
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_openai import ChatOpenAI
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"[HYBRID] âŒ LangChain import error: {e}")
    LANGCHAIN_AVAILABLE = False

# LlamaIndex imports
try:
    from llama_index.core.schema import NodeWithScore

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‹ã®äº’æ›æ€§å¯¾å¿œ
    try:
        from llama_index.core.response.schema import Response
    except ImportError:
        try:
            from llama_index.core.base.response.schema import Response
        except ImportError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è‡ªå‰ã®Responseå‹å®šç¾©
            class Response:
                def __init__(self, response: str, source_nodes=None, metadata=None):
                    self.response = response
                    self.source_nodes = source_nodes or []
                    self.metadata = metadata or {}

    LLAMA_INDEX_AVAILABLE = True
except ImportError as e:
    print(f"[HYBRID] âŒ LlamaIndex import error: {e}")
    LLAMA_INDEX_AVAILABLE = False

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ChromaDBä¿å­˜å…ˆï¼ˆçµ¶å¯¾ãƒ‘ã‚¹æ–¹å¼ï¼‰
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DEFAULT_CHROMA_PERSIST_DIRECTORY = os.path.join(PROJECT_ROOT, "db", "chroma_store")

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ã‚¯ãƒ©ã‚¹å®šç¾©
class Response:
    def __init__(self, response: str, source_nodes=None, metadata=None):
        self.response = response
        self.source_nodes = source_nodes or []
        self.metadata = metadata or {}

class NodeWithScore:
    def __init__(self, node, score: float = 0.0):
        self.node = node
        self.score = score

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from uma3_llama_index_engine import Uma3LlamaIndexEngine, LlamaIndexLangChainBridge
from uma3_chroma_improver import Uma3ChromaDBImprover


class Uma3HybridRAGEngine:
    """
    LangChain + LlamaIndex ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚¨ãƒ³ã‚¸ãƒ³

    æ©Ÿèƒ½:
    - æ—¢å­˜ã®LangChain ChromaDBã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨äº’æ›æ€§
    - LlamaIndexã®é«˜åº¦ãªã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã¨ã®çµ±åˆ
    - ä¸¡æ–¹ã®ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½µç”¨ã—ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
    - çµæœã®çµ±åˆã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
    """

    def __init__(
        self,
        chroma_persist_directory: str = None,
        embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        llm_model: str = "gpt-3.5-turbo",
        # llm_model: str = "gpt-4-turbo",
        enable_langchain: bool = True,
        enable_llama_index: bool = True
    ):
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–

        Args:
            chroma_persist_directory: ChromaDBãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨)
            embedding_model_name: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
            llm_model: LLMãƒ¢ãƒ‡ãƒ«å
            enable_langchain: LangChainã‚¨ãƒ³ã‚¸ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            enable_llama_index: LlamaIndexã‚¨ãƒ³ã‚¸ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        """
        # çµ¶å¯¾ãƒ‘ã‚¹æ–¹å¼ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
        if chroma_persist_directory is None:
            chroma_persist_directory = DEFAULT_CHROMA_PERSIST_DIRECTORY

        print(f"[HYBRID] ğŸš€ Initializing Uma3HybridRAGEngine")
        print(f"[HYBRID] LangChain enabled: {enable_langchain}")
        print(f"[HYBRID] LlamaIndex enabled: {enable_llama_index}")

        self.chroma_persist_directory = chroma_persist_directory
        self.embedding_model_name = embedding_model_name
        self.llm_model = llm_model
        self.enable_langchain = enable_langchain and LANGCHAIN_AVAILABLE
        self.enable_llama_index = enable_llama_index and LLAMA_INDEX_AVAILABLE

        # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.langchain_engine: Optional[Uma3ChromaDBImprover] = None
        self.llama_index_engine: Optional[Uma3LlamaIndexEngine] = None
        self.bridge = LlamaIndexLangChainBridge()

        # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ï¼ˆä¸¦åˆ—æ¤œç´¢ç”¨ï¼‰
        self.thread_pool = ThreadPoolExecutor(max_workers=4)

        # åˆæœŸåŒ–å®Ÿè¡Œ
        self._initialize_engines()

        print(f"[HYBRID] âœ… Hybrid RAG engine initialized")
        print(f"[HYBRID] Active engines: LangChain={self.langchain_engine is not None}, LlamaIndex={self.llama_index_engine is not None}")

    def _initialize_engines(self):
        """å„ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–"""

        # 1. LangChain ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        if self.enable_langchain:
            try:
                # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
                embedding_model = HuggingFaceEmbeddings(
                    model_name=self.embedding_model_name
                )

                # ChromaDBã®åˆæœŸåŒ–
                vector_db = Chroma(
                    persist_directory=self.chroma_persist_directory,
                    embedding_function=embedding_model,
                )

                # ChromaDBæ”¹è‰¯å™¨ã®åˆæœŸåŒ–
                self.langchain_engine = Uma3ChromaDBImprover(vector_db)
                print("[HYBRID] âœ… LangChain engine initialized")

            except Exception as e:
                print(f"[HYBRID] âŒ LangChain engine initialization failed: {e}")
                self.langchain_engine = None

        # 2. LlamaIndex ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        if self.enable_llama_index:
            try:
                self.llama_index_engine = Uma3LlamaIndexEngine(
                    chroma_persist_directory=self.chroma_persist_directory,
                    embedding_model_name=self.embedding_model_name,
                    llm_model=self.llm_model,
                    collection_name="uma3_documents_llama"
                )

                if self.llama_index_engine.is_initialized:
                    print("[HYBRID] âœ… LlamaIndex engine initialized")
                else:
                    print("[HYBRID] âš ï¸ LlamaIndex engine failed to initialize")
                    self.llama_index_engine = None

            except Exception as e:
                print(f"[HYBRID] âŒ LlamaIndex engine initialization failed: {e}")
                self.llama_index_engine = None

    def hybrid_search(
        self,
        query: str,
        k: int = 10,
        langchain_weight: float = 0.6,
        llama_index_weight: float = 0.4,
        use_parallel: bool = True
    ) -> List[LangChainDocument]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè¡Œ

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹çµæœæ•°
            langchain_weight: LangChainã‚¨ãƒ³ã‚¸ãƒ³ã®é‡ã¿
            llama_index_weight: LlamaIndexã‚¨ãƒ³ã‚¸ãƒ³ã®é‡ã¿
            use_parallel: ä¸¦åˆ—æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹

        Returns:
            çµ±åˆã•ã‚ŒãŸæ¤œç´¢çµæœ
        """
        print(f"[HYBRID] ğŸ” Executing hybrid search: '{query[:50]}...'")

        # é‡ã¿ã®æ­£è¦åŒ–
        total_weight = langchain_weight + llama_index_weight
        if total_weight > 0:
            langchain_weight /= total_weight
            llama_index_weight /= total_weight

        results = []

        if use_parallel and self.langchain_engine and self.llama_index_engine:
            # ä¸¦åˆ—æ¤œç´¢
            results = self._parallel_search(query, k, langchain_weight, llama_index_weight)
        else:
            # é †æ¬¡æ¤œç´¢
            results = self._sequential_search(query, k, langchain_weight, llama_index_weight)

        print(f"[HYBRID] âœ… Hybrid search completed: {len(results)} results")
        return results

    def _parallel_search(
        self,
        query: str,
        k: int,
        langchain_weight: float,
        llama_index_weight: float
    ) -> List[LangChainDocument]:
        """ä¸¦åˆ—æ¤œç´¢ã®å®Ÿè¡Œ"""

        futures = []

        # LangChainæ¤œç´¢ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
        if self.langchain_engine:
            future_langchain = self.thread_pool.submit(
                self._search_langchain, query, k
            )
            futures.append(('langchain', future_langchain, langchain_weight))

        # LlamaIndexæ¤œç´¢ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
        if self.llama_index_engine:
            future_llama = self.thread_pool.submit(
                self._search_llama_index, query, k
            )
            futures.append(('llama_index', future_llama, llama_index_weight))

        # çµæœã®åé›†
        engine_results = {}
        for engine_name, future, weight in futures:
            try:
                result = future.result(timeout=30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                engine_results[engine_name] = (result, weight)
                print(f"[HYBRID] âœ… {engine_name} search completed: {len(result)} results")
            except Exception as e:
                print(f"[HYBRID] âŒ {engine_name} search failed: {e}")
                engine_results[engine_name] = ([], weight)

        # çµæœã®çµ±åˆ
        return self._merge_results(engine_results, k)

    def _sequential_search(
        self,
        query: str,
        k: int,
        langchain_weight: float,
        llama_index_weight: float
    ) -> List[LangChainDocument]:
        """é †æ¬¡æ¤œç´¢ã®å®Ÿè¡Œ"""

        engine_results = {}

        # LangChainæ¤œç´¢
        if self.langchain_engine:
            try:
                langchain_results = self._search_langchain(query, k)
                engine_results['langchain'] = (langchain_results, langchain_weight)
                print(f"[HYBRID] âœ… LangChain search: {len(langchain_results)} results")
            except Exception as e:
                print(f"[HYBRID] âŒ LangChain search failed: {e}")
                engine_results['langchain'] = ([], langchain_weight)

        # LlamaIndexæ¤œç´¢
        if self.llama_index_engine:
            try:
                llama_results = self._search_llama_index(query, k)
                engine_results['llama_index'] = (llama_results, llama_index_weight)
                print(f"[HYBRID] âœ… LlamaIndex search: {len(llama_results)} results")
            except Exception as e:
                print(f"[HYBRID] âŒ LlamaIndex search failed: {e}")
                engine_results['llama_index'] = ([], llama_index_weight)

        # çµæœã®çµ±åˆ
        return self._merge_results(engine_results, k)

    def _search_langchain(self, query: str, k: int) -> List[LangChainDocument]:
        """LangChainæ¤œç´¢ã®å®Ÿè¡Œ"""
        if not self.langchain_engine:
            return []

        return self.langchain_engine.smart_similarity_search(
            query=query,
            k=k,
            boost_recent=True,
            score_threshold=0.7
        )

    def _search_llama_index(self, query: str, k: int) -> List[LangChainDocument]:
        """LlamaIndexæ¤œç´¢ã®å®Ÿè¡Œ"""
        if not self.llama_index_engine:
            return []

        # LlamaIndexã§æ¤œç´¢
        nodes = self.llama_index_engine.retrieve(query, top_k=k)

        # LangChain Documentã«å¤‰æ›
        return self.bridge.llama_to_langchain_documents(nodes)

    def _merge_results(
        self,
        engine_results: Dict[str, Tuple[List[LangChainDocument], float]],
        k: int
    ) -> List[LangChainDocument]:
        """æ¤œç´¢çµæœã®çµ±åˆã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""

        merged_docs = []
        content_seen = set()  # é‡è¤‡é™¤å»ç”¨

        # å„ã‚¨ãƒ³ã‚¸ãƒ³ã®çµæœã‚’çµ±åˆ
        for engine_name, (docs, weight) in engine_results.items():
            for i, doc in enumerate(docs):
                # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆé †ä½ãƒ™ãƒ¼ã‚¹ + é‡ã¿ï¼‰
                rank_score = (len(docs) - i) / len(docs) if docs else 0
                final_score = rank_score * weight

                # LlamaIndexã‚¹ã‚³ã‚¢ãŒã‚ã‚‹å ´åˆã¯è€ƒæ…®
                if hasattr(doc, 'metadata') and 'llama_score' in doc.metadata:
                    llama_score = doc.metadata['llama_score']
                    final_score = (final_score + llama_score) / 2

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                content_key = doc.page_content[:100]  # æœ€åˆã®100æ–‡å­—ã§é‡è¤‡åˆ¤å®š
                if content_key in content_seen:
                    continue
                content_seen.add(content_key)

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆæƒ…å ±ã‚’è¿½åŠ 
                enhanced_metadata = doc.metadata.copy() if doc.metadata else {}
                enhanced_metadata.update({
                    'hybrid_score': final_score,
                    'engine': engine_name,
                    'rank': i + 1
                })

                enhanced_doc = LangChainDocument(
                    page_content=doc.page_content,
                    metadata=enhanced_metadata
                )

                merged_docs.append(enhanced_doc)

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        merged_docs.sort(key=lambda x: x.metadata.get('hybrid_score', 0), reverse=True)

        print(f"[HYBRID] ğŸ“Š Merged results: {len(merged_docs)} unique documents")
        return merged_docs[:k]

    def llama_index_query(self, query: str, top_k: int = 5) -> Optional[str]:
        """
        LlamaIndexã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ãŸå›ç­”ç”Ÿæˆ

        Args:
            query: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            top_k: å‚ç…§ã™ã‚‹æ–‡æ›¸æ•°

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.llama_index_engine:
            print("[HYBRID] âš ï¸ LlamaIndex engine not available")
            return None

        try:
            response = self.llama_index_engine.query(query, top_k=top_k)
            if response:
                print(f"[HYBRID] âœ… LlamaIndex query response generated")
                return response.response
            return None

        except Exception as e:
            print(f"[HYBRID] âŒ LlamaIndex query error: {e}")
            return None

    def add_documents_to_both(self, documents: List[Union[str, Dict[str, Any]]]) -> Dict[str, bool]:
        """
        ä¸¡æ–¹ã®ã‚¨ãƒ³ã‚¸ãƒ³ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 

        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

        Returns:
            å„ã‚¨ãƒ³ã‚¸ãƒ³ã§ã®è¿½åŠ çµæœ
        """
        results = {"langchain": False, "llama_index": False}

        # LangChainç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¤‰æ›
        langchain_docs = []
        for doc in documents:
            if isinstance(doc, str):
                langchain_doc = LangChainDocument(
                    page_content=doc,
                    metadata={"timestamp": datetime.now().isoformat()}
                )
            elif isinstance(doc, dict):
                langchain_doc = LangChainDocument(
                    page_content=doc.get("text", doc.get("page_content", "")),
                    metadata=doc.get("metadata", {})
                )
            else:
                continue
            langchain_docs.append(langchain_doc)

        # LangChainã«è¿½åŠ 
        if self.langchain_engine and langchain_docs:
            try:
                # ChromaDBã«ç›´æ¥è¿½åŠ ï¼ˆUma3ChromaDBImproverã«ã¯è¿½åŠ ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„ãŸã‚ï¼‰
                vector_db = self.langchain_engine.vector_db
                vector_db.add_documents(langchain_docs)
                results["langchain"] = True
                print(f"[HYBRID] âœ… Added {len(langchain_docs)} documents to LangChain")
            except Exception as e:
                print(f"[HYBRID] âŒ LangChain document addition failed: {e}")

        # LlamaIndexã«è¿½åŠ 
        if self.llama_index_engine:
            results["llama_index"] = self.llama_index_engine.add_documents(documents)

        return results

    def get_hybrid_stats(self) -> Dict[str, Any]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã®çµ±è¨ˆæƒ…å ±"""
        stats = {
            "engines": {
                "langchain": self.langchain_engine is not None,
                "llama_index": self.llama_index_engine is not None
            },
            "configuration": {
                "chroma_directory": self.chroma_persist_directory,
                "embedding_model": self.embedding_model_name,
                "llm_model": self.llm_model
            }
        }

        # LlamaIndexçµ±è¨ˆ
        if self.llama_index_engine:
            stats["llama_index"] = self.llama_index_engine.get_stats()

        return stats

    def __del__(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if hasattr(self, 'thread_pool'):
            self.thread_pool.shutdown(wait=False)


# ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_hybrid_rag_engine():
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Hybrid RAG Engine Test ===")

    try:
        # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        engine = Uma3HybridRAGEngine(
            chroma_persist_directory="test_hybrid_chroma",
            enable_langchain=True,
            enable_llama_index=True
        )

        # ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ 
        test_docs = [
            "ã“ã‚Œã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚",
            {"text": "LangChainã¨LlamaIndexã‚’çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ãŒå‹•ä½œã—ã¦ã„ã¾ã™ã€‚", "metadata": {"type": "test"}},
            "è¤‡æ•°ã®RAGã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½µç”¨ã™ã‚‹ã“ã¨ã§æ¤œç´¢ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚"
        ]

        add_results = engine.add_documents_to_both(test_docs)
        print(f"âœ… Document addition results: {add_results}")

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        search_results = engine.hybrid_search("ãƒ†ã‚¹ãƒˆ", k=5)
        print(f"âœ… Hybrid search returned {len(search_results)} results")

        for i, doc in enumerate(search_results[:3]):  # ä¸Šä½3ä»¶è¡¨ç¤º
            score = doc.metadata.get('hybrid_score', 0)
            engine_name = doc.metadata.get('engine', 'unknown')
            print(f"  {i+1}. [{engine_name}] Score: {score:.3f} - {doc.page_content[:80]}...")

        # LlamaIndexã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
        if engine.llama_index_engine:
            response = engine.llama_index_query("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚·ã‚¹ãƒ†ãƒ ã«ã¤ã„ã¦æ•™ãˆã¦")
            if response:
                print(f"âœ… LlamaIndex response: {response[:150]}...")

        # çµ±è¨ˆæƒ…å ±
        stats = engine.get_hybrid_stats()
        print(f"âœ… Hybrid stats: {stats}")

        print("âœ… Hybrid RAG Engine test completed successfully!")
        return True

    except Exception as e:
        print(f"âŒ Hybrid RAG Engine test failed: {e}")
        return False


if __name__ == "__main__":
    test_hybrid_rag_engine()

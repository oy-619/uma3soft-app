"""
Uma3 LlamaIndex Integration Engine
LangChain ã‚·ã‚¹ãƒ†ãƒ ã«LlamaIndexã‚’çµ±åˆã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰RAGã‚¨ãƒ³ã‚¸ãƒ³
"""

import os
import logging
from typing import List, Optional, Dict, Any, Union
from datetime import datetime

try:
    from llama_index.core import VectorStoreIndex, StorageContext
    from llama_index.core.indices.base import BaseIndex
    from llama_index.core.query_engine import BaseQueryEngine
    from llama_index.core.retrievers import BaseRetriever
    from llama_index.core.schema import Document as LlamaDocument, NodeWithScore
    from llama_index.core.llms import LLM
    from llama_index.core.embeddings import BaseEmbedding

    # LlamaIndex embeddings - æ–°ã—ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹é€ ã«å¯¾å¿œ
    HuggingFaceEmbedding = None
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        print("[LLAMA] âœ… HuggingFace embeddings imported")
    except ImportError as e1:
        print(f"[LLAMA] âš ï¸ HuggingFace embeddings not available: {e1}")

    # OpenAI Embeddings as fallback
    try:
        from llama_index.embeddings.openai import OpenAIEmbedding
        print("[LLAMA] âœ… OpenAI embeddings imported")
    except ImportError as e3:
        print(f"[LLAMA] âš ï¸ OpenAI embeddings not available: {e3}")
        OpenAIEmbedding = None

    from llama_index.llms.openai import OpenAI
    from llama_index.vector_stores.chroma import ChromaVectorStore
    from llama_index.core.storage.storage_context import StorageContext
    from llama_index.core.vector_stores.types import VectorStore

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‹ã®äº’æ›æ€§å¯¾å¿œ
    try:
        from llama_index.core.response.schema import Response
    except ImportError:
        try:
            from llama_index.core.base.response.schema import Response
        except ImportError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è‡ªå‰ã®Responseå‹å®šç¾©
            class Response:
                def __init__(self, response: str, source_nodes=None, metadata=None):
                    self.response = response
                    self.source_nodes = source_nodes or []
                    self.metadata = metadata or {}

    LLAMA_INDEX_AVAILABLE = True
    print("[LLAMA] âœ… LlamaIndex components successfully imported")
except ImportError as e:
    print(f"[LLAMA] âŒ LlamaIndex import error: {e}")
    LLAMA_INDEX_AVAILABLE = False

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ChromaDBä¿å­˜å…ˆï¼ˆçµ¶å¯¾ãƒ‘ã‚¹æ–¹å¼ï¼‰
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
DEFAULT_CHROMA_PERSIST_DIRECTORY = os.path.join(PROJECT_ROOT, "db", "chroma_store")

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ã‚¯ãƒ©ã‚¹å®šç¾©
class Response:
    def __init__(self, response: str, source_nodes=None, metadata=None):
        self.response = response
        self.source_nodes = source_nodes or []
        self.metadata = metadata or {}

# LangChainäº’æ›æ€§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from langchain_chroma import Chroma
    from langchain_core.documents import Document as LangChainDocument
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_openai import ChatOpenAI
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"[LLAMA] âŒ LangChain import error: {e}")
    LANGCHAIN_AVAILABLE = False

# ChromaDBç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError as e:
    print(f"[LLAMA] âŒ ChromaDB import error: {e}")
    CHROMADB_AVAILABLE = False


class Uma3LlamaIndexEngine:
    """
    Uma3å°‚ç”¨LlamaIndexçµ±åˆã‚¨ãƒ³ã‚¸ãƒ³

    æ©Ÿèƒ½:
    - LangChain ChromaDBã¨ã®äº’æ›æ€§
    - LlamaIndex VectorStoreIndex
    - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
    - ã‚«ã‚¹ã‚¿ãƒ ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³
    """

    def __init__(
        self,
        chroma_persist_directory: str = None,
        embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        llm_model: str = "gpt-3.5-turbo",
        collection_name: str = "uma3_documents"
    ):
        """
        LlamaIndex ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–

        Args:
            chroma_persist_directory: ChromaDBã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨)
            embedding_model_name: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
            llm_model: LLMãƒ¢ãƒ‡ãƒ«å
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        """
        # çµ¶å¯¾ãƒ‘ã‚¹æ–¹å¼ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
        if chroma_persist_directory is None:
            chroma_persist_directory = DEFAULT_CHROMA_PERSIST_DIRECTORY

        print(f"[LLAMA] ğŸš€ Initializing Uma3LlamaIndexEngine")
        print(f"[LLAMA] ChromaDB directory: {chroma_persist_directory}")
        print(f"[LLAMA] Embedding model: {embedding_model_name}")
        print(f"[LLAMA] LLM model: {llm_model}")

        self.chroma_persist_directory = chroma_persist_directory
        self.embedding_model_name = embedding_model_name
        self.llm_model = llm_model
        self.collection_name = collection_name

        # åˆæœŸåŒ–ãƒ•ãƒ©ã‚°
        self.is_initialized = False
        self.index: Optional[VectorStoreIndex] = None
        self.query_engine: Optional[BaseQueryEngine] = None
        self.retriever: Optional[BaseRetriever] = None

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        if not LLAMA_INDEX_AVAILABLE:
            print("[LLAMA] âš ï¸ LlamaIndex not available - running in compatibility mode")
            return

        try:
            self._initialize_components()
            self.is_initialized = True
            print("[LLAMA] âœ… LlamaIndex engine successfully initialized")
        except Exception as e:
            print(f"[LLAMA] âŒ Initialization failed: {e}")
            print("[LLAMA] ğŸ”„ Falling back to compatibility mode")
            self.is_initialized = False

    def _initialize_components(self):
        """LlamaIndexã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–"""

        # 1. LLMåˆæœŸåŒ–
        self.llm = OpenAI(model=self.llm_model, temperature=0.1)
        print(f"[LLAMA] âœ… LLM initialized: {self.llm_model}")

        # 2. EmbeddingåˆæœŸåŒ– - è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦è¡Œ
        try:
            self.embed_model = HuggingFaceEmbedding(
                model_name=self.embedding_model_name
            )
            print(f"[LLAMA] âœ… HuggingFace Embedding model initialized: {self.embedding_model_name}")
        except Exception as e:
            print(f"[LLAMA] âš ï¸ HuggingFace embedding failed: {e}")
            # OpenAIåŸ‹ã‚è¾¼ã¿ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                from llama_index.embeddings.openai import OpenAIEmbedding
                self.embed_model = OpenAIEmbedding()
                print(f"[LLAMA] ğŸ”„ Fallback to OpenAI embedding")
            except Exception as e2:
                print(f"[LLAMA] âŒ All embedding options failed: {e2}")
                raise e2

        # 3. ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        if CHROMADB_AVAILABLE:
            self.chroma_client = chromadb.PersistentClient(
                path=self.chroma_persist_directory
            )
            print(f"[LLAMA] âœ… ChromaDB client initialized")

            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ã¾ãŸã¯ä½œæˆ
            try:
                self.chroma_collection = self.chroma_client.get_collection(
                    name=self.collection_name
                )
                print(f"[LLAMA] âœ… Existing collection loaded: {self.collection_name}")
            except:
                self.chroma_collection = self.chroma_client.create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "cosine"}
                )
                print(f"[LLAMA] âœ… New collection created: {self.collection_name}")

        # 4. LlamaIndex ChromaVectorStoreåˆæœŸåŒ–
        self.vector_store = ChromaVectorStore(
            chroma_collection=self.chroma_collection
        )
        print("[LLAMA] âœ… LlamaIndex ChromaVectorStore initialized")

        # 5. StorageContextåˆæœŸåŒ–
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )
        print("[LLAMA] âœ… StorageContext initialized")

        # 6. ServiceContextåˆæœŸåŒ–ï¼ˆLlamaIndex v0.10+ï¼‰
        try:
            from llama_index.core import Settings
            Settings.llm = self.llm
            Settings.embed_model = self.embed_model
            print("[LLAMA] âœ… Global Settings configured")
        except ImportError:
            # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            from llama_index.core import ServiceContext
            self.service_context = ServiceContext.from_defaults(
                llm=self.llm,
                embed_model=self.embed_model
            )
            print("[LLAMA] âœ… ServiceContext initialized (legacy)")

        # 7. VectorStoreIndexåˆæœŸåŒ–
        try:
            # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            self.index = VectorStoreIndex.from_vector_store(
                vector_store=self.vector_store,
                storage_context=self.storage_context
            )
            print("[LLAMA] âœ… Existing VectorStoreIndex loaded")
        except Exception as e:
            print(f"[LLAMA] ğŸ“ Creating new VectorStoreIndex: {e}")
            # æ–°è¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§åˆæœŸåŒ–ï¼‰
            self.index = VectorStoreIndex(
                nodes=[],
                storage_context=self.storage_context
            )
            print("[LLAMA] âœ… New VectorStoreIndex created")

        # 8. QueryEngineåˆæœŸåŒ–
        self.query_engine = self.index.as_query_engine(
            similarity_top_k=10,
            response_mode="compact"
        )
        print("[LLAMA] âœ… QueryEngine initialized")

        # 9. RetrieveråˆæœŸåŒ–
        self.retriever = self.index.as_retriever(
            similarity_top_k=10
        )
        print("[LLAMA] âœ… Retriever initialized")

    def add_documents(self, documents: List[Union[str, Dict[str, Any]]]) -> bool:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’LlamaIndexã«è¿½åŠ 

        Args:
            documents: è¿½åŠ ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ

        Returns:
            bool: è¿½åŠ æˆåŠŸãƒ•ãƒ©ã‚°
        """
        if not self.is_initialized:
            print("[LLAMA] âš ï¸ Engine not initialized - cannot add documents")
            return False

        try:
            llama_docs = []

            for doc in documents:
                if isinstance(doc, str):
                    llama_doc = LlamaDocument(
                        text=doc,
                        metadata={
                            "timestamp": datetime.now().isoformat(),
                            "source": "uma3_system"
                        }
                    )
                elif isinstance(doc, dict):
                    text = doc.get("text", doc.get("page_content", ""))
                    metadata = doc.get("metadata", {})
                    metadata.update({
                        "timestamp": datetime.now().isoformat(),
                        "source": "uma3_system"
                    })
                    llama_doc = LlamaDocument(text=text, metadata=metadata)
                else:
                    continue

                llama_docs.append(llama_doc)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
            self.index.insert_documents(llama_docs)
            print(f"[LLAMA] âœ… Added {len(llama_docs)} documents to index")
            return True

        except Exception as e:
            print(f"[LLAMA] âŒ Error adding documents: {e}")
            return False

    def query(
        self,
        query_text: str,
        top_k: int = 5,
        response_mode: str = "compact"
    ) -> Optional[Response]:
        """
        LlamaIndex ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚¯ã‚¨ãƒªå®Ÿè¡Œ

        Args:
            query_text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            top_k: å–å¾—ã™ã‚‹çµæœæ•°
            response_mode: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ¼ãƒ‰

        Returns:
            LlamaIndex Response ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if not self.is_initialized:
            print("[LLAMA] âš ï¸ Engine not initialized - cannot execute query")
            return None

        try:
            # å‹•çš„ã«top_kã‚’è¨­å®š
            query_engine = self.index.as_query_engine(
                similarity_top_k=top_k,
                response_mode=response_mode
            )

            response = query_engine.query(query_text)
            print(f"[LLAMA] âœ… Query executed: '{query_text[:50]}...'")
            return response

        except Exception as e:
            print(f"[LLAMA] âŒ Query error: {e}")
            return None

    def retrieve(self, query_text: str, top_k: int = 5) -> List[NodeWithScore]:
        """
        é¡ä¼¼æ–‡æ›¸ã®æ¤œç´¢ï¼ˆå›ç­”ç”Ÿæˆãªã—ï¼‰

        Args:
            query_text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            top_k: å–å¾—ã™ã‚‹çµæœæ•°

        Returns:
            NodeWithScore ã®ãƒªã‚¹ãƒˆ
        """
        if not self.is_initialized:
            print("[LLAMA] âš ï¸ Engine not initialized - cannot retrieve")
            return []

        try:
            # å‹•çš„ã«top_kã‚’è¨­å®š
            retriever = self.index.as_retriever(similarity_top_k=top_k)
            nodes = retriever.retrieve(query_text)

            print(f"[LLAMA] âœ… Retrieved {len(nodes)} nodes for: '{query_text[:50]}...'")
            return nodes

        except Exception as e:
            print(f"[LLAMA] âŒ Retrieval error: {e}")
            return []

    def get_stats(self) -> Dict[str, Any]:
        """
        ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

        Returns:
            çµ±è¨ˆæƒ…å ±è¾æ›¸
        """
        if not self.is_initialized:
            return {"status": "not_initialized"}

        try:
            # ChromaDBã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ
            collection_count = self.chroma_collection.count() if hasattr(self.chroma_collection, 'count') else 0

            return {
                "status": "initialized",
                "llm_model": self.llm_model,
                "embedding_model": self.embedding_model_name,
                "collection_name": self.collection_name,
                "document_count": collection_count,
                "chroma_directory": self.chroma_persist_directory
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }


# LangChainäº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼
class LlamaIndexLangChainBridge:
    """
    LlamaIndexã¨LangChainã®æ©‹æ¸¡ã—ã‚¯ãƒ©ã‚¹
    LangChainã®Documentã‚’LlamaIndexã«å¤‰æ›ã—ã€é€†å¤‰æ›ã‚‚è¡Œã†
    """

    @staticmethod
    def langchain_to_llama_documents(langchain_docs: List[LangChainDocument]) -> List[LlamaDocument]:
        """LangChain Documentã‚’LlamaIndex Documentã«å¤‰æ›"""
        if not LLAMA_INDEX_AVAILABLE:
            return []

        llama_docs = []
        for doc in langchain_docs:
            llama_doc = LlamaDocument(
                text=doc.page_content,
                metadata=doc.metadata or {}
            )
            llama_docs.append(llama_doc)

        return llama_docs

    @staticmethod
    def llama_to_langchain_documents(llama_nodes: List[NodeWithScore]) -> List[LangChainDocument]:
        """LlamaIndex NodeWithScoreã‚’LangChain Documentã«å¤‰æ›"""
        if not LANGCHAIN_AVAILABLE:
            return []

        langchain_docs = []
        for node_with_score in llama_nodes:
            doc = LangChainDocument(
                page_content=node_with_score.node.text,
                metadata={
                    **node_with_score.node.metadata,
                    "llama_score": node_with_score.score
                }
            )
            langchain_docs.append(doc)

        return langchain_docs


# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
def test_llama_index_integration():
    """LlamaIndexçµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("\n=== LlamaIndex Integration Test ===")

    if not LLAMA_INDEX_AVAILABLE:
        print("âŒ LlamaIndex not available - skipping test")
        return False

    try:
        # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        test_directory = "test_chroma_store"

        # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        engine = Uma3LlamaIndexEngine(
            chroma_persist_directory=test_directory,
            collection_name="test_collection"
        )

        if not engine.is_initialized:
            print("âŒ Engine initialization failed")
            return False

        # ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ 
        test_docs = [
            "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚",
            {"text": "LlamaIndexã¨LangChainã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚", "metadata": {"type": "test"}},
            "RAGã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚"
        ]

        success = engine.add_documents(test_docs)
        if not success:
            print("âŒ Document addition failed")
            return False

        # ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
        response = engine.query("ãƒ†ã‚¹ãƒˆ")
        if response is None:
            print("âŒ Query failed")
            return False

        print(f"âœ… Query response: {response.response[:100]}...")

        # çµ±è¨ˆæƒ…å ±å–å¾—
        stats = engine.get_stats()
        print(f"âœ… Stats: {stats}")

        print("âœ… LlamaIndex integration test passed!")
        return True

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        return False


if __name__ == "__main__":
    test_llama_index_integration()

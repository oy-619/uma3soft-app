"""
LlamaIndex RAG ã‚¨ãƒ³ã‚¸ãƒ³
LangChain + LlamaIndex ã‚’çµ„ã¿åˆã‚ã›ãŸé«˜åº¦ãªRAGã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import sys
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

try:
    from llama_index.core import (
        Document,
        QueryBundle,
        Settings,
        StorageContext,
        VectorStoreIndex,
        load_index_from_storage,
    )
    from llama_index.core.node_parser import SimpleNodeParser
    from llama_index.core.query_engine import BaseQueryEngine
    from llama_index.core.retrievers import BaseRetriever
    from llama_index.core.schema import NodeWithScore
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.llms.openai import OpenAI
    from llama_index.vector_stores.chroma import ChromaVectorStore
except ImportError as e:
    print(f"âš ï¸ LlamaIndex import error: {e}")
    print(
        "Please install: pip install llama-index llama-index-vector-stores-chroma llama-index-embeddings-huggingface llama-index-llms-openai"
    )
    sys.exit(1)

import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings


class Uma3RAGEngine:
    """
    LlamaIndex ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªRAG ã‚¨ãƒ³ã‚¸ãƒ³

    æ©Ÿèƒ½:
    - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢ï¼ˆVector + Semantic + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
    - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - æ™‚é–“è»¸æ¤œç´¢
    - ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç‰¹åŒ–æ¤œç´¢
    """

    def __init__(
        self,
        persist_directory: str = "Lesson25/uma3soft-app/db/chroma_store",
        embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        llm_model: str = "gpt-3.5-turbo",
        openai_api_key: Optional[str] = None,
    ):
        """
        RAG ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–

        Args:
            persist_directory: ChromaDBã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            embedding_model_name: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
            llm_model: LLMãƒ¢ãƒ‡ãƒ«å
            openai_api_key: OpenAI APIã‚­ãƒ¼
        """
        self.persist_directory = persist_directory
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")

        if not self.openai_api_key:
            raise ValueError("OpenAI API key is required")

        # LlamaIndex ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š
        Settings.embed_model = HuggingFaceEmbedding(model_name=embedding_model_name)
        Settings.llm = OpenAI(model=llm_model, api_key=self.openai_api_key)

        # ChromaDB ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.chroma_client = chromadb.PersistentClient(path=persist_directory)

        # LangChain ChromaDBï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®äº’æ›æ€§ï¼‰
        self.langchain_embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name
        )
        self.langchain_vectordb = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.langchain_embeddings,
        )

        # LlamaIndex VectorStore
        try:
            chroma_collection = self.chroma_client.get_or_create_collection("langchain")
            self.vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã¾ãŸã¯èª­ã¿è¾¼ã¿
            self._initialize_index()

        except Exception as e:
            print(f"âš ï¸ ChromaVectorStore initialization error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æ—¢å­˜ã®ChromaDBã‚’ä½¿ç”¨
            self.vector_store = None
            self.index = None
            print("Falling back to LangChain ChromaDB compatibility mode")

    def _initialize_index(self):
        """LlamaIndex ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®åˆæœŸåŒ–"""
        try:
            if self.vector_store:
                # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¿
                storage_context = StorageContext.from_defaults(
                    vector_store=self.vector_store
                )
                self.index = VectorStoreIndex.from_vector_store(
                    vector_store=self.vector_store, storage_context=storage_context
                )
                print("âœ… LlamaIndex initialized from existing ChromaDB")
            else:
                self.index = None
                print("âš ï¸ LlamaIndex index not available, using compatibility mode")

        except Exception as e:
            print(f"âš ï¸ Index initialization error: {e}")
            self.index = None

    def hybrid_search(
        self,
        query: str,
        k: int = 5,
        score_threshold: float = 0.0,
        metadata_filters: Optional[Dict[str, Any]] = None,
        include_schedule_data: bool = True,
        time_range_days: Optional[int] = None,
    ) -> List[Document]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆVector + Semantic + ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹çµæœæ•°
            score_threshold: ã‚¹ã‚³ã‚¢é–¾å€¤
            metadata_filters: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶
            include_schedule_data: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹ã‹
            time_range_days: æ™‚é–“ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ—¥æ•°ï¼‰

        Returns:
            æ¤œç´¢çµæœã®Documentãƒªã‚¹ãƒˆ
        """
        print(f"[RAG] Hybrid search: '{query}' (k={k}, threshold={score_threshold})")

        results = []

        # LlamaIndex ã«ã‚ˆã‚‹æ¤œç´¢
        if self.index:
            try:
                # æ™‚é–“ãƒ•ã‚£ãƒ«ã‚¿ã®è¨­å®š
                filters = {}
                if time_range_days:
                    current_date = datetime.now()
                    start_date = current_date - timedelta(days=time_range_days)
                    filters["timestamp"] = {">=": start_date.isoformat()}

                if metadata_filters:
                    filters.update(metadata_filters)

                # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
                query_engine = self.index.as_query_engine(
                    similarity_top_k=k,
                    response_mode="no_text",  # æ¤œç´¢çµæœã®ã¿å–å¾—
                )

                response = query_engine.query(query)

                # çµæœã‚’Documentå½¢å¼ã«å¤‰æ›
                if hasattr(response, "source_nodes"):
                    for node_with_score in response.source_nodes:
                        if node_with_score.score >= score_threshold:
                            doc = Document(
                                text=node_with_score.node.text,
                                metadata=node_with_score.node.metadata or {},
                            )
                            results.append(doc)

                print(f"[RAG] LlamaIndex search returned {len(results)} results")

            except Exception as e:
                print(f"âš ï¸ LlamaIndex search error: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: LangChain ChromaDB æ¤œç´¢
        if len(results) < k // 2:  # çµæœãŒå°‘ãªã„å ´åˆã¯è£œå®Œ
            try:
                langchain_results = self._langchain_fallback_search(
                    query, k=k - len(results), score_threshold=score_threshold
                )

                # é‡è¤‡é™¤å»ã—ã¦è¿½åŠ 
                existing_texts = {doc.text for doc in results}
                for lc_doc in langchain_results:
                    if lc_doc.page_content not in existing_texts:
                        doc = Document(
                            text=lc_doc.page_content, metadata=lc_doc.metadata
                        )
                        results.append(doc)

                print(
                    f"[RAG] Added {len(langchain_results)} results from LangChain fallback"
                )

            except Exception as e:
                print(f"âš ï¸ LangChain fallback error: {e}")

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç‰¹åŒ–æ¤œç´¢ã®è¿½åŠ 
        if include_schedule_data and self._is_schedule_query(query):
            schedule_results = self._schedule_enhanced_search(query, k=2)
            for sched_doc in schedule_results:
                if sched_doc.text not in {doc.text for doc in results}:
                    results.append(sched_doc)

        # çµæœã®å¾Œå‡¦ç†
        results = self._post_process_results(results, query)

        print(f"[RAG] Final results: {len(results)} documents")
        return results[:k]  # æœ€å¤§kä»¶ã¾ã§

    def _langchain_fallback_search(
        self, query: str, k: int = 5, score_threshold: float = 0.0
    ):
        """LangChain ChromaDB ã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢"""
        return self.langchain_vectordb.similarity_search_with_score(query, k=k)

    def _is_schedule_query(self, query: str) -> bool:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é–¢é€£ã‚¯ã‚¨ãƒªã®åˆ¤å®š"""
        schedule_keywords = [
            "äºˆå®š",
            "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«",
            "å¤§ä¼š",
            "ç·´ç¿’",
            "è©¦åˆ",
            "ãƒªãƒ¼ã‚°",
            "ä»Šæ—¥",
            "æ˜æ—¥",
            "æ¥é€±",
            "ä»Šé€±",
            "é€±æœ«",
            "ã„ã¤",
            "æ—¥ç¨‹",
        ]
        return any(keyword in query for keyword in schedule_keywords)

    def _schedule_enhanced_search(self, query: str, k: int = 2) -> List[Document]:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç‰¹åŒ–æ¤œç´¢"""
        results = []
        try:
            # [ãƒãƒ¼ãƒˆ]ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆçš„ã«æ¤œç´¢
            note_query = f"[ãƒãƒ¼ãƒˆ] {query}"
            note_results = self.langchain_vectordb.similarity_search(note_query, k=k)

            for doc in note_results:
                if "[ãƒãƒ¼ãƒˆ]" in doc.page_content:
                    results.append(
                        Document(text=doc.page_content, metadata=doc.metadata)
                    )

            print(f"[RAG] Schedule enhanced search found {len(results)} note documents")

        except Exception as e:
            print(f"âš ï¸ Schedule enhanced search error: {e}")

        return results

    def _post_process_results(
        self, results: List[Document], query: str
    ) -> List[Document]:
        """æ¤œç´¢çµæœã®å¾Œå‡¦ç†ï¼ˆé‡è¤‡é™¤å»ã€é–¢é€£åº¦ã‚½ãƒ¼ãƒˆç­‰ï¼‰"""
        # é‡è¤‡é™¤å»
        seen_texts = set()
        unique_results = []

        for doc in results:
            text_signature = doc.text[:100]  # æœ€åˆã®100æ–‡å­—ã§é‡è¤‡åˆ¤å®š
            if text_signature not in seen_texts:
                seen_texts.add(text_signature)
                unique_results.append(doc)

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é–¢é€£ã®å ´åˆã¯[ãƒãƒ¼ãƒˆ]ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
        if self._is_schedule_query(query):
            note_docs = [doc for doc in unique_results if "[ãƒãƒ¼ãƒˆ]" in doc.text]
            other_docs = [doc for doc in unique_results if "[ãƒãƒ¼ãƒˆ]" not in doc.text]
            unique_results = note_docs + other_docs

        return unique_results

    def get_query_engine(self, **kwargs) -> Optional[BaseQueryEngine]:
        """LlamaIndex QueryEngine ã®å–å¾—"""
        if self.index:
            return self.index.as_query_engine(**kwargs)
        return None

    def get_retriever(self, **kwargs) -> Optional[BaseRetriever]:
        """LlamaIndex Retriever ã®å–å¾—"""
        if self.index:
            return self.index.as_retriever(**kwargs)
        return None

    def add_documents(self, documents: List[Document]) -> bool:
        """æ–‡æ›¸ã®è¿½åŠ """
        try:
            if self.index and documents:
                # LlamaIndex ã«æ–‡æ›¸ã‚’è¿½åŠ 
                for doc in documents:
                    self.index.insert(doc)
                print(f"âœ… Added {len(documents)} documents to LlamaIndex")
                return True
        except Exception as e:
            print(f"âš ï¸ Document addition error: {e}")

        return False

    def get_analytics(self, query: str) -> Dict[str, Any]:
        """æ¤œç´¢åˆ†ææƒ…å ±ã®å–å¾—"""
        analytics = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "is_schedule_query": self._is_schedule_query(query),
            "available_engines": {
                "llamaindex": self.index is not None,
                "langchain": self.langchain_vectordb is not None,
            },
        }

        return analytics


def test_rag_engine():
    """RAG ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    try:
        print("ğŸ§ª Testing Uma3RAGEngine...")

        # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        rag_engine = Uma3RAGEngine()

        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
        test_queries = [
            "ä»Šé€±ã®äºˆå®šã‚’æ•™ãˆã¦",
            "ç¾½æ‘ãƒ©ã‚¤ã‚ªãƒ³ã‚ºã®è©¦åˆã¯ã„ã¤ï¼Ÿ",
            "ç·´ç¿’ã®äºˆå®šã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
        ]

        for query in test_queries:
            print(f"\nğŸ“ Testing query: '{query}'")
            results = rag_engine.hybrid_search(query, k=3)

            print(f"Results: {len(results)}")
            for i, doc in enumerate(results, 1):
                print(f"  {i}. {doc.text[:100]}...")

        print("âœ… RAG Engine test completed")

    except Exception as e:
        print(f"âŒ RAG Engine test failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    test_rag_engine()

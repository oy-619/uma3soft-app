"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¼šè©±å†…å®¹ã®è©³ç´°åˆ†æã¨å®Ÿéš›ã®é¸æ‰‹åæŠ½å‡º
"""

import sqlite3
import re
import json
from datetime import datetime
from typing import List, Dict, Set, Optional
import os

class DetailedPlayerNameAnalyzer:
    """è©³ç´°é¸æ‰‹ååˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, db_path: str):
        self.db_path = db_path

        # æ—¢å­˜ã®13å
        self.existing_players = [
            "é™¸åŠŸ", "æ¹Š", "éŒ¬", "å—", "çµ±å¸", "æ˜¥è¼", "æ–°",
            "ç”±çœ", "å¿ƒå¯§", "å”¯æµ¬", "æœ‹æ¨¹", "ä½‘å¤š", "ç©‚ç¾"
        ]

        # å®Ÿéš›ã®æ—¥æœ¬äººåãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
        self.strict_name_patterns = [
            r'([ä¸€-é¾¯]{2,3})(?:é¸æ‰‹|å›|ã•ã‚“)(?:ãŒ|ã¯|ã‚‚|ã«ã¤ã„ã¦|ã®)',  # æ•¬ç§°ä»˜ã
            r'([ä¸€-é¾¯]{2,3})(?:é¸æ‰‹|å›|ã•ã‚“)(?:ã§ã™|ã )',  # æ–­å®šå½¢
            r'([ä¸€-é¾¯]{2,3})(?:é¸æ‰‹|å›|ã•ã‚“)(?:ã‚’|ã«|ã§)',  # åŠ©è©ä»˜ã
            r'(?:æŠ•æ‰‹|æ•æ‰‹|å†…é‡æ‰‹|å¤–é‡æ‰‹)ã®([ä¸€-é¾¯]{2,3})',  # ãƒã‚¸ã‚·ãƒ§ãƒ³
            r'(?:ã‚­ãƒ£ãƒ—ãƒ†ãƒ³|ã‚³ãƒ¼ãƒ|ç›£ç£)ã®([ä¸€-é¾¯]{2,3})',  # å½¹è·
            r'([ä¸€-é¾¯]{2,3})(?:ãŒæŠ•ã’|ãŒæ‰“ã£|ãŒèµ°ã£|ãŒå®ˆã£)',  # å‹•ä½œ
            r'([ä¸€-é¾¯]{2,3})(?:ã®æˆç¸¾|ã®ãƒ’ãƒƒãƒˆ|ã®å¾—ç‚¹|ã®ã‚¨ãƒ©ãƒ¼)',  # æˆç¸¾
        ]

        # ç¢ºå®Ÿã«é™¤å¤–ã™ã¹ãèª
        self.definitely_not_names = {
            'å‚åŠ ', 'ç™»éŒ²', 'å‡ºå ´', 'è©¦åˆ', 'ç·´ç¿’', 'å¤§ä¼š', 'å„ªå‹', 'æº–å„ªå‹',
            'æˆç¸¾', 'çµæœ', 'è¨˜éŒ²', 'å¾—ç‚¹', 'å¤±ç‚¹', 'ãƒ’ãƒƒãƒˆ', 'ã‚¨ãƒ©ãƒ¼',
            'ä»Šæ—¥', 'æ˜æ—¥', 'æ˜¨æ—¥', 'ä»Šå¹´', 'æ¥å¹´', 'å»å¹´', 'æœ€è¿‘',
            'é¸æ‰‹', 'ãƒãƒ¼ãƒ ', 'ãƒ¡ãƒ³ãƒãƒ¼', 'ã‚³ãƒ¼ãƒ', 'ç›£ç£', 'ã‚­ãƒ£ãƒ—ãƒ†ãƒ³',
            'å°å­¦ç”Ÿ', 'ä¸­å­¦ç”Ÿ', 'é«˜æ ¡ç”Ÿ', 'å¤§å­¦ç”Ÿ', 'ç¤¾ä¼šäºº',
            'æŠ•æ‰‹', 'æ•æ‰‹', 'å†…é‡æ‰‹', 'å¤–é‡æ‰‹', 'ãƒ”ãƒƒãƒãƒ£ãƒ¼', 'ã‚­ãƒ£ãƒƒãƒãƒ£ãƒ¼',
            'ä¸€ç•ª', 'äºŒç•ª', 'ä¸‰ç•ª', 'å››ç•ª', 'äº”ç•ª', 'å…­ç•ª', 'ä¸ƒç•ª', 'å…«ç•ª', 'ä¹ç•ª',
            'æ´»èº', 'æˆé•·', 'åŠªåŠ›', 'é ‘å¼µ', 'ä¸Šé”', 'å‘ä¸Š', 'æ”¹å–„',
            'æƒ…å ±', 'è©³ç´°', 'å…·ä½“', 'ä¸€èˆ¬', 'å…¨ä½“', 'éƒ¨åˆ†', 'å€‹åˆ¥', 'ç‰¹åˆ¥',
            'é¦¬ä¸‰', 'ã‚½ãƒ•ãƒˆ', 'ã‚½ãƒ•ãƒˆãƒœãƒ¼ãƒ«', 'çƒå ´', 'ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰'
        }

        # å®Ÿåœ¨ã—ãã†ãªæ—¥æœ¬äººã®å§“
        self.common_surnames = {
            'ç”°ä¸­', 'ä½è—¤', 'éˆ´æœ¨', 'é«˜æ©‹', 'æ¸¡è¾º', 'ä¼Šè—¤', 'å±±ç”°', 'ä¸­æ‘',
            'å°æ—', 'åŠ è—¤', 'å‰ç”°', 'å±±æœ¬', 'æ¾æœ¬', 'äº•ä¸Š', 'æœ¨æ‘', 'æ—',
            'æ¸…æ°´', 'å±±å´', 'æ£®', 'æ± ç”°', 'æ©‹æœ¬', 'é˜¿éƒ¨', 'çŸ³å·', 'å‰ç”°',
            'è—¤ç”°', 'å²¡ç”°', 'å¾Œè—¤', 'é•·è°·å·', 'æ‘ä¸Š', 'è¿‘è—¤', 'çŸ³ç”°', 'æ–è—¤',
            'åŸç”°', 'é’æœ¨', 'ç«¹å†…', 'è¥¿ç”°', 'ä»Šäº•', 'é‡ç”°', 'æ°´é‡', 'èŠåœ°'
        }

    def analyze_all_conversation_content(self):
        """å…¨ä¼šè©±å†…å®¹ã®è©³ç´°åˆ†æ"""
        print("ğŸ“‹ å…¨ä¼šè©±å†…å®¹è©³ç´°åˆ†æ")
        print("=" * 40)

        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # å…¨ä¼šè©±ã‚’å–å¾—
            cursor.execute("""
                SELECT id, content, metadata, timestamp, user_id
                FROM conversation_history
                WHERE content IS NOT NULL AND content != ''
                ORDER BY timestamp DESC
            """)

            conversations = cursor.fetchall()
            print(f"ğŸ“Š ç·ä¼šè©±æ•°: {len(conversations)} ä»¶")

            # å®Ÿéš›ã®ä¼šè©±å†…å®¹ã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            print(f"\nğŸ“ ä¼šè©±å†…å®¹ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€æ–°10ä»¶ï¼‰:")
            print("-" * 60)

            for i, (id, content, metadata, timestamp, user_id) in enumerate(conversations[:10], 1):
                print(f"{i:2d}. [{timestamp}] (ID:{id})")
                print(f"    å†…å®¹: {content[:150]}...")
                if metadata:
                    try:
                        meta = json.loads(metadata) if metadata else {}
                        print(f"    ãƒ¡ã‚¿: {str(meta)[:100]}...")
                    except:
                        print(f"    ãƒ¡ã‚¿: {metadata[:100]}...")
                print()

            return conversations

        except Exception as e:
            print(f"âŒ ä¼šè©±åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return []
        finally:
            if 'conn' in locals():
                conn.close()

    def extract_potential_names_from_conversations(self, conversations: List[tuple]):
        """ä¼šè©±ã‹ã‚‰æ½œåœ¨çš„ãªåå‰ã‚’æŠ½å‡º"""
        print(f"\nğŸ” æ½œåœ¨çš„åå‰æŠ½å‡ºåˆ†æ")
        print("=" * 30)

        potential_names = set()
        name_contexts = {}

        # å„ä¼šè©±ã‚’è©³ç´°åˆ†æ
        for id, content, metadata, timestamp, user_id in conversations:
            text = content or ''

            # æ—¢å­˜é¸æ‰‹ä»¥å¤–ã®2-3æ–‡å­—ã®æ¼¢å­—ã‚’å…¨ã¦æŠ½å‡º
            all_possible_names = re.findall(r'([ä¸€-é¾¯]{2,3})', text)

            for possible_name in all_possible_names:
                # æ—¢å­˜é¸æ‰‹ã¯é™¤å¤–
                if possible_name in self.existing_players:
                    continue

                # ç¢ºå®Ÿã«é™¤å¤–ã™ã¹ãèªã¯é™¤å¤–
                if possible_name in self.definitely_not_names:
                    continue

                # æ–‡è„ˆã‚’ãƒã‚§ãƒƒã‚¯
                if self.check_name_context(possible_name, text):
                    potential_names.add(possible_name)

                    if possible_name not in name_contexts:
                        name_contexts[possible_name] = []

                    name_contexts[possible_name].append({
                        'conversation_id': id,
                        'context': text,
                        'timestamp': timestamp,
                        'user_id': user_id
                    })

        print(f"ğŸ“Š æŠ½å‡ºã•ã‚ŒãŸæ½œåœ¨çš„åå‰: {len(potential_names)} å€‹")

        # å„åå‰ã®æ–‡è„ˆã‚’åˆ†æ
        analyzed_names = {}
        for name in potential_names:
            contexts = name_contexts[name]
            analysis = self.analyze_name_likelihood(name, contexts)
            analyzed_names[name] = analysis

            print(f"\nğŸ” å€™è£œ: '{name}' (ä¿¡é ¼åº¦: {analysis['confidence_score']:.1f})")
            print(f"   å‡ºç¾å›æ•°: {len(contexts)} å›")
            print(f"   åˆ¤å®šè¦å› : {', '.join(analysis['factors'])}")

            # ä»£è¡¨çš„ãªæ–‡è„ˆã‚’è¡¨ç¤º
            for i, ctx in enumerate(contexts[:2], 1):
                context_snippet = self.extract_context_snippet(name, ctx['context'])
                print(f"   æ–‡è„ˆ{i}: ...{context_snippet}...")

        # ä¿¡é ¼åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        high_confidence_names = {
            name: analysis for name, analysis in analyzed_names.items()
            if analysis['confidence_score'] >= 3.0
        }

        print(f"\nâœ… é«˜ä¿¡é ¼åº¦åå‰å€™è£œ: {len(high_confidence_names)} å€‹")
        for name, analysis in high_confidence_names.items():
            print(f"   ğŸŒŸ {name} (ä¿¡é ¼åº¦: {analysis['confidence_score']:.1f})")

        return high_confidence_names, name_contexts

    def check_name_context(self, name: str, text: str) -> bool:
        """åå‰ã®æ–‡è„ˆã‚’ãƒã‚§ãƒƒã‚¯"""
        # åå‰ã‚‰ã—ã„æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³
        name_context_patterns = [
            f'{name}é¸æ‰‹',
            f'{name}å›',
            f'{name}ã•ã‚“',
            f'{name}ã¡ã‚ƒã‚“',
            f'{name}ã«ã¤ã„ã¦',
            f'{name}ã®',
            f'{name}ã¯',
            f'{name}ãŒ',
            f'{name}ã‚’'
        ]

        return any(pattern in text for pattern in name_context_patterns)

    def extract_context_snippet(self, name: str, text: str) -> str:
        """åå‰å‘¨è¾ºã®æ–‡è„ˆã‚’æŠ½å‡º"""
        name_index = text.find(name)
        if name_index == -1:
            return text[:50]

        start = max(0, name_index - 30)
        end = min(len(text), name_index + len(name) + 30)
        return text[start:end]

    def analyze_name_likelihood(self, name: str, contexts: List[Dict]) -> Dict:
        """åå‰ã‚‰ã—ã•ã®åˆ†æ"""
        analysis = {
            'confidence_score': 0.0,
            'factors': [],
            'context_count': len(contexts)
        }

        # 1. æ—¢çŸ¥ã®å§“ã‚’ãƒã‚§ãƒƒã‚¯
        if any(name.startswith(surname) for surname in self.common_surnames):
            analysis['confidence_score'] += 2.0
            analysis['factors'].append('æ—¢çŸ¥ã®å§“')

        # 2. æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
        if len(name) == 2 or len(name) == 3:
            analysis['confidence_score'] += 1.0
            analysis['factors'].append('é©åˆ‡ãªæ–‡å­—æ•°')

        # 3. æ•¬ç§°ã§ã®ä½¿ç”¨ãƒã‚§ãƒƒã‚¯
        honorific_count = 0
        for ctx in contexts:
            text = ctx['context']
            if f'{name}é¸æ‰‹' in text or f'{name}å›' in text or f'{name}ã•ã‚“' in text:
                honorific_count += 1

        if honorific_count > 0:
            analysis['confidence_score'] += min(honorific_count * 0.5, 2.0)
            analysis['factors'].append(f'æ•¬ç§°ä½¿ç”¨{honorific_count}å›')

        # 4. æ–‡è„ˆã§ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        consistent_contexts = 0
        for ctx in contexts:
            text = ctx['context']
            if any(keyword in text for keyword in ['é¸æ‰‹', 'ãƒãƒ¼ãƒ ', 'é¦¬ä¸‰ã‚½ãƒ•ãƒˆ', 'è©¦åˆ', 'ç·´ç¿’']):
                consistent_contexts += 1

        if consistent_contexts > 0:
            analysis['confidence_score'] += min(consistent_contexts * 0.3, 1.5)
            analysis['factors'].append(f'ä¸€è²«ã—ãŸæ–‡è„ˆ{consistent_contexts}å›')

        # 5. å‡ºç¾é »åº¦
        if len(contexts) >= 3:
            analysis['confidence_score'] += 1.0
            analysis['factors'].append('ååˆ†ãªå‡ºç¾é »åº¦')
        elif len(contexts) >= 2:
            analysis['confidence_score'] += 0.5
            analysis['factors'].append('é©åº¦ãªå‡ºç¾é »åº¦')

        return analysis

    def create_verified_player_list(self, high_confidence_names: Dict):
        """æ¤œè¨¼æ¸ˆã¿é¸æ‰‹ãƒªã‚¹ãƒˆä½œæˆ"""
        print(f"\nâœ… æ¤œè¨¼æ¸ˆã¿é¸æ‰‹ãƒªã‚¹ãƒˆä½œæˆ")
        print("=" * 30)

        # æ—¢å­˜é¸æ‰‹ + é«˜ä¿¡é ¼åº¦æ–°è¦é¸æ‰‹
        verified_new_players = list(high_confidence_names.keys())
        all_verified_players = self.existing_players + verified_new_players

        verified_database = {
            'verification_info': {
                'verification_date': datetime.now().isoformat(),
                'original_players': len(self.existing_players),
                'verified_new_players': len(verified_new_players),
                'total_verified_players': len(all_verified_players),
                'team_name': 'é¦¬ä¸‰ã‚½ãƒ•ãƒˆ'
            },
            'original_13_players': self.existing_players,
            'verified_new_players': verified_new_players,
            'all_verified_players': all_verified_players,
            'confidence_analysis': high_confidence_names
        }

        print(f"ğŸ“Š æ¤œè¨¼çµæœ:")
        print(f"   æ—¢å­˜é¸æ‰‹: {len(self.existing_players)} å")
        print(f"   æ¤œè¨¼æ¸ˆã¿æ–°è¦é¸æ‰‹: {len(verified_new_players)} å")
        print(f"   ç·æ¤œè¨¼æ¸ˆã¿é¸æ‰‹: {len(all_verified_players)} å")

        if verified_new_players:
            print(f"\nğŸ¯ æ¤œè¨¼æ¸ˆã¿æ–°è¦é¸æ‰‹:")
            for player in verified_new_players:
                confidence = high_confidence_names[player]['confidence_score']
                factors = high_confidence_names[player]['factors']
                print(f"   âœ… {player} (ä¿¡é ¼åº¦: {confidence:.1f}) - {', '.join(factors)}")
        else:
            print(f"\nâš ï¸ æ¤œè¨¼æ¸ˆã¿æ–°è¦é¸æ‰‹ãªã—")
            print(f"   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯æ—¢å­˜ã®13åä»¥å¤–ã®ç¢ºå®Ÿãªé¸æ‰‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        db_path = 'verified_player_database.json'
        with open(db_path, 'w', encoding='utf-8') as f:
            json.dump(verified_database, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ æ¤œè¨¼æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜: {db_path}")

        return verified_database

    def generate_detailed_analysis_report(self, conversations: List[tuple], verified_database: Dict):
        """è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"\nğŸ“Š è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        print("=" * 30)

        report = {
            'analysis_metadata': {
                'analysis_date': datetime.now().isoformat(),
                'total_conversations_analyzed': len(conversations),
                'analysis_methods': [
                    'strict_pattern_matching',
                    'context_analysis',
                    'confidence_scoring',
                    'name_likelihood_assessment'
                ]
            },
            'findings': {
                'existing_players_confirmed': len(self.existing_players),
                'new_players_discovered': len(verified_database['verified_new_players']),
                'total_verified_players': len(verified_database['all_verified_players'])
            },
            'analysis_summary': {
                'database_contains_mainly_existing_13': True,
                'additional_player_data_limited': len(verified_database['verified_new_players']) == 0,
                'recommendation': 'focus_on_existing_13_players'
            },
            'data_quality_assessment': {
                'conversation_quality': 'system_generated_responses',
                'player_name_mentions': 'primarily_existing_13',
                'new_player_evidence': 'insufficient_for_confident_identification'
            }
        }

        print(f"ğŸ“‹ åˆ†æçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ğŸ” åˆ†æå¯¾è±¡ä¼šè©±: {report['analysis_metadata']['total_conversations_analyzed']} ä»¶")
        print(f"   âœ… æ—¢å­˜é¸æ‰‹ç¢ºèª: {report['findings']['existing_players_confirmed']} å")
        print(f"   ğŸ†• æ–°è¦é¸æ‰‹ç™ºè¦‹: {report['findings']['new_players_discovered']} å")
        print(f"   ğŸ† ç·æ¤œè¨¼é¸æ‰‹æ•°: {report['findings']['total_verified_players']} å")

        if report['findings']['new_players_discovered'] == 0:
            print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
            print(f"   - ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯æ—¢å­˜ã®13åä»¥å¤–ã®ç¢ºå®Ÿãªé¸æ‰‹æƒ…å ±ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            print(f"   - æ—¢å­˜ã®13åã®é¸æ‰‹æƒ…å ±ã‚’å¼·åŒ–ãƒ»è©³ç´°åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
            print(f"   - æ–°ãŸãªé¸æ‰‹æƒ…å ±ã¯å®Ÿéš›ã®ä¼šè©±ã‚„ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‹ã‚‰å­¦ç¿’ã™ã‚‹ã“ã¨ãŒå¿…è¦ã§ã™")

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = 'detailed_player_analysis_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

        return report

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ” è©³ç´°é¸æ‰‹ååˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    print(f"ğŸ“… å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    db_path = os.path.join('..', 'db', 'conversation_history.db')

    if not os.path.exists(db_path):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
        return

    # è©³ç´°åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    analyzer = DetailedPlayerNameAnalyzer(db_path)

    print(f"ğŸ“Š åˆ†æå¯¾è±¡æ—¢å­˜é¸æ‰‹: {len(analyzer.existing_players)}å")
    print(f"   {', '.join(analyzer.existing_players)}")
    print()

    # 1. å…¨ä¼šè©±å†…å®¹ã®è©³ç´°åˆ†æ
    conversations = analyzer.analyze_all_conversation_content()

    if not conversations:
        print("âŒ ä¼šè©±ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    # 2. æ½œåœ¨çš„åå‰ã®æŠ½å‡ºã¨åˆ†æ
    high_confidence_names, name_contexts = analyzer.extract_potential_names_from_conversations(conversations)

    # 3. æ¤œè¨¼æ¸ˆã¿é¸æ‰‹ãƒªã‚¹ãƒˆä½œæˆ
    verified_database = analyzer.create_verified_player_list(high_confidence_names)

    # 4. è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_detailed_analysis_report(conversations, verified_database)

    print(f"\nğŸ‰ è©³ç´°é¸æ‰‹ååˆ†æå®Œäº†ï¼")

    if verified_database['verified_new_players']:
        print(f"âœ¨ æ–°è¦ç™ºè¦‹é¸æ‰‹: {', '.join(verified_database['verified_new_players'])}")
        print(f"ğŸ“Š ç·é¸æ‰‹æ•°: {verified_database['verification_info']['total_verified_players']}å")
    else:
        print(f"ğŸ“Š ç¢ºèªæ¸ˆã¿é¸æ‰‹: æ—¢å­˜ã®{len(analyzer.existing_players)}åã®ã¿")
        print(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯è¿½åŠ ã®ç¢ºå®Ÿãªé¸æ‰‹æƒ…å ±ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

if __name__ == "__main__":
    main()

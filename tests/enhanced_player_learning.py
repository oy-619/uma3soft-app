"""
æ”¹è‰¯ç‰ˆï¼šã‚ˆã‚Šç²¾å¯†ãªé¸æ‰‹åå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
"""

import sqlite3
import re
import json
from datetime import datetime
from typing import List, Dict, Set
import os

class EnhancedPlayerNameLearningSystem:
    """æ”¹è‰¯ç‰ˆé¸æ‰‹åå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.learned_names = set()

        # å®Ÿéš›ã®ç«¶é¦¬é¨æ‰‹åãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.known_jockeys = {
            'æ­¦è±Š', 'ç¦æ°¸ç¥ä¸€', 'å·ç”°å°†é›…', 'æˆ¸å´åœ­å¤ª', 'å²©ç”°åº·èª ', 'æ± æ·»è¬™ä¸€', 'å’Œç”°ç«œäºŒ',
            'è—¤å²¡åº·å¤ª', 'æ¾å±±å¼˜å¹³', 'é®«å³¶å…‹é§¿', 'ä¸¸å±±å…ƒæ°—', 'æ¨ªå±±å…¸å¼˜', 'è›¯åæ­£ç¾©', 'å†…ç”°åšå¹¸',
            'ç”°è¾ºè£•ä¿¡', 'çŸ³æ©‹è„©', 'åŒ—æ‘å‹ä¸€', 'å¹¸è‹±æ˜', 'è—¤å²¡ä½‘ä»‹', 'å‰ç”°éš¼äºº', 'æŸ´å±±é›„ä¸€',
            'ä¸‰æµ¦çš‡æˆ', 'å¤§é‡æ‹“å¼¥', 'æ¾ç”°å¤§ä½œ', 'è±ç”°è£•äºŒ', 'é‡ä¸­æ‚ å¤ªéƒ', 'æ°¸å³¶ã¾ãªã¿',
            'å¤å·å‰æ´‹', 'è—¤ç”°èœä¸ƒå­', 'ä»Šæ‘è–å¥ˆ', 'è…åŸæ˜è‰¯', 'æ°¸é‡çŒ›è”µ', 'å‚äº•ç‘ æ˜Ÿ'
        }

        # ä¸€èˆ¬çš„ãªç«¶é¦¬ç”¨èªï¼ˆé™¤å¤–ç”¨ï¼‰
        self.racing_terms = {
            'ç«¶é¦¬', 'ç«¶èµ°', 'é¨ä¹—', 'èª¿æ•™', 'å©èˆ', 'é¦¬ä¸»', 'ç”Ÿç”£', 'è¡€çµ±', 'é…åˆ',
            'ãƒ¬ãƒ¼ã‚¹', 'ã‚³ãƒ¼ã‚¹', 'èŠ', 'ãƒ€ãƒ¼ãƒˆ', 'è·é›¢', 'é‡è³', 'G1', 'G2', 'G3',
            'å‹åˆ©', 'å„ªå‹', 'å…¥ç€', 'ç€é †', 'é¦¬åˆ¸', 'å˜å‹', 'è¤‡å‹', 'é¦¬é€£', 'é¦¬å˜',
            'ä¸‰é€£è¤‡', 'ä¸‰é€£å˜', 'ãƒ¯ã‚¤ãƒ‰', 'æ é€£', 'æ å˜', 'WIN5'
        }

    def analyze_enhanced_database(self):
        """å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ"""
        print("ğŸ” å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æé–‹å§‹")
        print("=" * 50)

        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ã‚ˆã‚Šåºƒç¯„å›²ãªç«¶é¦¬é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
            racing_keywords = [
                'ç«¶é¦¬', 'é¨æ‰‹', 'é¨ä¹—', 'å‹åˆ©', 'ãƒ¬ãƒ¼ã‚¹', 'ç«¶èµ°', 'èª¿æ•™', 'å©èˆ',
                'é…è»Š', 'é¸æ‰‹', 'ãƒ‰ãƒ©ã‚¤ãƒãƒ¼', 'æ­¦è±Š', 'ç¦æ°¸', 'å·ç”°', 'æˆ¸å´',
                'G1', 'G2', 'G3', 'é‡è³', 'é¦¬åˆ¸', 'å˜å‹', 'è¤‡å‹'
            ]

            all_conversations = []

            for keyword in racing_keywords:
                cursor.execute("""
                    SELECT content, metadata, timestamp
                    FROM conversation_history
                    WHERE content LIKE ?
                    ORDER BY timestamp DESC
                """, (f'%{keyword}%',))

                keyword_conversations = cursor.fetchall()
                all_conversations.extend(keyword_conversations)
                print(f"   ğŸ” '{keyword}' ã§ {len(keyword_conversations)} ä»¶ã®ä¼šè©±ã‚’ç™ºè¦‹")

            # é‡è¤‡é™¤å»
            unique_conversations = list(set(all_conversations))
            print(f"ğŸ¯ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªç«¶é¦¬é–¢é€£ä¼šè©±: {len(unique_conversations)} ä»¶")

            # é¸æ‰‹åæŠ½å‡º
            extracted_names = self.enhanced_name_extraction(unique_conversations)

            return {
                'racing_conversations': unique_conversations,
                'extracted_names': extracted_names,
                'total_conversations': len(unique_conversations)
            }

        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            if 'conn' in locals():
                conn.close()

    def enhanced_name_extraction(self, conversations: List[tuple]) -> Set[str]:
        """å¼·åŒ–ã•ã‚ŒãŸåå‰æŠ½å‡º"""
        print("\nğŸ‡ å¼·åŒ–é¸æ‰‹åæŠ½å‡ºå‡¦ç†")
        print("-" * 30)

        extracted_names = set()

        # ã‚ˆã‚Šç²¾å¯†ãªåå‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        precise_patterns = [
            r'é¨æ‰‹\s*[ï¼š:]\s*([ä¸€-é¾¯]{2,4})',           # é¨æ‰‹: åå‰
            r'([ä¸€-é¾¯]{2,4})\s*é¨æ‰‹',                   # åå‰ é¨æ‰‹
            r'([ä¸€-é¾¯]{2,4})\s*é¸æ‰‹',                   # åå‰ é¸æ‰‹
            r'([ä¸€-é¾¯]{2,4})\s*ã‚¸ãƒ§ãƒƒã‚­ãƒ¼',             # åå‰ ã‚¸ãƒ§ãƒƒã‚­ãƒ¼
            r'([ä¸€-é¾¯]{2,4})\s*ãŒ\s*é¨ä¹—',              # åå‰ ãŒé¨ä¹—
            r'([ä¸€-é¾¯]{2,4})\s*ãŒ\s*å‹åˆ©',              # åå‰ ãŒå‹åˆ©
            r'([ä¸€-é¾¯]{2,4})\s*ãŒ\s*å„ªå‹',              # åå‰ ãŒå„ªå‹
            r'([ä¸€-é¾¯]{2,4})\s*ã®\s*é¨ä¹—',              # åå‰ ã®é¨ä¹—
            r'([ä¸€-é¾¯]{2,4})\s*ã«ã‚ˆã‚‹\s*å‹åˆ©',          # åå‰ ã«ã‚ˆã‚‹å‹åˆ©
        ]

        for content, metadata, timestamp in conversations:
            text = content or ''

            # æ—¢çŸ¥ã®é¨æ‰‹åã®ç›´æ¥æ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
            for jockey in self.known_jockeys:
                if jockey in text:
                    extracted_names.add(jockey)
                    print(f"   ğŸ† æ—¢çŸ¥é¨æ‰‹åæ¤œå‡º: '{jockey}'")

            # ç²¾å¯†ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            for pattern in precise_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    name = match.strip()

                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶
                    if (len(name) >= 2 and len(name) <= 4 and
                        name not in self.racing_terms and
                        not any(term in name for term in ['ã§ã™', 'ã¾ã™', 'ã™ã‚‹', 'ã—ãŸ', 'ã‚ã‚‹', 'ãªã‚‹']) and
                        self.is_likely_person_name(name)):

                        extracted_names.add(name)
                        print(f"   ğŸ“ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ: '{name}' (ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern})")

        # å€™è£œåã®ä¿¡é ¼åº¦è©•ä¾¡
        validated_names = self.validate_extracted_names(extracted_names, conversations)

        print(f"\nğŸ“Š æœ€çµ‚é¸æ‰‹å: {len(validated_names)} å€‹")
        return validated_names

    def is_likely_person_name(self, name: str) -> bool:
        """äººåã‚‰ã—ã•ã®åˆ¤å®š"""
        # ä¸€èˆ¬çš„ãªæ—¥æœ¬ã®å§“ã®ä¸€éƒ¨
        common_surname_chars = ['ç”°', 'ä¸­', 'ä½', 'è—¤', 'å±±', 'æœ¨', 'å·', 'äº•', 'æ‘', 'å³¶', 'åŸ', 'æœ¬', 'æ¾', 'æ—', 'æ± ', 'æ©‹', 'çŸ³', 'å‰', 'å¾Œ', 'å²¡']

        # å§“ã®æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
        has_surname_char = any(char in name for char in common_surname_chars)

        # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒå¤šã™ããªã„ã‹
        hiragana_katakana_count = len([c for c in name if 'ã' <= c <= 'ã‚“' or 'ã‚¡' <= c <= 'ãƒ³'])

        return has_surname_char or hiragana_katakana_count <= 1

    def validate_extracted_names(self, names: Set[str], conversations: List[tuple]) -> Set[str]:
        """æŠ½å‡ºã•ã‚ŒãŸåå‰ã®æ¤œè¨¼"""
        print("\nğŸ” åå‰å€™è£œæ¤œè¨¼")
        print("-" * 20)

        validated_names = set()

        for name in names:
            confidence_score = 0.0
            context_count = 0

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            for content, metadata, timestamp in conversations:
                text = content or ''
                if name in text:
                    context_count += 1

                    # ç«¶é¦¬é–¢é€£æ–‡è„ˆã®ç¢ºèª
                    racing_context_keywords = ['é¨æ‰‹', 'é¨ä¹—', 'å‹åˆ©', 'å„ªå‹', 'ãƒ¬ãƒ¼ã‚¹', 'ç«¶èµ°', 'ç«¶é¦¬']
                    context_score = sum(1 for keyword in racing_context_keywords if keyword in text)
                    confidence_score += context_score * 0.1

            # æ—¢çŸ¥é¨æ‰‹åã¯æœ€é«˜ã‚¹ã‚³ã‚¢
            if name in self.known_jockeys:
                confidence_score = 1.0

            # å‡ºç¾é »åº¦ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
            if context_count > 1:
                confidence_score += 0.2

            # åå‰ã®é•·ã•ã«ã‚ˆã‚‹èª¿æ•´
            if 2 <= len(name) <= 4:
                confidence_score += 0.1

            # æœ€çµ‚åˆ¤å®š
            if confidence_score >= 0.3:  # é–¾å€¤ã‚’ä¸‹ã’ã¦å¹…åºƒãå­¦ç¿’
                validated_names.add(name)
                print(f"   âœ… æ¤œè¨¼OK: '{name}' (ä¿¡é ¼åº¦: {confidence_score:.2f}, å‡ºç¾: {context_count}å›)")
            else:
                print(f"   âŒ æ¤œè¨¼NG: '{name}' (ä¿¡é ¼åº¦: {confidence_score:.2f}, å‡ºç¾: {context_count}å›)")

        return validated_names

    def create_enhanced_templates(self, validated_names: Set[str]) -> Dict[str, str]:
        """å¼·åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ"""
        print("\nğŸ“ å¼·åŒ–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ")
        print("-" * 30)

        templates = {}

        for name in validated_names:
            # åŸºæœ¬ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            templates[f'{name}ã«ã¤ã„ã¦'] = f'{name}ã«ã¤ã„ã¦ãŠè©±ã—ã—ã¾ã™ã€‚ã©ã®ã‚ˆã†ãªã“ã¨ã‚’çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ'
            templates[f'{name}é¸æ‰‹'] = f'{name}é¸æ‰‹ã®ã“ã¨ã§ã™ã­ã€‚ç«¶é¦¬ã«é–¢ã™ã‚‹ã“ã¨ã§ã—ãŸã‚‰ä½•ã§ã‚‚ãŠèããã ã•ã„ã€‚'
            templates[f'{name}é¨æ‰‹'] = f'{name}é¨æ‰‹ã«ã¤ï¿½ï¿½ï¿½ã¦ã”æ¡ˆå†…ã—ã¾ã™ã€‚é¨ä¹—æˆç¸¾ã‚„æœ€è¿‘ã®ãƒ¬ãƒ¼ã‚¹çµæœãªã©ã€ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ'

            # æ—¢çŸ¥é¨æ‰‹ã®å ´åˆã¯ã‚ˆã‚Šè©³ç´°ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            if name in self.known_jockeys:
                templates[f'{name}ã®æˆç¸¾'] = f'{name}é¨æ‰‹ã®æˆç¸¾ã«ã¤ã„ã¦ãŠç­”ãˆã—ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã©ã®æœŸé–“ã‚„å†…å®¹ã‚’ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ'
                templates[f'{name}ã®é¨ä¹—'] = f'{name}é¨æ‰‹ã®é¨ä¹—ã«ã¤ã„ã¦ã”æ¡ˆå†…ã—ã¾ã™ã€‚ã©ã®ãƒ¬ãƒ¼ã‚¹ã‚„é¦¬ã«ã¤ã„ã¦ãŠèãã«ãªã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ'
                templates[f'{name}ã®å‹åˆ©'] = f'{name}é¨æ‰‹ã®å‹åˆ©ã«ã¤ã„ã¦ãŠè©±ã—ã—ã¾ã™ã€‚é‡è³å‹åˆ©ã‚„æœ€è¿‘ã®æ´»èºã«ã¤ã„ã¦ãŠçŸ¥ã‚Šã«ãªã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ'

        print(f"âœ… ä½œæˆã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ•°: {len(templates)}")
        return templates

    def save_enhanced_results(self, results: Dict, output_dir: str):
        """å¼·åŒ–ã•ã‚ŒãŸçµæœä¿å­˜"""
        os.makedirs(output_dir, exist_ok=True)

        # å­¦ç¿’çµæœä¿å­˜
        learning_results = {
            'timestamp': datetime.now().isoformat(),
            'total_conversations_analyzed': results['total_conversations'],
            'extracted_names': list(results['extracted_names']),
            'known_jockeys_found': list(results['extracted_names'] & self.known_jockeys),
            'new_names_discovered': list(results['extracted_names'] - self.known_jockeys),
            'extraction_method': 'enhanced_pattern_matching_with_validation'
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        results_path = os.path.join(output_dir, 'enhanced_player_names.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(learning_results, f, ensure_ascii=False, indent=2)

        templates = self.create_enhanced_templates(results['extracted_names'])
        templates_path = os.path.join(output_dir, 'enhanced_player_templates.json')
        with open(templates_path, 'w', encoding='utf-8') as f:
            json.dump(templates, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ä¿å­˜å®Œäº†:")
        print(f"   ğŸ“Š å­¦ç¿’çµæœ: {results_path}")
        print(f"   ğŸ“ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {templates_path}")

        return learning_results, templates

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ‡ å¼·åŒ–ç‰ˆé¸æ‰‹åå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print(f"ğŸ“… å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
    db_path = os.path.join('..', 'db', 'conversation_history.db')

    if not os.path.exists(db_path):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
        return

    # å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 
    enhanced_system = EnhancedPlayerNameLearningSystem(db_path)

    # 1. å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æ
    analysis_results = enhanced_system.analyze_enhanced_database()

    if not analysis_results:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # 2. çµæœä¿å­˜
    output_dir = '.'
    learning_results, templates = enhanced_system.save_enhanced_results(analysis_results, output_dir)

    # 3. çµæœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“Š å¼·åŒ–å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼")
    print("-" * 50)
    print(f"   ğŸ¯ åˆ†æã—ãŸä¼šè©±æ•°: {analysis_results['total_conversations']}")
    print(f"   ğŸ‘¤ æŠ½å‡ºã•ã‚ŒãŸé¸æ‰‹å: {len(analysis_results['extracted_names'])}")
    print(f"   ğŸ† æ—¢çŸ¥é¨æ‰‹åç™ºè¦‹: {len(learning_results['known_jockeys_found'])}")
    print(f"   ğŸ†• æ–°ç™ºè¦‹é¸æ‰‹å: {len(learning_results['new_names_discovered'])}")
    print(f"   ğŸ“ ä½œæˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {len(templates)}")

    # ç™ºè¦‹ã•ã‚ŒãŸé¨æ‰‹åã‚’è¡¨ç¤º
    if learning_results['known_jockeys_found']:
        print(f"\nğŸ† ç™ºè¦‹ã•ã‚ŒãŸæ—¢çŸ¥é¨æ‰‹å:")
        for jockey in sorted(learning_results['known_jockeys_found']):
            print(f"      âœ… {jockey}")

    if learning_results['new_names_discovered']:
        print(f"\nğŸ†• æ–°ç™ºè¦‹ã®é¸æ‰‹åå€™è£œ:")
        for name in sorted(learning_results['new_names_discovered']):
            print(f"      ğŸ” {name}")

    print(f"\nğŸ‰ å¼·åŒ–ç‰ˆé¸æ‰‹åå­¦ç¿’å®Œäº†ï¼")

if __name__ == "__main__":
    main()

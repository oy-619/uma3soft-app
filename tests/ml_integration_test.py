#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ©Ÿæ¢°å­¦ç¿’çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
Uma3 Softwareãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç”¨ã®åŒ…æ‹¬çš„MLã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
"""

import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’å–å¾—
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))
sys.path.append(str(PROJECT_ROOT / 'src'))

from realtime_ml_analyzer import Uma3RealTimeMLAnalyzer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{PROJECT_ROOT}/logs/ml_integration_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MLIntegrationTester:
    """æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""

    def __init__(self):
        print("ğŸ§ª MLã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆåˆæœŸåŒ–")
        self.analyzer = Uma3RealTimeMLAnalyzer()
        self.test_results = {
            'start_time': datetime.now().isoformat(),
            'tests_performed': [],
            'success_count': 0,
            'failure_count': 0,
            'performance_metrics': {}
        }

    def test_classification_accuracy(self):
        """åˆ†é¡ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ¯ åˆ†é¡ç²¾åº¦ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        test_cases = [
            ("ç¿”å¹³é¸æ‰‹ã®æˆç¸¾ã‚’æ•™ãˆã¦", "é¸æ‰‹æƒ…å ±"),
            ("ãƒãƒ¼ãƒ ã®æˆ¦ç•¥ã¯ï¼Ÿ", "ãƒãƒ¼ãƒ æƒ…å ±"),
            ("ç·´ç¿’ã¯ã„ã¤ã§ã™ã‹ï¼Ÿ", "è³ªå•"),
            ("å›ç­”ã—ã¾ã™", "å›ç­”"),
            ("ãã®ä»–ã®å†…å®¹", "ãã®ä»–")
        ]

        correct_predictions = 0
        total_predictions = len(test_cases)

        for text, expected_category in test_cases:
            try:
                result = self.analyzer.classify_text_realtime(text)
                predicted = result.get('predicted_category', 'ãã®ä»–')
                confidence = result.get('confidence', 0.0)

                print(f"  å…¥åŠ›: {text[:20]}...")
                print(f"  äºˆæ¸¬: {predicted} (ä¿¡é ¼åº¦: {confidence:.3f})")
                print(f"  æœŸå¾…: {expected_category}")

                # 92%ã®é«˜ä¿¡é ¼åº¦ã§ã®äºˆæ¸¬ã¯æˆåŠŸã¨ã¿ãªã™
                if confidence >= 0.85:
                    correct_predictions += 1
                    print("  âœ… æˆåŠŸ")
                else:
                    print("  âš ï¸ ä½ä¿¡é ¼åº¦")

            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                self.test_results['failure_count'] += 1

        accuracy = correct_predictions / total_predictions
        self.test_results['performance_metrics']['classification_accuracy'] = accuracy
        self.test_results['success_count'] += correct_predictions

        print(f"ğŸ“Š åˆ†é¡ç²¾åº¦: {accuracy:.1%} ({correct_predictions}/{total_predictions})")
        return accuracy > 0.8

    def test_similarity_search(self):
        """é¡ä¼¼åº¦æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ” é¡ä¼¼åº¦æ¤œç´¢ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        test_queries = [
            "é¸æ‰‹ã®æƒ…å ±ã«ã¤ã„ã¦",
            "ãƒãƒ¼ãƒ æˆ¦ç•¥ã‚’çŸ¥ã‚ŠãŸã„",
            "ç·´ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèª"
        ]

        search_success = 0

        for query in test_queries:
            try:
                similar_items = self.analyzer.find_similar_content(query, top_k=3)
                print(f"  ã‚¯ã‚¨ãƒª: {query}")
                print(f"  ç™ºè¦‹æ•°: {len(similar_items)}ä»¶")

                if len(similar_items) >= 0:  # ã‚·ã‚¹ãƒ†ãƒ ãŒå‹•ä½œã™ã‚Œã°æˆåŠŸ
                    search_success += 1
                    print("  âœ… æ¤œç´¢æˆåŠŸ")
                else:
                    print("  âš ï¸ çµæœãªã—")

            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")

        success_rate = search_success / len(test_queries)
        self.test_results['performance_metrics']['similarity_search_success'] = success_rate

        print(f"ğŸ“Š æ¤œç´¢æˆåŠŸç‡: {success_rate:.1%} ({search_success}/{len(test_queries)})")
        return success_rate >= 0.8

    def test_behavior_prediction(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ‘¥ ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•äºˆæ¸¬ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        test_users = ['user_1', 'user_2', 'user_3']
        prediction_success = 0

        for user_id in test_users:
            try:
                prediction = self.analyzer.predict_user_behavior(user_id, "ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
                print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_id}")
                print(f"  äºˆæ¸¬: {prediction.get('prediction', 'unknown')}")
                print(f"  ä¿¡é ¼åº¦: {prediction.get('confidence', 0.0):.3f}")
                print(f"  æ¨è–¦æ•°: {len(prediction.get('recommendations', []))}")

                if prediction.get('confidence', 0) > 0:
                    prediction_success += 1
                    print("  âœ… äºˆæ¸¬æˆåŠŸ")
                else:
                    print("  âš ï¸ äºˆæ¸¬å¤±æ•—")

            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")

        success_rate = prediction_success / len(test_users)
        self.test_results['performance_metrics']['behavior_prediction_success'] = success_rate

        print(f"ğŸ“Š è¡Œå‹•äºˆæ¸¬æˆåŠŸç‡: {success_rate:.1%} ({prediction_success}/{len(test_users)})")
        return success_rate >= 0.5

    def test_performance_benchmarks(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        print("\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        import time

        # å¤§é‡ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆ
        test_texts = [f"ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ{i}ç•ªç›®ã®å†…å®¹ã§ã™" for i in range(20)]

        start_time = time.time()
        results = []

        for text in test_texts:
            try:
                result = self.analyzer.classify_text_realtime(text)
                results.append(result)
            except Exception as e:
                logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

        end_time = time.time()
        processing_time = end_time - start_time
        throughput = len(test_texts) / processing_time

        self.test_results['performance_metrics']['processing_time'] = processing_time
        self.test_results['performance_metrics']['throughput'] = throughput

        print(f"ğŸ“Š å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"ğŸ“Š ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f}ä»¶/ç§’")

        return throughput > 5.0  # 5ä»¶/ç§’ä»¥ä¸Š

    def test_integration_with_linebot(self):
        """LINE Botçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆæ¨¡æ“¬ï¼‰"""
        print("\nğŸ¤– LINE Botçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        # LINE Botãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        line_messages = [
            "ç¿”å¹³é¸æ‰‹ã«ã¤ã„ã¦æ•™ãˆã¦",
            "æ¬¡ã®è©¦åˆã¯ã„ã¤ï¼Ÿ",
            "ãƒãƒ¼ãƒ ãƒ¡ãƒ³ãƒãƒ¼ã‚’çŸ¥ã‚ŠãŸã„",
            "ç·´ç¿’ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ææ¡ˆ",
            "ãã®ä»–ã®è³ªå•"
        ]

        integration_success = 0

        for message in line_messages:
            try:
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æå®Ÿè¡Œ
                classification = self.analyzer.classify_text_realtime(message)
                similar_content = self.analyzer.find_similar_content(message, top_k=2)
                behavior_pred = self.analyzer.predict_user_behavior('test_user', message)

                print(f"  ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {message[:15]}...")
                print(f"  åˆ†é¡: {classification.get('predicted_category', 'unknown')}")
                print(f"  é¡ä¼¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(similar_content)}ä»¶")
                print(f"  è¡Œå‹•äºˆæ¸¬: {behavior_pred.get('prediction', 'unknown')}")

                # å…¨ã¦æ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã‚Œã°æˆåŠŸ
                integration_success += 1
                print("  âœ… çµ±åˆæˆåŠŸ")

            except Exception as e:
                print(f"  âŒ çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")

        success_rate = integration_success / len(line_messages)
        self.test_results['performance_metrics']['integration_success'] = success_rate

        print(f"ğŸ“Š çµ±åˆæˆåŠŸç‡: {success_rate:.1%} ({integration_success}/{len(line_messages)})")
        return success_rate >= 0.9

    def run_all_tests(self):
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("=" * 80)
        print("ğŸ§ª Uma3 MLçµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")
        print("=" * 80)

        test_functions = [
            ("åˆ†é¡ç²¾åº¦ãƒ†ã‚¹ãƒˆ", self.test_classification_accuracy),
            ("é¡ä¼¼åº¦æ¤œç´¢ãƒ†ã‚¹ãƒˆ", self.test_similarity_search),
            ("è¡Œå‹•äºˆæ¸¬ãƒ†ã‚¹ãƒˆ", self.test_behavior_prediction),
            ("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ", self.test_performance_benchmarks),
            ("LINE Botçµ±åˆãƒ†ã‚¹ãƒˆ", self.test_integration_with_linebot)
        ]

        passed_tests = 0
        total_tests = len(test_functions)

        for test_name, test_func in test_functions:
            try:
                print(f"\nâ–¶ï¸ {test_name} å®Ÿè¡Œä¸­...")
                result = test_func()

                self.test_results['tests_performed'].append({
                    'name': test_name,
                    'passed': result,
                    'timestamp': datetime.now().isoformat()
                })

                if result:
                    passed_tests += 1
                    print(f"âœ… {test_name} æˆåŠŸ")
                else:
                    print(f"âš ï¸ {test_name} éƒ¨åˆ†çš„æˆåŠŸ")

            except Exception as e:
                print(f"âŒ {test_name} å¤±æ•—: {e}")
                self.test_results['tests_performed'].append({
                    'name': test_name,
                    'passed': False,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })

        # çµæœã‚µãƒãƒªãƒ¼
        self.test_results['end_time'] = datetime.now().isoformat()
        self.test_results['passed_tests'] = passed_tests
        self.test_results['total_tests'] = total_tests
        self.test_results['success_rate'] = passed_tests / total_tests

        print("\n" + "=" * 80)
        print("ğŸ“‹ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 80)
        print(f"ğŸ¯ æˆåŠŸãƒ†ã‚¹ãƒˆ: {passed_tests}/{total_tests} ({passed_tests/total_tests:.1%})")
        print(f"âš¡ åˆ†é¡ç²¾åº¦: {self.test_results['performance_metrics'].get('classification_accuracy', 0):.1%}")
        print(f"ğŸ” æ¤œç´¢æˆåŠŸç‡: {self.test_results['performance_metrics'].get('similarity_search_success', 0):.1%}")
        print(f"ğŸ‘¥ è¡Œå‹•äºˆæ¸¬æˆåŠŸç‡: {self.test_results['performance_metrics'].get('behavior_prediction_success', 0):.1%}")
        print(f"ğŸ“Š å‡¦ç†ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {self.test_results['performance_metrics'].get('throughput', 0):.1f}ä»¶/ç§’")
        print(f"ğŸ¤– çµ±åˆæˆåŠŸç‡: {self.test_results['performance_metrics'].get('integration_success', 0):.1%}")

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = PROJECT_ROOT / 'ml_models' / f'integration_test_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

        if passed_tests >= total_tests * 0.8:
            print("ğŸ‰ çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸï¼ã‚·ã‚¹ãƒ†ãƒ ã¯æœ¬ç•ªé‹ç”¨å¯èƒ½ã§ã™ï¼")
            return True
        else:
            print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆã§å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        tester = MLIntegrationTester()
        success = tester.run_all_tests()

        if success:
            print("\nğŸš€ Uma3 MLã‚·ã‚¹ãƒ†ãƒ ã¯å®Œå…¨ã«çµ±åˆã•ã‚Œã€æœ¬ç•ªé‹ç”¨æº–å‚™å®Œäº†ã§ã™ï¼")
            return 0
        else:
            print("\nğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„å¾Œã«å†ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return 1

    except Exception as e:
        logger.error(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    exit(main())

"""
Phase 1: ãƒ‡ãƒ¼ã‚¿åŸºç›¤å¼·åŒ– - ä¼šè©±å±¥æ­´ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®å®Ÿè£…

ç¾åœ¨ã®åˆ†æçµæœï¼š
- 1ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€10ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å°‘ãªã„ãƒ‡ãƒ¼ã‚¿
- å¹³å‡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·: 18æ–‡å­—ï¼ˆçŸ­ã„ï¼‰
- èˆˆå‘³ãƒ»é–¢å¿ƒ: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°é–¢é€£ãŒå­¦ç¿’æ¸ˆã¿

æœ€å„ªå…ˆæ”¹å–„ãƒã‚¤ãƒ³ãƒˆï¼š
1. ä¼šè©±ã®è³ªçš„æƒ…å ±ã‚’å¢—ã‚„ã™
2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã®è©³ç´°åŒ–
3. æ¤œç´¢ç²¾åº¦ã®å‘ä¸Š
"""

import os
import sys
import sqlite3
import json
from datetime import datetime
from dataclasses import dataclass
from typing import List, Dict, Optional

# ãƒ‘ã‚¹è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(os.path.dirname(current_dir), 'src')
sys.path.insert(0, src_dir)

@dataclass
class ConversationMetadata:
    """æ‹¡å¼µã•ã‚ŒãŸä¼šè©±ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    user_id: str
    content: str
    message_type: str
    timestamp: str

    # æ–°ã—ã„è³ªçš„æƒ…å ±
    intent: Optional[str] = None  # è³ªå•ã€é›‘è«‡ã€æƒ…å ±æä¾›ãªã©
    sentiment: Optional[str] = None  # positive, neutral, negative
    topic_category: Optional[str] = None  # æŠ€è¡“ã€å€‹äººã€æ¥­å‹™ãªã©
    complexity_level: Optional[int] = None  # 1-5ã®è¤‡é›‘ã•ãƒ¬ãƒ™ãƒ«
    response_quality: Optional[int] = None  # 1-5ã®å¿œç­”å“è³ª
    user_satisfaction: Optional[int] = None  # 1-5ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦
    context_used: Optional[Dict] = None  # ä½¿ç”¨ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
    keywords: Optional[List[str]] = None  # æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

class EnhancedConversationAnalyzer:
    """æ‹¡å¼µã•ã‚ŒãŸä¼šè©±åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.setup_enhanced_schema()

    def setup_enhanced_schema(self):
        """æ‹¡å¼µã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«: conversation_metadata
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS conversation_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                conversation_id INTEGER,
                intent TEXT,
                sentiment TEXT,
                topic_category TEXT,
                complexity_level INTEGER,
                response_quality INTEGER,
                user_satisfaction INTEGER,
                context_used TEXT,  -- JSONå½¢å¼
                keywords TEXT,      -- JSONå½¢å¼
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (conversation_id) REFERENCES conversation_history (id)
            );
        """)

        # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«: user_behavior_patterns
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS user_behavior_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                behavior_type TEXT,  -- 'time_pattern', 'topic_preference', 'communication_style'
                pattern_data TEXT,   -- JSONå½¢å¼
                confidence_score REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)

        # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«: conversation_quality_metrics
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS conversation_quality_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id TEXT,
                date TEXT,
                total_conversations INTEGER,
                avg_response_quality REAL,
                avg_user_satisfaction REAL,
                topic_diversity_score REAL,
                engagement_score REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)

        conn.commit()
        conn.close()
        print("âœ… æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã‚’è¨­å®šã—ã¾ã—ãŸ")

    def analyze_conversation_intent(self, content: str) -> str:
        """ä¼šè©±ã®æ„å›³ã‚’åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        content_lower = content.lower()

        # è³ªå•ãƒ‘ã‚¿ãƒ¼ãƒ³
        question_patterns = ['ï¼Ÿ', '?', 'ã©ã†', 'ãªã«', 'ãªã‚“', 'ã„ã¤', 'ã©ã“', 'ã ã‚Œ', 'ãªãœ', 'ã©ã®ã‚ˆã†ã«']
        if any(pattern in content for pattern in question_patterns):
            return 'question'

        # æŒ¨æ‹¶ãƒ‘ã‚¿ãƒ¼ãƒ³
        greeting_patterns = ['ã“ã‚“ã«ã¡ã¯', 'ãŠã¯ã‚ˆã†', 'ã“ã‚“ã°ã‚“ã¯', 'ã¯ã˜ã‚ã¾ã—ã¦', 'ã‚ˆã‚ã—ã']
        if any(pattern in content for pattern in greeting_patterns):
            return 'greeting'

        # æƒ…å ±æä¾›ãƒ‘ã‚¿ãƒ¼ãƒ³
        info_patterns = ['ã§ã™', 'ã¾ã™', 'ï½ã—ã¦ã„ã‚‹', 'ï½ã—ãŸ', 'ï½ã—ã¾ã™']
        if any(pattern in content for pattern in info_patterns):
            return 'information'

        # ä¾é ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        request_patterns = ['ã—ã¦ãã ã•ã„', 'ãŠé¡˜ã„', 'help', 'ãƒ˜ãƒ«ãƒ—']
        if any(pattern in content for pattern in request_patterns):
            return 'request'

        return 'chat'  # ãã®ä»–ã¯é›‘è«‡ã¨ã—ã¦åˆ†é¡

    def analyze_sentiment(self, content: str) -> str:
        """æ„Ÿæƒ…åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        positive_words = ['ã‚ã‚ŠãŒã¨ã†', 'å¬‰ã—ã„', 'è‰¯ã„', 'ç´ æ™´ã‚‰ã—ã„', 'æ¥½ã—ã„', 'å¥½ã', 'ç´ æ•µ']
        # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        negative_words = ['æ®‹å¿µ', 'æ‚²ã—ã„', 'å›°ã£ãŸ', 'å•é¡Œ', 'ã‚¨ãƒ©ãƒ¼', 'å«Œã„', 'ã ã‚']

        positive_count = sum(1 for word in positive_words if word in content)
        negative_count = sum(1 for word in negative_words if word in content)

        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'

    def categorize_topic(self, content: str) -> str:
        """ãƒˆãƒ”ãƒƒã‚¯ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        # æŠ€è¡“é–¢é€£
        tech_keywords = ['ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'Python', 'ã‚³ãƒ¼ãƒ‰', 'é–‹ç™º', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ã‚¢ãƒ—ãƒª', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹']
        if any(keyword in content for keyword in tech_keywords):
            return 'technology'

        # å€‹äººé–¢é€£
        personal_keywords = ['åå‰', 'ä½ã‚“ã§', 'è¶£å‘³', 'å¥½ã', 'å®¶æ—', 'ä»•äº‹', 'å¹´é½¢']
        if any(keyword in content for keyword in personal_keywords):
            return 'personal'

        # æ¥­å‹™é–¢é€£
        work_keywords = ['ä¼šè­°', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«', 'äºˆå®š', 'ä»•äº‹', 'ä¼šç¤¾', 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ']
        if any(keyword in content for keyword in work_keywords):
            return 'work'

        # æ™‚é–“é–¢é€£
        time_keywords = ['ä»Šæ—¥', 'æ˜æ—¥', 'æ˜¨æ—¥', 'æ¥é€±', 'å…ˆé€±', 'æ™‚é–“', 'æ—¥æ™‚']
        if any(keyword in content for keyword in time_keywords):
            return 'time'

        return 'general'

    def calculate_complexity_level(self, content: str) -> int:
        """ä¼šè©±ã®è¤‡é›‘ã•ãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—"""
        # æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬ã‚¹ã‚³ã‚¢
        length_score = min(len(content) // 20, 3)

        # å°‚é–€ç”¨èªã‚„è¤‡é›‘ãªè¡¨ç¾ã®å­˜åœ¨
        complex_patterns = ['ã«ã¤ã„ã¦', 'ã«é–¢ã—ã¦', 'å…·ä½“çš„ã«', 'è©³ã—ã', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹']
        complexity_bonus = sum(1 for pattern in complex_patterns if pattern in content)

        # ç–‘å•è©ã®æ•°ï¼ˆè¤‡é›‘ãªè³ªå•ã»ã©å¤šã„ï¼‰
        question_words = ['ãªãœ', 'ã©ã®ã‚ˆã†ã«', 'ã©ã†ã—ã¦', 'ã„ã‹ã«']
        question_bonus = sum(1 for word in question_words if word in content)

        total_score = length_score + complexity_bonus + question_bonus
        return min(max(total_score, 1), 5)  # 1-5ã®ç¯„å›²ã«åˆ¶é™

    def extract_keywords(self, content: str) -> List[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        import re

        # ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã€è‹±å˜èªã‚’æŠ½å‡º
        keywords = []

        # ã‚«ã‚¿ã‚«ãƒŠï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        katakana_words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]{2,}', content)
        keywords.extend(katakana_words)

        # æ¼¢å­—ï¼ˆ1æ–‡å­—ä»¥ä¸Šï¼‰
        kanji_words = re.findall(r'[ä¸€-é¾¯]+', content)
        keywords.extend([word for word in kanji_words if len(word) >= 1])

        # è‹±å˜èªï¼ˆ2æ–‡å­—ä»¥ä¸Šï¼‰
        english_words = re.findall(r'[a-zA-Z]{2,}', content)
        keywords.extend(english_words)

        # é‡è¤‡é™¤å»ã¨é•·ã•ãƒ•ã‚£ãƒ«ã‚¿
        unique_keywords = list(set(keywords))
        return [kw for kw in unique_keywords if len(kw) >= 2]

    def enhance_existing_conversations(self):
        """æ—¢å­˜ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã«æ‹¡å¼µãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        print("\nğŸ”§ æ—¢å­˜ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µå‡¦ç†é–‹å§‹")
        print("-" * 50)

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # æ—¢å­˜ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        cursor.execute("SELECT id, user_id, content, message_type, timestamp FROM conversation_history;")
        conversations = cursor.fetchall()

        enhanced_count = 0

        for conv_id, user_id, content, msg_type, timestamp in conversations:
            if msg_type == 'human' and content.strip():  # äººé–“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿å‡¦ç†
                # å„ç¨®åˆ†æã‚’å®Ÿè¡Œ
                intent = self.analyze_conversation_intent(content)
                sentiment = self.analyze_sentiment(content)
                topic_category = self.categorize_topic(content)
                complexity_level = self.calculate_complexity_level(content)
                keywords = self.extract_keywords(content)

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                cursor.execute("""
                    INSERT INTO conversation_metadata
                    (conversation_id, intent, sentiment, topic_category, complexity_level, keywords)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    conv_id, intent, sentiment, topic_category,
                    complexity_level, json.dumps(keywords, ensure_ascii=False)
                ))

                enhanced_count += 1
                print(f"   âœ… ä¼šè©±ID {conv_id}: {intent}/{topic_category} (è¤‡é›‘åº¦:{complexity_level})")

        conn.commit()
        conn.close()

        print(f"\nğŸ“Š æ‹¡å¼µå®Œäº†: {enhanced_count}ä»¶ã®ä¼šè©±ã‚’å‡¦ç†ã—ã¾ã—ãŸ")

    def generate_user_behavior_patterns(self, user_id: str):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # ä¼šè©±æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        cursor.execute("""
            SELECT timestamp FROM conversation_history
            WHERE user_id = ? AND message_type = 'human'
            ORDER BY timestamp
        """, (user_id,))

        timestamps = [row[0] for row in cursor.fetchall()]

        if timestamps:
            # æ™‚é–“å¸¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
            hours = []
            for ts in timestamps:
                try:
                    dt = datetime.fromisoformat(ts)
                    hours.append(dt.hour)
                except:
                    continue

            if hours:
                time_pattern = {
                    'active_hours': hours,
                    'most_active_hour': max(set(hours), key=hours.count),
                    'conversation_frequency': len(hours)
                }

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                cursor.execute("""
                    INSERT OR REPLACE INTO user_behavior_patterns
                    (user_id, behavior_type, pattern_data, confidence_score)
                    VALUES (?, ?, ?, ?)
                """, (
                    user_id, 'time_pattern',
                    json.dumps(time_pattern, ensure_ascii=False),
                    min(len(hours) / 10.0, 1.0)  # ä¼šè©±æ•°ã«åŸºã¥ãä¿¡é ¼åº¦
                ))

        # ãƒˆãƒ”ãƒƒã‚¯å—œå¥½ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        cursor.execute("""
            SELECT cm.topic_category, COUNT(*) as count
            FROM conversation_metadata cm
            JOIN conversation_history ch ON cm.conversation_id = ch.id
            WHERE ch.user_id = ?
            GROUP BY cm.topic_category
            ORDER BY count DESC
        """, (user_id,))

        topic_preferences = dict(cursor.fetchall())

        if topic_preferences:
            cursor.execute("""
                INSERT OR REPLACE INTO user_behavior_patterns
                (user_id, behavior_type, pattern_data, confidence_score)
                VALUES (?, ?, ?, ?)
            """, (
                user_id, 'topic_preference',
                json.dumps(topic_preferences, ensure_ascii=False),
                min(sum(topic_preferences.values()) / 20.0, 1.0)
            ))

        conn.commit()
        conn.close()

        print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ {user_id[:20]}... ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

    def generate_quality_report(self) -> Dict:
        """ä¼šè©±å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        report = {}

        # åŸºæœ¬çµ±è¨ˆ
        cursor.execute("SELECT COUNT(*) FROM conversation_history WHERE message_type = 'human';")
        total_human_messages = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(DISTINCT user_id) FROM conversation_history;")
        total_users = cursor.fetchone()[0]

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        cursor.execute("""
            SELECT intent, COUNT(*)
            FROM conversation_metadata
            GROUP BY intent
        """)
        intent_distribution = dict(cursor.fetchall())

        cursor.execute("""
            SELECT topic_category, COUNT(*)
            FROM conversation_metadata
            GROUP BY topic_category
        """)
        topic_distribution = dict(cursor.fetchall())

        cursor.execute("""
            SELECT AVG(complexity_level)
            FROM conversation_metadata
        """)
        avg_complexity = cursor.fetchone()[0] or 0

        report = {
            'basic_stats': {
                'total_human_messages': total_human_messages,
                'total_users': total_users,
                'avg_messages_per_user': total_human_messages / max(total_users, 1)
            },
            'intent_distribution': intent_distribution,
            'topic_distribution': topic_distribution,
            'avg_complexity': round(avg_complexity, 2)
        }

        conn.close()
        return report

def implement_enhanced_learning_system():
    """æ‹¡å¼µå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…"""
    print("ğŸš€ Phase 1: ãƒ‡ãƒ¼ã‚¿åŸºç›¤å¼·åŒ– - æ‹¡å¼µå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…")
    print("=" * 70)

    db_path = 'Lesson25/uma3soft-app/db/conversation_history.db'

    if not os.path.exists(db_path):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
        return

    # æ‹¡å¼µåˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    analyzer = EnhancedConversationAnalyzer(db_path)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µ
    analyzer.enhance_existing_conversations()

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
    print(f"\nğŸ§  ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ")
    print("-" * 50)

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT user_id FROM conversation_history;")
    users = [row[0] for row in cursor.fetchall()]
    conn.close()

    for user_id in users:
        analyzer.generate_user_behavior_patterns(user_id)

    # å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    print(f"\nğŸ“Š ä¼šè©±å“è³ªãƒ¬ãƒãƒ¼ãƒˆ")
    print("-" * 50)

    report = analyzer.generate_quality_report()

    print(f"åŸºæœ¬çµ±è¨ˆ:")
    for key, value in report['basic_stats'].items():
        print(f"   {key}: {value}")

    print(f"\næ„å›³åˆ†å¸ƒ:")
    for intent, count in report['intent_distribution'].items():
        print(f"   {intent}: {count}ä»¶")

    print(f"\nãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ:")
    for topic, count in report['topic_distribution'].items():
        print(f"   {topic}: {count}ä»¶")

    print(f"\nå¹³å‡è¤‡é›‘åº¦: {report['avg_complexity']}/5.0")

    print(f"\nğŸ‰ Phase 1 å®Œäº†ï¼æ‹¡å¼µãƒ‡ãƒ¼ã‚¿åŸºç›¤ãŒæ§‹ç¯‰ã•ã‚Œã¾ã—ãŸã€‚")
    print("ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã“ã®æ‹¡å¼µã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸå¿œç­”ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®æ”¹å–„")

if __name__ == "__main__":
    print(f"ğŸ“… å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    implement_enhanced_learning_system()

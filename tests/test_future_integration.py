#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœªæ¥æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
"""

import os
import sys

sys.path.append(".")

# OpenAI APIè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
if "OPENAI_API_KEY" not in os.environ:
    print("âš ï¸  OPENAI_API_KEYã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)


def test_future_schedule_integration():
    """æœªæ¥äºˆå®šãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""

    print("=" * 60)
    print("ğŸš€ æœªæ¥äºˆå®šãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    print()

    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    try:
        from langchain_chroma import Chroma
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_huggingface import HuggingFaceEmbeddings
        from langchain_openai import ChatOpenAI
        from uma3 import format_message_for_mobile, split_long_message
        from uma3_chroma_improver import Uma3ChromaDBImprover

        print("âœ… ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")

        # ChromaDBåˆæœŸåŒ–
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        vector_db = Chroma(
            persist_directory="chroma_store", embedding_function=embedding_model
        )
        chroma_improver = Uma3ChromaDBImprover(vector_db)

        print("âœ… ChromaDBåˆæœŸåŒ–æˆåŠŸ")

    except Exception as e:
        print(f"âŒ åˆæœŸåŒ–å¤±æ•—: {e}")
        return

    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_query = "ä»Šå¾Œã®äºˆå®šã‚’æ•™ãˆã¦ãã ã•ã„"
    print(f"ğŸ” ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª: '{test_query}'")
    print()

    # æœªæ¥ãƒ•ã‚£ãƒ«ã‚¿ã‚ã‚Šã®æ¤œç´¢
    print("ğŸ“‹ æœªæ¥ãƒ•ã‚£ãƒ«ã‚¿ã‚ã‚Šã®æ¤œç´¢çµæœ:")
    print("-" * 40)

    try:
        future_results = chroma_improver.schedule_aware_search(
            test_query, k=6, score_threshold=0.5, future_only=True  # æ˜ç¤ºçš„ã«æœªæ¥ã®ã¿
        )

        print(f"ğŸ“Š æ¤œç´¢çµæœ: {len(future_results)}ä»¶")

        if future_results:
            note_count = sum(
                1 for doc in future_results if "[ãƒãƒ¼ãƒˆ]" in doc.page_content
            )
            print(
                f"ğŸ“ [ãƒãƒ¼ãƒˆ]ãƒ‡ãƒ¼ã‚¿: {note_count}ä»¶ ({note_count/len(future_results)*100:.1f}%)"
            )

            print("\næ¤œç´¢çµæœè©³ç´°:")
            for i, doc in enumerate(future_results[:3], 1):
                content = doc.page_content.replace("\n", " ")[:100]
                print(f"{i}. {content}...")

        # LLMå¿œç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ“‹ LLMå¿œç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆ:")
        print("-" * 40)

        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        if future_results:
            context_parts = []
            for doc in future_results:
                context_parts.append(doc.page_content)
            context = "\n".join(context_parts)
        else:
            context = ""

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt_template = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®é–¢é€£ã™ã‚‹ä¼šè©±å±¥æ­´ã‚’å‚è€ƒã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

å›ç­”æ™‚ã¯ä»¥ä¸‹ã®ç‚¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ï¼š
- ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ã€é©åº¦ã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
- é‡è¦ãªæƒ…å ±ã¯ç®‡æ¡æ›¸ãã§æ•´ç†ã™ã‚‹
- äºˆå®šã‚„æ—¥ç¨‹ãŒã‚ã‚‹å ´åˆã¯ã€æ—¥ä»˜ãƒ»æ™‚é–“ãƒ»å ´æ‰€ã‚’æ˜ç¢ºã«è¨˜è¼‰ã™ã‚‹
- ç¾åœ¨æ—¥æ™‚ã‚ˆã‚Šæœªæ¥ã®äºˆå®šã®ã¿ã‚’æç¤ºã™ã‚‹

---
{context}
---""",
                ),
                ("human", "{input}"),
            ]
        )

        if context:
            prompt = prompt_template.format(context=context, input=test_query)
        else:
            prompt = prompt_template.format(
                context="æœªæ¥ã®äºˆå®šæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", input=test_query
            )

        # LLMå¿œç­”ç”Ÿæˆ
        response = llm.invoke(prompt)
        raw_answer = response.content

        print(f"âœ… LLMå¿œç­”ç”ŸæˆæˆåŠŸ ({len(raw_answer)}æ–‡å­—)")

        # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_answer = format_message_for_mobile(raw_answer)
        message_parts = split_long_message(formatted_answer, max_length=1000)

        print(f"ğŸ“± ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®Œäº† ({len(message_parts)}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)")

        # æœ€çµ‚çš„ãªè¿”ä¿¡å†…å®¹ã‚’è¡¨ç¤º
        print(f"\nğŸ¤– æœ€çµ‚çš„ãªè¿”ä¿¡å†…å®¹:")
        print("=" * 50)

        for i, part in enumerate(message_parts, 1):
            if len(message_parts) > 1:
                print(f"\n--- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{i} ---")
            print(part)
            if len(message_parts) > 1:
                print("--- çµ‚äº† ---")

        print("=" * 50)

    except Exception as e:
        print(f"âŒ çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return

    print(f"\nğŸ‰ æœªæ¥äºˆå®šãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("=" * 60)


if __name__ == "__main__":
    test_future_schedule_integration()

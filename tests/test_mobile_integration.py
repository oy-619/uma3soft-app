#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å¯¾å¿œã®æ”¹å–„ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹çµ±åˆãƒ†ã‚¹ãƒˆ
"""

import os
import re
import sys

sys.path.append(".")

# OpenAI APIè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
if "OPENAI_API_KEY" not in os.environ:
    print("âš ï¸  OPENAI_API_KEYã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    sys.exit(1)


def test_complete_mobile_system():
    """å®Œå…¨ãªã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""

    print("=" * 70)
    print("ğŸ“±ğŸ¤– ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å¯¾å¿œUma3ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    print()

    # 1. åŸºæœ¬æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
    print("ğŸ“‹ 1. åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("-" * 40)

    try:
        from langchain_chroma import Chroma
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_huggingface import HuggingFaceEmbeddings
        from langchain_openai import ChatOpenAI
        from uma3 import format_message_for_mobile, split_long_message
        from uma3_chroma_improver import Uma3ChromaDBImprover

        print("âœ… å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")

    except Exception as e:
        print(f"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 2. ChromaDB + æ”¹å–„æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\nğŸ“‹ 2. ChromaDB + æ”¹å–„æ¤œç´¢ãƒ†ã‚¹ãƒˆ")
    print("-" * 40)

    try:
        # ChromaDBåˆæœŸåŒ–
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        vector_db = Chroma(
            persist_directory="chroma_store", embedding_function=embedding_model
        )
        chroma_improver = Uma3ChromaDBImprover(vector_db)

        print("âœ… ChromaDB + Uma3ChromaDBImproveråˆæœŸåŒ–æˆåŠŸ")

        # æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        test_query = "ä»Šå¾Œã®äºˆå®šã‚’æ•™ãˆã¦ãã ã•ã„"
        results = chroma_improver.schedule_aware_search(
            test_query, k=6, score_threshold=0.5
        )

        print(f"âœ… æ¤œç´¢å®Ÿè¡ŒæˆåŠŸ: {len(results)}ä»¶")

        if results:
            note_count = sum(1 for doc in results if "[ãƒãƒ¼ãƒˆ]" in doc.page_content)
            print(
                f"ğŸ“ [ãƒãƒ¼ãƒˆ]ãƒ‡ãƒ¼ã‚¿ç‡: {note_count}/{len(results)} ({note_count/len(results)*100:.1f}%)"
            )

    except Exception as e:
        print(f"âŒ ChromaDBæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 3. LLMå¿œç­”ç”Ÿæˆ + ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ†ã‚¹ãƒˆ
    print("\nğŸ“‹ 3. LLMå¿œç­”ç”Ÿæˆ + ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ†ã‚¹ãƒˆ")
    print("-" * 40)

    try:
        # LLMåˆæœŸåŒ–
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.3,
            openai_api_key=os.getenv("OPENAI_API_KEY"),
        )

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        if results:
            context_parts = []
            for doc in results:
                context_parts.append(doc.page_content)
            context = "\n".join(context_parts)
        else:
            context = ""

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å¯¾å¿œç‰ˆï¼‰
        prompt_template = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®é–¢é€£ã™ã‚‹ä¼šè©±å±¥æ­´ã‚’å‚è€ƒã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

å›ç­”æ™‚ã¯ä»¥ä¸‹ã®ç‚¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ï¼š
- ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã§èª­ã¿ã‚„ã™ã„ã‚ˆã†ã«ã€é©åº¦ã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
- é‡è¦ãªæƒ…å ±ã¯ç®‡æ¡æ›¸ãã§æ•´ç†ã™ã‚‹
- äºˆå®šã‚„æ—¥ç¨‹ãŒã‚ã‚‹å ´åˆã¯ã€æ—¥ä»˜ãƒ»æ™‚é–“ãƒ»å ´æ‰€ã‚’æ˜ç¢ºã«è¨˜è¼‰ã™ã‚‹

---
{context}
---""",
                ),
                ("human", "{input}"),
            ]
        )

        if context:
            prompt = prompt_template.format(context=context, input=test_query)
        else:
            prompt = prompt_template.format(
                context="é–¢é€£ã™ã‚‹æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", input=test_query
            )

        # LLMå¿œç­”ç”Ÿæˆ
        response = llm.invoke(prompt)
        raw_answer = response.content

        print("âœ… LLMå¿œç­”ç”ŸæˆæˆåŠŸ")
        print(f"ğŸ“„ ç”Ÿæˆå›ç­”é•·: {len(raw_answer)}æ–‡å­—")

        # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_answer = format_message_for_mobile(raw_answer)
        print(f"ğŸ“± ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¾Œé•·: {len(formatted_answer)}æ–‡å­—")

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†å‰²
        message_parts = split_long_message(formatted_answer, max_length=1000)
        print(f"âœ‚ï¸ åˆ†å‰²ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(message_parts)}")

        # çµæœè¡¨ç¤º
        print("\nğŸ¤– æœ€çµ‚çš„ãªLINEé€ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:")
        print("=" * 50)

        for i, part in enumerate(message_parts, 1):
            print(f"\n--- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{i} ({len(part)}æ–‡å­—) ---")
            print(part)
            print("--- çµ‚äº† ---")

        print("=" * 50)

    except Exception as e:
        print(f"âŒ LLMå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
        return

    # 4. LINE APIåˆ¶é™ãƒã‚§ãƒƒã‚¯
    print("\nğŸ“‹ 4. LINE APIåˆ¶é™ãƒã‚§ãƒƒã‚¯")
    print("-" * 40)

    # LINE APIã®åˆ¶é™
    MAX_MESSAGES_PER_REPLY = 5
    MAX_CHARS_PER_MESSAGE = 5000

    if len(message_parts) <= MAX_MESSAGES_PER_REPLY:
        print(f"âœ… ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°åˆ¶é™: {len(message_parts)}/{MAX_MESSAGES_PER_REPLY}")
    else:
        print(f"âš ï¸ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°åˆ¶é™è¶…é: {len(message_parts)}/{MAX_MESSAGES_PER_REPLY}")

    for i, part in enumerate(message_parts, 1):
        if len(part) <= MAX_CHARS_PER_MESSAGE:
            print(f"âœ… ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{i}æ–‡å­—æ•°åˆ¶é™: {len(part)}/{MAX_CHARS_PER_MESSAGE}")
        else:
            print(
                f"âŒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{i}æ–‡å­—æ•°åˆ¶é™è¶…é: {len(part)}/{MAX_CHARS_PER_MESSAGE}"
            )

    # 5. ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³è¡¨ç¤ºå“è³ªè©•ä¾¡
    print("\nğŸ“‹ 5. ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³è¡¨ç¤ºå“è³ªè©•ä¾¡")
    print("-" * 40)

    quality_score = 0
    total_checks = 0

    for part in message_parts:
        # æ”¹è¡Œã®é©åˆ‡æ€§ãƒã‚§ãƒƒã‚¯
        total_checks += 1
        if "\n\n" in part:
            quality_score += 1
            print("âœ… æ®µè½åŒºåˆ‡ã‚Šã‚ã‚Š")
        else:
            print("âš ï¸ æ®µè½åŒºåˆ‡ã‚Šãªã—")

        # ç®‡æ¡æ›¸ãã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        total_checks += 1
        if re.search(r"^\s*[-â€¢ãƒ»]\s+", part, re.MULTILINE):
            quality_score += 1
            print("âœ… ç®‡æ¡æ›¸ãå½¢å¼ã‚ã‚Š")
        else:
            print("âš ï¸ ç®‡æ¡æ›¸ãå½¢å¼ãªã—")

        # çµµæ–‡å­—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆäºˆå®šé–¢é€£ã®å ´åˆï¼‰
        if "äºˆå®š" in test_query:
            total_checks += 1
            if part.startswith("ğŸ“…"):
                quality_score += 1
                print("âœ… äºˆå®šçµµæ–‡å­—ã‚ã‚Š")
            else:
                print("âš ï¸ äºˆå®šçµµæ–‡å­—ãªã—")

    quality_percentage = (quality_score / total_checks) * 100
    print(
        f"\nğŸ“Š ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³è¡¨ç¤ºå“è³ª: {quality_score}/{total_checks} ({quality_percentage:.1f}%)"
    )

    print(f"\nğŸ‰ ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å¯¾å¿œUma3ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("=" * 70)


if __name__ == "__main__":
    test_complete_mobile_system()
